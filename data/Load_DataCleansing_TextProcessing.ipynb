{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W266 Final Project - Load_DataCleansing_TextProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mfrazzini/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /home/mfrazzini/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Initial data load, summary, and high level sanity checks\n",
    "\n",
    "# Import a bunch of libraries.\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "from IPython.core.display import clear_output\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import brown\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import webtext\n",
    "#import parseyface\n",
    "\n",
    "#Prod \n",
    "#raw_reviews = pd.read_csv('RR_Pared_All_Active_2018-07-09.csv')\n",
    "#Dev:\n",
    "raw_reviews = pd.read_csv('RR_Pared_All_Active_Dev.csv')\n",
    "# Errors:\n",
    "#raw_reviews = pd.read_csv('Error_Records.csv')\n",
    "#raw_reviews = pd.read_csv('Error_Records_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1070247, 3)\n"
     ]
    }
   ],
   "source": [
    "path =r'.' # use your path\n",
    "allFiles = glob.glob(path + \"/RR_Pared_All_Active_SBD_2018-07-09_*.csv\")\n",
    "review_sentences = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_, index_col=None, header=0)\n",
    "    list_.append(df)\n",
    "review_sentences = pd.concat(list_)\n",
    "\n",
    "print(review_sentences.shape)\n",
    "\n",
    "#review_sentences = pd.read_csv(\"RR_Pared_All_Active_SBD_2018-07-09_0.csv\")\n",
    "#print(review_sentences)\n",
    "#print(review_sentences.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove html special characters that end up in the review text\n",
    "from bs4 import BeautifulSoup \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spell Check Procedures with Custom Dictionary Adds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands = pd.read_csv('Brands_All_Active_2018-07-09.csv')\n",
    "features = pd.read_csv('FeatureDump_All_Active_2018-07-09T16_31_17.csv')\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(brown.words())\n",
    "\n",
    "#Add custom dictionary brands\n",
    "for index, row in brands.iterrows():\n",
    "    parsed_words = words(row['BRAND'])\n",
    "    for w in parsed_words:\n",
    "        if len(w) >=3:\n",
    "            WORDS[w] += 1\n",
    "\n",
    "#Add custom dictionary features\n",
    "for index, row in features.iterrows():\n",
    "    feature_blurb = row['FEATURENAME']\n",
    "    if type(feature_blurb) is str: \n",
    "        nopunc = ''\n",
    "        nopunc = [char for char in feature_blurb if char not in string.punctuation]\n",
    "        nopunc = ''.join(nopunc)\n",
    "        parsed_words = words(nopunc)\n",
    "        for w in parsed_words:\n",
    "            if len(w) >=2:\n",
    "                WORDS[w] += 1\n",
    "\n",
    "#Add custom dictionary collections\n",
    "WORDS[\"tls\"] += 1\n",
    "\n",
    "#Add custom dictionary travel words\n",
    "WORDS[\"repacked\"] += 1\n",
    "WORDS[\"repack\"] += 1\n",
    "WORDS[\"carryon\"] += 1\n",
    "WORDS[\"toiletry\"] += 1\n",
    "WORDS[\"laptop\"] += 1\n",
    "WORDS[\"carousel\"] += 1\n",
    "WORDS[\"kindle\"] += 1\n",
    "WORDS[\"ipad\"] += 1\n",
    "WORDS[\"unpacked\"] += 1\n",
    "WORDS[\"unpack\"] += 1\n",
    "WORDS[\"mac\"] += 1\n",
    "WORDS[\"iphone\"] += 1\n",
    "WORDS[\"dvd\"] += 1\n",
    "WORDS[\"tsa\"] += 1\n",
    "WORDS[\"repacking\"] += 1\n",
    "WORDS[\"overpack\"] += 1\n",
    "WORDS[\"cram\"] += 1\n",
    "\n",
    "#Add custom dictionary Misc\n",
    "WORDS[\"ebag\"] += 1\n",
    "WORDS[\"downside\"] += 1\n",
    "WORDS[\"andor\"] += 1\n",
    "WORDS[\"classy\"] += 1\n",
    "WORDS[\"online\"] += 1\n",
    "WORDS[\"bike\"] += 1\n",
    "WORDS[\"hassle\"] += 1\n",
    "WORDS[\"rips\"] += 1\n",
    "WORDS[\"undies\"] += 1\n",
    "WORDS[\"alot\"] += 1\n",
    "WORDS[\"wouldnt\"] += 1\n",
    "WORDS[\"pricey\"] += 1\n",
    "WORDS[\"im\"] += 1\n",
    "WORDS[\"picky\"] += 1\n",
    "WORDS[\"unusable\"] += 1\n",
    "WORDS[\"manageable\"] += 1\n",
    "WORDS[\"yrs\"] += 1\n",
    "WORDS[\"neater\"] += 1\n",
    "WORDS[\"plusses\"] += 1\n",
    "WORDS[\"alot\"] += 1\n",
    "WORDS[\"gripe\"] += 1\n",
    "\n",
    "\n",
    "def dict_check(word):\n",
    "    if word.lower() in WORDS:\n",
    "        return(word)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comfortable\n",
      "\n",
      "comfortable\n",
      "\n",
      "unpacked\n"
     ]
    }
   ],
   "source": [
    "# Test mod for initial dictionary check\n",
    "print(dict_check(\"comfortable\"))\n",
    "print(dict_check(\"confortable\"))\n",
    "print(correction(\"confortable\"))\n",
    "print(dict_check(\"Baggalini\"))\n",
    "print(correction(\"unpacked\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total review sentences:  132350 [13752263] words correct:  [6839826] corrected:  [216521] corrected default: [34302]\n",
      "Total words:  [13752265]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "#review_sentences = pd.read_csv(\"Review_sentences_Dev_Active_2018-07-09.csv\")\n",
    "\n",
    "\n",
    "def text_process(text):\n",
    "    #Takes in a string of text, then performs the following:\n",
    "    #1. Remove all html special characters, with regex to pull partially remove html special characters\n",
    "    #2. Spelling correction\n",
    "    #3. Remove all punctuation\n",
    "    #4. Remove all stopwords\n",
    "    #5. Stemming of words\n",
    "    #6. Return the cleaned text as a list of words\n",
    "    if type(text) is str:\n",
    "        review = []\n",
    "        nopunc = ''\n",
    "        nopunc = [char for char in text if char not in string.punctuation]\n",
    "        nopunc = ''.join(nopunc)\n",
    "        for word in nopunc.split():\n",
    "            #print(word)\n",
    "            if len(word) <= int(45):\n",
    "                total_words[0] += 1\n",
    "                if word.lower() not in stopwords.words('english'):\n",
    "                    #html_decoded = BeautifulSoup(word, \"lxml\")\n",
    "                    #html_decoded_string = html_decoded.string\n",
    "                    if re.search('\\w(\\d+)\\w', word): #html_decoded_string):\n",
    "                        word = word.replace(\"34\",\"\\\"\")\n",
    "                        word = word.replace(\"39\",\"\\'\")\n",
    "                    if word == dict_check(word):\n",
    "                        correct[0] += 1\n",
    "                        spell_corrected = word\n",
    "                    else:\n",
    "                        if word in corrections:\n",
    "                            spell_corrected = corrections[word]\n",
    "                            corrections[word] = spell_corrected\n",
    "                            corrections_count[spell_corrected + ':' + word] += 1\n",
    "                            corrected[0] += 1\n",
    "                        else:\n",
    "                            spell_corrected = correction(word)\n",
    "                            if word != spell_corrected:\n",
    "                                corrections[word] = spell_corrected\n",
    "                                corrections_count[spell_corrected + ':' + word] += 1\n",
    "                                corrected[0] += 1  \n",
    "                            else:\n",
    "                                corrections[word] = word\n",
    "                                corrected_default[0] += 1\n",
    "                    #porter = PorterStemmer()\n",
    "                    #stemmed = porter.stem(spell_corrected)\n",
    "                    review.append(spell_corrected) \n",
    "                else:\n",
    "                    review.append(word)\n",
    "            else:\n",
    "                review.append('')\n",
    "        return ' '.join(review)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "total_review_sentences = [0]\n",
    "total_words = [0]\n",
    "correct = [0]\n",
    "corrected = [0]\n",
    "corrected_default = [0]\n",
    "corrections = dict()\n",
    "corrections_count = Counter()\n",
    "\n",
    "processed_testimonial = str\n",
    "\n",
    "#f = open('debug.txt','w')\n",
    "    \n",
    "for index, row in review_sentences.iterrows():\n",
    "    total_review_sentences = index\n",
    "    clear_output()\n",
    "    print(\"On record: {}; \".format(index), \"total words\", total_words, \"words correct: \", correct, \\\n",
    "          \"corrected: \", corrected, \"corrected default:\", corrected_default, end=\"\\r\")\n",
    "    review_sentences.at[index, 'SENTENCE'] = text_process(row['SENTENCE'])\n",
    "\n",
    "print(\"Total review sentences: \", total_review_sentences)\n",
    "print(\"Total words: \", total_words)  \n",
    "\n",
    "review_sentences.to_csv(\"Review_sentences_Prod_Active_Cleansed_2018-07-09.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     1069186\n",
       "unique     123796\n",
       "top       Love it\n",
       "freq         3395\n",
       "Name: SENTENCE, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_sentences = pd.read_csv(\"Review_sentences_Prod_Active_Cleansed_2018-07-09.csv\")\n",
    "review_sentences['SENTENCE'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print spell corrections for analysis and tuning\n",
    "f = open(\"corrections_count.csv\", encoding='utf-8-sig', mode='w')  \n",
    "f .write('SPELL_CORRECTION, COUNT\\n')  \n",
    "for key, value in corrections_count.items():  \n",
    "    f.write('{},{}\\n'.format(key, value))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"I'm:I'm\", 11590),\n",
       " ('I:Im', 8392),\n",
       " (\"I've:I've\", 7610),\n",
       " ('ve:Ive', 5992),\n",
       " ('didn:didnt', 3443),\n",
       " (\"I'll:I'll\", 1988),\n",
       " (\"I'd:I'd\", 1916),\n",
       " ('haven:havent', 1900),\n",
       " ('slot:alot', 1609),\n",
       " ('want:wasnt', 1175),\n",
       " ('wellmade:wellmade', 1169),\n",
       " ('couldnt:wouldnt', 1055),\n",
       " ('price:pricey', 945),\n",
       " ('17:17\"', 883),\n",
       " ('15:15\"', 672),\n",
       " ('Haven:Havent', 635),\n",
       " ('21:21\"', 530),\n",
       " ('a:¿', 483),\n",
       " ('25:25\"', 477),\n",
       " ('in:im', 449),\n",
       " ('packing:repacking', 420),\n",
       " ('or:yr', 405),\n",
       " ('under:undies', 403),\n",
       " ('definitely:definately', 382),\n",
       " ('I:I¿m', 358),\n",
       " ('Spain:Spain', 340),\n",
       " ('France:France', 339),\n",
       " ('accommodate:accomodate', 321),\n",
       " ('bag:bag59', 317),\n",
       " (\"bags:bag's\", 309),\n",
       " ('welldesigned:welldesigned', 309),\n",
       " ('13:13\"', 308),\n",
       " ('stuff:\"stuff\"', 295),\n",
       " ('baggallini:Baggalini', 291),\n",
       " ('pick:picky', 284),\n",
       " ('its:it¿s', 284),\n",
       " ('Its:It¿s', 281),\n",
       " ('June:June', 278),\n",
       " ('unable:unusable', 273),\n",
       " ('chin:chka', 272),\n",
       " ('Vegas:Vegas', 269),\n",
       " ('have:I¿ve', 267),\n",
       " ('unmanageable:manageable', 266),\n",
       " ('cream:cram', 262),\n",
       " ('girlfriend:girlfriend', 250),\n",
       " ('September:September', 247),\n",
       " ('yes:yrs', 245),\n",
       " ('heater:neater', 241),\n",
       " ('dont:don¿t', 238),\n",
       " ('overlock:overpack', 236),\n",
       " ('roominess:roominess', 225),\n",
       " ('disappointed:disappoint', 224),\n",
       " ('July:July', 222),\n",
       " ('received:recieved', 213),\n",
       " ('29:29\"', 213),\n",
       " ('downside:downsize', 213),\n",
       " ('January:January', 211),\n",
       " ('eggplant:eggplant', 206),\n",
       " ('backpack:backback', 206),\n",
       " ('bag:bag\"', 204),\n",
       " ('stopped:shopped', 204),\n",
       " ('goodlooking:goodlooking', 203),\n",
       " ('pc:3pc', 202),\n",
       " ('22:22\"', 200),\n",
       " ('wellconstructed:wellconstructed', 189),\n",
       " ('researcher:researched', 181),\n",
       " ('recommend:recomend', 180),\n",
       " ('steadiness:sturdiness', 179),\n",
       " ('bag:bagI', 179),\n",
       " ('daughterinlaw:daughterinlaw', 178),\n",
       " ('Motherlode:Motherlode', 177),\n",
       " ('grip:gripe', 175),\n",
       " ('coworkers:coworker', 175),\n",
       " ('give:ive', 174),\n",
       " ('20:20\"', 172),\n",
       " ('173:173\"', 170),\n",
       " ('recommend:reccomend', 168),\n",
       " ('14:14\"', 168),\n",
       " ('November:November', 167),\n",
       " ('soninlaw:soninlaw', 166),\n",
       " (\"':5'\", 164),\n",
       " ('schooner:schooler', 159),\n",
       " ('lifesaver:lifesaver', 158),\n",
       " ('week:2week', 157),\n",
       " ('overtime:everytime', 156),\n",
       " ('were:werent', 153),\n",
       " ('negative:negatives', 153),\n",
       " ('Cost:Costa', 153),\n",
       " (\"'im:i'm\", 152),\n",
       " ('Rico:Rica', 151),\n",
       " ('rummaged:rummage', 149),\n",
       " ('the:tho', 147),\n",
       " ('Scotland:Scotland', 147),\n",
       " ('grandis:grandkids', 144),\n",
       " ('baggallini:baggalini', 143),\n",
       " ('housework:homework', 141),\n",
       " ('he:hes', 140),\n",
       " ('Did:Didnt', 140),\n",
       " ('to:yo', 139),\n",
       " ('Las:Las', 139)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrections_count.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
