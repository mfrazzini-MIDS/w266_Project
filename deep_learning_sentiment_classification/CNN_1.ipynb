{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN_1 Basic Model with NB-BoW Baseline and comparable CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os, sys, re, json, time, datetime, shutil, csv\n",
    "import itertools, collections\n",
    "from importlib import reload\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.8\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS_SENTIMENT\n",
      "0    213\n",
      "1    826\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load labeled data set\n",
    "review_sentences = pd.read_csv('./data/Feature_sentences_Prod_Final_2018-07-09_Labeled_1K.csv', encoding='utf-8')\n",
    "# Distribution by Sentiment Class\n",
    "print(review_sentences.groupby('POS_SENTIMENT').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (830, 4)\n",
      "test shape: (209, 4)\n"
     ]
    }
   ],
   "source": [
    "# Create training and test sets, keeping 80:20 ratio of sentiment class\n",
    "dataset_class_0 = review_sentences[review_sentences['POS_SENTIMENT'] == 0]\n",
    "dataset_class_1 = review_sentences[review_sentences['POS_SENTIMENT'] == 1]\n",
    "\n",
    "#train = 170:660 = 830\n",
    "#test = 43:166 = 209\n",
    "#               == 1,039\n",
    "\n",
    "train = dataset_class_0.iloc[0:170,]\n",
    "train = train.append(dataset_class_1.iloc[0:660,])\n",
    "print('train shape:', train.shape)\n",
    "\n",
    "test = dataset_class_0.iloc[170:213,]\n",
    "test = test.append(dataset_class_1.iloc[660:826,])\n",
    "print('test shape:', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train['SENTENCE']\n",
    "train_y = train['POS_SENTIMENT']\n",
    "test_x = test['SENTENCE']\n",
    "test_y = test['POS_SENTIMENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences\n",
    "\n",
    "train_x_tokens = []\n",
    "test_x_tokens = []\n",
    "\n",
    "for x in train_x:\n",
    "    train_x_tokens.append(word_tokenize(x))\n",
    "    \n",
    "for x in test_x:\n",
    "    test_x_tokens.append(word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1,699\n"
     ]
    }
   ],
   "source": [
    "# define vocabulary using w266 common vocab\n",
    "\n",
    "train_text_all = [item for sublist in train_x_tokens for item in sublist]\n",
    "\n",
    "vocab = vocabulary.Vocabulary(train_text_all, size=20000)\n",
    "print(\"Vocabulary size: {:,}\".format(vocab.size))\n",
    "#print(\"Vocabulary dict: \", vocab.word_to_id)\n",
    "\n",
    "train_x_ids = []\n",
    "test_x_ids = []\n",
    "\n",
    "for x in train_x_tokens:\n",
    "    train_x_ids.append(vocab.words_to_ids(x))\n",
    "    \n",
    "for x in test_x_tokens:\n",
    "    test_x_ids.append(vocab.words_to_ids(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating id lists...\n",
      "starting id lists to sparse bow conversion...\n",
      "training Multinomial Naive Bayes for simple baseline model...\n",
      "Accuracy on test set: 84.21%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.42      0.52        43\n",
      "          1       0.86      0.95      0.91       166\n",
      "\n",
      "avg / total       0.83      0.84      0.83       209\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NB - BoW Baseline\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_x_fdict = []\n",
    "test_x_fdict = []\n",
    "\n",
    "for x in train_x_ids:\n",
    "    train_x_fdict.append(collections.Counter(x))\n",
    "    \n",
    "for x in test_x_ids:\n",
    "    test_x_fdict.append(collections.Counter(x))\n",
    "\n",
    "train_x_vector = []\n",
    "test_x_vector = []\n",
    "\n",
    "num_features = vocab.size\n",
    "print('creating id lists...')\n",
    "for x in train_x_fdict:\n",
    "    train_x_vector.append([x.get(i, 0) for i in range(num_features)])\n",
    "    \n",
    "for x in test_x_fdict:\n",
    "    test_x_vector.append([x.get(i, 0) for i in range(num_features)])\n",
    "\n",
    "# use w266 common utils to convert id lists to sparse bow matrix\n",
    "print('starting id lists to sparse bow conversion...')\n",
    "train_x_sparse_bow = utils.id_lists_to_sparse_bow(train_x_fdict, vocab.size)\n",
    "test_x_sparse_bow = utils.id_lists_to_sparse_bow(test_x_fdict, vocab.size)\n",
    "\n",
    "# training Multinomial Naive Bayes for simple baseline model\n",
    "print('training Multinomial Naive Bayes for simple baseline model...')\n",
    "nb = MultinomialNB()\n",
    "nb.fit(train_x_sparse_bow, train_y)\n",
    "y_pred = nb.predict(test_x_sparse_bow)\n",
    "\n",
    "acc = accuracy_score(test_y, y_pred)\n",
    "print(\"Accuracy on test set: {:.02%}\".format(acc))\n",
    "classrep = classification_report(test_y, y_pred)\n",
    "print(classrep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for CNN model\n",
    "\n",
    "from sklearn import preprocessing\n",
    "# pad all id vectors to max length for Neural model\n",
    "\n",
    "max_comment_length = max([len(x) for x in train_x_ids])\n",
    "\n",
    "train_x_neural = train_x_ids\n",
    "test_x_neural = test_x_ids\n",
    "\n",
    "for x in train_x_neural:\n",
    "    if len(x) < max_comment_length:\n",
    "        for n in range(max_comment_length - len(x)):\n",
    "            x.append(1)\n",
    "    elif len(x) > max_comment_length:\n",
    "        x = x[:max_comment_length]\n",
    "        \n",
    "for x in test_x_neural:\n",
    "    if len(x) < max_comment_length:\n",
    "        for n in range(max_comment_length - len(x)):\n",
    "            x.append(1)\n",
    "    elif len(x) > max_comment_length:\n",
    "        x = x[:max_comment_length]\n",
    "\n",
    "# convert labels to one-hot vectors for Neural model \n",
    "\n",
    "terms_list = [[0],[1]]\n",
    "lb = preprocessing.MultiLabelBinarizer()\n",
    "lb.fit(terms_list)\n",
    "\n",
    "train_labels = np.array(train_y.tolist())\n",
    "test_labels = np.array(test_y.tolist())\n",
    "\n",
    "train_lables_format = [train_labels[i:i+1] for i in range(0, len(train_labels), 1)]    \n",
    "test_lables_format = [test_labels[i:i+1] for i in range(0, len(test_labels), 1)]\n",
    "\n",
    "train_y_neural = lb.transform(train_lables_format)\n",
    "test_y_neural = lb.transform(test_lables_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=<absl.flags._flag.BooleanFlag object at 0x7fa40fa566d8>\n",
      "BATCH_SIZE=<absl.flags._flag.Flag object at 0x7fa40fa56c88>\n",
      "CHECKPOINT_EVERY=<absl.flags._flag.Flag object at 0x7fa40fa56828>\n",
      "DROPOUT_KEEP_PROB=<absl.flags._flag.Flag object at 0x7fa40fa56b70>\n",
      "EMBEDDING_DIM=<absl.flags._flag.Flag object at 0x7fa40fa56cc0>\n",
      "EVALUATE_EVERY=<absl.flags._flag.Flag object at 0x7fa40fa568d0>\n",
      "F=<absl.flags._flag.Flag object at 0x7fa40fa50dd8>\n",
      "FILTER_SIZES=<absl.flags._flag.Flag object at 0x7fa40fa56da0>\n",
      "L2_REG_LAMBDA=<absl.flags._flag.Flag object at 0x7fa40fa56b38>\n",
      "LOG_DEVICE_PLACEMENT=<absl.flags._flag.BooleanFlag object at 0x7fa40fa566a0>\n",
      "NUM_CHECKPOINTS=<absl.flags._flag.Flag object at 0x7fa40fa56780>\n",
      "NUM_EPOCHS=<absl.flags._flag.Flag object at 0x7fa40fa56be0>\n",
      "NUM_FILTERS=<absl.flags._flag.Flag object at 0x7fa40fa56d68>\n",
      "\n",
      "x_train (830, 71)\n",
      "y_train (830, 2)\n",
      "x_dev (209, 71)\n",
      "x_dev (209, 2)\n",
      "Vocabulary Size: 1699\n",
      "Train/Dev split: 830/209\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533921767\n",
      "\n",
      "2018-08-10T17:22:48.561234: step 1, loss 4.87387, acc 0.265625\n",
      "2018-08-10T17:22:48.686057: step 2, loss 2.3335, acc 0.5\n",
      "2018-08-10T17:22:48.807553: step 3, loss 1.60371, acc 0.5625\n",
      "2018-08-10T17:22:48.922082: step 4, loss 1.468, acc 0.71875\n",
      "2018-08-10T17:22:49.044016: step 5, loss 0.800965, acc 0.8125\n",
      "2018-08-10T17:22:49.175787: step 6, loss 1.18167, acc 0.828125\n",
      "2018-08-10T17:22:49.317073: step 7, loss 1.76552, acc 0.75\n",
      "2018-08-10T17:22:49.448662: step 8, loss 1.2491, acc 0.8125\n",
      "2018-08-10T17:22:49.576886: step 9, loss 2.47015, acc 0.703125\n",
      "2018-08-10T17:22:49.717155: step 10, loss 1.89978, acc 0.78125\n",
      "2018-08-10T17:22:49.858125: step 11, loss 2.2838, acc 0.71875\n",
      "2018-08-10T17:22:49.997860: step 12, loss 0.773939, acc 0.875\n",
      "2018-08-10T17:22:50.136772: step 13, loss 1.35716, acc 0.790323\n",
      "2018-08-10T17:22:50.277064: step 14, loss 1.12014, acc 0.796875\n",
      "2018-08-10T17:22:50.418821: step 15, loss 0.686268, acc 0.84375\n",
      "2018-08-10T17:22:50.560105: step 16, loss 1.27561, acc 0.734375\n",
      "2018-08-10T17:22:50.700404: step 17, loss 0.916245, acc 0.8125\n",
      "2018-08-10T17:22:50.839811: step 18, loss 0.875155, acc 0.75\n",
      "2018-08-10T17:22:50.979703: step 19, loss 0.669619, acc 0.8125\n",
      "2018-08-10T17:22:51.118171: step 20, loss 1.28017, acc 0.609375\n",
      "2018-08-10T17:22:51.260254: step 21, loss 1.00002, acc 0.703125\n",
      "2018-08-10T17:22:51.401397: step 22, loss 1.06735, acc 0.6875\n",
      "2018-08-10T17:22:51.544702: step 23, loss 0.865287, acc 0.71875\n",
      "2018-08-10T17:22:51.687235: step 24, loss 0.753402, acc 0.796875\n",
      "2018-08-10T17:22:51.827113: step 25, loss 0.864931, acc 0.765625\n",
      "2018-08-10T17:22:51.965588: step 26, loss 0.917462, acc 0.693548\n",
      "2018-08-10T17:22:52.106326: step 27, loss 1.07483, acc 0.734375\n",
      "2018-08-10T17:22:52.247763: step 28, loss 1.02905, acc 0.71875\n",
      "2018-08-10T17:22:52.388551: step 29, loss 0.481541, acc 0.8125\n",
      "2018-08-10T17:22:52.528977: step 30, loss 0.636528, acc 0.8125\n",
      "2018-08-10T17:22:52.674890: step 31, loss 1.26856, acc 0.65625\n",
      "2018-08-10T17:22:52.820726: step 32, loss 0.747813, acc 0.796875\n",
      "2018-08-10T17:22:52.965601: step 33, loss 0.809706, acc 0.75\n",
      "2018-08-10T17:22:53.110368: step 34, loss 0.905265, acc 0.8125\n",
      "2018-08-10T17:22:53.256981: step 35, loss 1.36226, acc 0.71875\n",
      "2018-08-10T17:22:53.402334: step 36, loss 0.591806, acc 0.8125\n",
      "2018-08-10T17:22:53.552025: step 37, loss 0.527115, acc 0.875\n",
      "2018-08-10T17:22:53.701778: step 38, loss 0.528164, acc 0.796875\n",
      "2018-08-10T17:22:53.843476: step 39, loss 1.13156, acc 0.725806\n",
      "2018-08-10T17:22:53.989684: step 40, loss 0.46165, acc 0.875\n",
      "2018-08-10T17:22:54.135214: step 41, loss 0.704316, acc 0.8125\n",
      "2018-08-10T17:22:54.282401: step 42, loss 0.454748, acc 0.8125\n",
      "2018-08-10T17:22:54.430083: step 43, loss 0.60281, acc 0.84375\n",
      "2018-08-10T17:22:54.574365: step 44, loss 0.803248, acc 0.78125\n",
      "2018-08-10T17:22:54.723374: step 45, loss 0.789049, acc 0.75\n",
      "2018-08-10T17:22:54.871321: step 46, loss 0.354484, acc 0.859375\n",
      "2018-08-10T17:22:55.017668: step 47, loss 0.782708, acc 0.78125\n",
      "2018-08-10T17:22:55.163677: step 48, loss 0.515579, acc 0.859375\n",
      "2018-08-10T17:22:55.312376: step 49, loss 0.526928, acc 0.8125\n",
      "2018-08-10T17:22:55.461257: step 50, loss 0.68988, acc 0.765625\n",
      "2018-08-10T17:22:55.611356: step 51, loss 0.466234, acc 0.828125\n",
      "2018-08-10T17:22:55.753879: step 52, loss 0.846669, acc 0.774194\n",
      "2018-08-10T17:22:55.900576: step 53, loss 0.507004, acc 0.875\n",
      "2018-08-10T17:22:56.047047: step 54, loss 0.62087, acc 0.84375\n",
      "2018-08-10T17:22:56.194528: step 55, loss 0.394333, acc 0.828125\n",
      "2018-08-10T17:22:56.340742: step 56, loss 0.339503, acc 0.921875\n",
      "2018-08-10T17:22:56.489760: step 57, loss 0.397856, acc 0.921875\n",
      "2018-08-10T17:22:56.638550: step 58, loss 0.36198, acc 0.890625\n",
      "2018-08-10T17:22:56.786279: step 59, loss 0.338552, acc 0.84375\n",
      "2018-08-10T17:22:56.935775: step 60, loss 0.26211, acc 0.921875\n",
      "2018-08-10T17:22:57.080960: step 61, loss 0.955774, acc 0.75\n",
      "2018-08-10T17:22:57.228121: step 62, loss 0.54977, acc 0.84375\n",
      "2018-08-10T17:22:57.373746: step 63, loss 0.721053, acc 0.75\n",
      "2018-08-10T17:22:57.522841: step 64, loss 0.842928, acc 0.734375\n",
      "2018-08-10T17:22:57.666982: step 65, loss 0.480785, acc 0.806452\n",
      "2018-08-10T17:22:57.812262: step 66, loss 0.228059, acc 0.90625\n",
      "2018-08-10T17:22:57.965261: step 67, loss 0.569555, acc 0.828125\n",
      "2018-08-10T17:22:58.113109: step 68, loss 0.415442, acc 0.875\n",
      "2018-08-10T17:22:58.260981: step 69, loss 0.409104, acc 0.890625\n",
      "2018-08-10T17:22:58.406939: step 70, loss 0.452007, acc 0.859375\n",
      "2018-08-10T17:22:58.554082: step 71, loss 0.288335, acc 0.90625\n",
      "2018-08-10T17:22:58.702551: step 72, loss 0.32011, acc 0.84375\n",
      "2018-08-10T17:22:58.848742: step 73, loss 0.281648, acc 0.875\n",
      "2018-08-10T17:22:58.994816: step 74, loss 0.331524, acc 0.90625\n",
      "2018-08-10T17:22:59.141358: step 75, loss 0.363216, acc 0.890625\n",
      "2018-08-10T17:22:59.288601: step 76, loss 0.307025, acc 0.875\n",
      "2018-08-10T17:22:59.442904: step 77, loss 0.485499, acc 0.875\n",
      "2018-08-10T17:22:59.583636: step 78, loss 0.194903, acc 0.887097\n",
      "2018-08-10T17:22:59.730536: step 79, loss 0.304825, acc 0.890625\n",
      "2018-08-10T17:22:59.875645: step 80, loss 0.519922, acc 0.828125\n",
      "2018-08-10T17:23:00.022101: step 81, loss 0.388381, acc 0.875\n",
      "2018-08-10T17:23:00.168910: step 82, loss 0.29443, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:23:00.315725: step 83, loss 0.238922, acc 0.921875\n",
      "2018-08-10T17:23:00.463899: step 84, loss 0.208937, acc 0.890625\n",
      "2018-08-10T17:23:00.608989: step 85, loss 0.223871, acc 0.90625\n",
      "2018-08-10T17:23:00.755720: step 86, loss 0.335182, acc 0.921875\n",
      "2018-08-10T17:23:00.904718: step 87, loss 0.420747, acc 0.890625\n",
      "2018-08-10T17:23:01.050558: step 88, loss 0.212601, acc 0.90625\n",
      "2018-08-10T17:23:01.196805: step 89, loss 0.270165, acc 0.890625\n",
      "2018-08-10T17:23:01.342304: step 90, loss 0.28581, acc 0.859375\n",
      "2018-08-10T17:23:01.490378: step 91, loss 0.233482, acc 0.903226\n",
      "2018-08-10T17:23:01.638439: step 92, loss 0.134244, acc 0.953125\n",
      "2018-08-10T17:23:01.784425: step 93, loss 0.324402, acc 0.84375\n",
      "2018-08-10T17:23:01.929684: step 94, loss 0.367473, acc 0.859375\n",
      "2018-08-10T17:23:02.073732: step 95, loss 0.325345, acc 0.90625\n",
      "2018-08-10T17:23:02.218620: step 96, loss 0.334747, acc 0.9375\n",
      "2018-08-10T17:23:02.363697: step 97, loss 0.448483, acc 0.859375\n",
      "2018-08-10T17:23:02.509236: step 98, loss 0.2947, acc 0.859375\n",
      "2018-08-10T17:23:02.655981: step 99, loss 0.187162, acc 0.90625\n",
      "2018-08-10T17:23:02.798460: step 100, loss 0.397232, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:23:02.974909: step 100, loss 0.70094, acc 0.808612\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533921767/checkpoints/model-100\n",
      "\n",
      "2018-08-10T17:23:03.176922: step 101, loss 0.479467, acc 0.828125\n",
      "2018-08-10T17:23:03.322139: step 102, loss 0.312567, acc 0.859375\n",
      "2018-08-10T17:23:03.468327: step 103, loss 0.138055, acc 0.953125\n",
      "2018-08-10T17:23:03.611629: step 104, loss 0.163155, acc 0.919355\n",
      "2018-08-10T17:23:03.755578: step 105, loss 0.153991, acc 0.953125\n",
      "2018-08-10T17:23:03.900885: step 106, loss 0.268919, acc 0.875\n",
      "2018-08-10T17:23:04.045393: step 107, loss 0.430736, acc 0.828125\n",
      "2018-08-10T17:23:04.186858: step 108, loss 0.29596, acc 0.890625\n",
      "2018-08-10T17:23:04.332575: step 109, loss 0.271765, acc 0.9375\n",
      "2018-08-10T17:23:04.479228: step 110, loss 0.339889, acc 0.875\n",
      "2018-08-10T17:23:04.626725: step 111, loss 0.316633, acc 0.921875\n",
      "2018-08-10T17:23:04.773687: step 112, loss 0.289037, acc 0.90625\n",
      "2018-08-10T17:23:04.920962: step 113, loss 0.308012, acc 0.875\n",
      "2018-08-10T17:23:05.068135: step 114, loss 0.193412, acc 0.875\n",
      "2018-08-10T17:23:05.216989: step 115, loss 0.281607, acc 0.90625\n",
      "2018-08-10T17:23:05.365370: step 116, loss 0.261572, acc 0.875\n",
      "2018-08-10T17:23:05.509179: step 117, loss 0.201716, acc 0.887097\n",
      "2018-08-10T17:23:05.658216: step 118, loss 0.0510273, acc 0.984375\n",
      "2018-08-10T17:23:05.806433: step 119, loss 0.0955788, acc 0.96875\n",
      "2018-08-10T17:23:05.952398: step 120, loss 0.12434, acc 0.921875\n",
      "2018-08-10T17:23:06.103570: step 121, loss 0.318235, acc 0.875\n",
      "2018-08-10T17:23:06.253161: step 122, loss 0.112905, acc 0.953125\n",
      "2018-08-10T17:23:06.401365: step 123, loss 0.27911, acc 0.921875\n",
      "2018-08-10T17:23:06.549000: step 124, loss 0.189893, acc 0.921875\n",
      "2018-08-10T17:23:06.699209: step 125, loss 0.225287, acc 0.9375\n",
      "2018-08-10T17:23:06.843619: step 126, loss 0.179974, acc 0.90625\n",
      "2018-08-10T17:23:06.990847: step 127, loss 0.242584, acc 0.921875\n",
      "2018-08-10T17:23:07.141724: step 128, loss 0.138238, acc 0.921875\n",
      "2018-08-10T17:23:07.290596: step 129, loss 0.353603, acc 0.84375\n",
      "2018-08-10T17:23:07.439787: step 130, loss 0.170356, acc 0.919355\n",
      "2018-08-10T17:23:07.585576: step 131, loss 0.107981, acc 0.953125\n",
      "2018-08-10T17:23:07.732907: step 132, loss 0.115176, acc 0.953125\n",
      "2018-08-10T17:23:07.880050: step 133, loss 0.0914064, acc 0.96875\n",
      "2018-08-10T17:23:08.027145: step 134, loss 0.18651, acc 0.9375\n",
      "2018-08-10T17:23:08.172427: step 135, loss 0.116746, acc 0.890625\n",
      "2018-08-10T17:23:08.322106: step 136, loss 0.135494, acc 0.96875\n",
      "2018-08-10T17:23:08.466582: step 137, loss 0.154659, acc 0.921875\n",
      "2018-08-10T17:23:08.613948: step 138, loss 0.130021, acc 0.953125\n",
      "2018-08-10T17:23:08.759913: step 139, loss 0.0917159, acc 0.953125\n",
      "2018-08-10T17:23:08.902943: step 140, loss 0.263761, acc 0.90625\n",
      "2018-08-10T17:23:09.042713: step 141, loss 0.0925266, acc 0.96875\n",
      "2018-08-10T17:23:09.179840: step 142, loss 0.15478, acc 0.96875\n",
      "2018-08-10T17:23:09.315398: step 143, loss 0.391039, acc 0.870968\n",
      "2018-08-10T17:23:09.457210: step 144, loss 0.0777067, acc 0.96875\n",
      "2018-08-10T17:23:09.598187: step 145, loss 0.109481, acc 0.953125\n",
      "2018-08-10T17:23:09.736777: step 146, loss 0.174941, acc 0.890625\n",
      "2018-08-10T17:23:09.874460: step 147, loss 0.13395, acc 0.96875\n",
      "2018-08-10T17:23:10.012979: step 148, loss 0.100859, acc 0.96875\n",
      "2018-08-10T17:23:10.150017: step 149, loss 0.127673, acc 0.96875\n",
      "2018-08-10T17:23:10.288381: step 150, loss 0.0637439, acc 0.984375\n",
      "2018-08-10T17:23:10.427957: step 151, loss 0.139362, acc 0.953125\n",
      "2018-08-10T17:23:10.565210: step 152, loss 0.101198, acc 0.96875\n",
      "2018-08-10T17:23:10.703763: step 153, loss 0.134812, acc 0.9375\n",
      "2018-08-10T17:23:10.842202: step 154, loss 0.151291, acc 0.96875\n",
      "2018-08-10T17:23:10.983820: step 155, loss 0.227067, acc 0.9375\n",
      "2018-08-10T17:23:11.117637: step 156, loss 0.281974, acc 0.887097\n",
      "2018-08-10T17:23:11.257187: step 157, loss 0.130632, acc 0.9375\n",
      "2018-08-10T17:23:11.397265: step 158, loss 0.151352, acc 0.90625\n",
      "2018-08-10T17:23:11.537268: step 159, loss 0.198433, acc 0.921875\n",
      "2018-08-10T17:23:11.677501: step 160, loss 0.0922439, acc 0.953125\n",
      "2018-08-10T17:23:11.817902: step 161, loss 0.0559396, acc 0.984375\n",
      "2018-08-10T17:23:11.955184: step 162, loss 0.0486295, acc 0.984375\n",
      "2018-08-10T17:23:12.092371: step 163, loss 0.167671, acc 0.9375\n",
      "2018-08-10T17:23:12.231742: step 164, loss 0.106099, acc 0.953125\n",
      "2018-08-10T17:23:12.372603: step 165, loss 0.274348, acc 0.890625\n",
      "2018-08-10T17:23:12.512217: step 166, loss 0.15123, acc 0.953125\n",
      "2018-08-10T17:23:12.653100: step 167, loss 0.0610981, acc 0.984375\n",
      "2018-08-10T17:23:12.790944: step 168, loss 0.154586, acc 0.953125\n",
      "2018-08-10T17:23:12.925486: step 169, loss 0.242074, acc 0.951613\n",
      "2018-08-10T17:23:13.063699: step 170, loss 0.0771248, acc 0.953125\n",
      "2018-08-10T17:23:13.202673: step 171, loss 0.188245, acc 0.9375\n",
      "2018-08-10T17:23:13.343605: step 172, loss 0.203132, acc 0.90625\n",
      "2018-08-10T17:23:13.485580: step 173, loss 0.0727296, acc 0.96875\n",
      "2018-08-10T17:23:13.632626: step 174, loss 0.086641, acc 0.9375\n",
      "2018-08-10T17:23:13.780467: step 175, loss 0.028192, acc 0.984375\n",
      "2018-08-10T17:23:13.926437: step 176, loss 0.101526, acc 0.984375\n",
      "2018-08-10T17:23:14.070596: step 177, loss 0.206176, acc 0.921875\n",
      "2018-08-10T17:23:14.216224: step 178, loss 0.104348, acc 0.96875\n",
      "2018-08-10T17:23:14.362430: step 179, loss 0.189213, acc 0.921875\n",
      "2018-08-10T17:23:14.509171: step 180, loss 0.0627969, acc 0.96875\n",
      "2018-08-10T17:23:14.656472: step 181, loss 0.0389518, acc 0.984375\n",
      "2018-08-10T17:23:14.797121: step 182, loss 0.0407309, acc 0.983871\n",
      "2018-08-10T17:23:14.941344: step 183, loss 0.105898, acc 0.953125\n",
      "2018-08-10T17:23:15.084094: step 184, loss 0.0644921, acc 0.96875\n",
      "2018-08-10T17:23:15.231665: step 185, loss 0.21316, acc 0.921875\n",
      "2018-08-10T17:23:15.379471: step 186, loss 0.206982, acc 0.9375\n",
      "2018-08-10T17:23:15.526286: step 187, loss 0.0621049, acc 0.984375\n",
      "2018-08-10T17:23:15.674167: step 188, loss 0.231831, acc 0.953125\n",
      "2018-08-10T17:23:15.817525: step 189, loss 0.152447, acc 0.953125\n",
      "2018-08-10T17:23:15.960422: step 190, loss 0.0428874, acc 0.96875\n",
      "2018-08-10T17:23:16.103735: step 191, loss 0.155141, acc 0.9375\n",
      "2018-08-10T17:23:16.247854: step 192, loss 0.164688, acc 0.921875\n",
      "2018-08-10T17:23:16.391299: step 193, loss 0.110231, acc 0.921875\n",
      "2018-08-10T17:23:16.536228: step 194, loss 0.058408, acc 0.984375\n",
      "2018-08-10T17:23:16.678921: step 195, loss 0.0349844, acc 1\n",
      "2018-08-10T17:23:16.823841: step 196, loss 0.0978892, acc 0.953125\n",
      "2018-08-10T17:23:16.971322: step 197, loss 0.136931, acc 0.9375\n",
      "2018-08-10T17:23:17.115351: step 198, loss 0.0622594, acc 0.96875\n",
      "2018-08-10T17:23:17.261817: step 199, loss 0.0528323, acc 1\n",
      "2018-08-10T17:23:17.409235: step 200, loss 0.118928, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:23:17.555375: step 200, loss 0.560571, acc 0.832536\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533921767/checkpoints/model-200\n",
      "\n",
      "2018-08-10T17:23:17.758679: step 201, loss 0.039111, acc 0.984375\n",
      "2018-08-10T17:23:17.906480: step 202, loss 0.08424, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:23:18.052475: step 203, loss 0.0476842, acc 0.984375\n",
      "2018-08-10T17:23:18.197471: step 204, loss 0.078586, acc 0.96875\n",
      "2018-08-10T17:23:18.344367: step 205, loss 0.101268, acc 0.9375\n",
      "2018-08-10T17:23:18.489857: step 206, loss 0.105888, acc 0.96875\n",
      "2018-08-10T17:23:18.636840: step 207, loss 0.22578, acc 0.890625\n",
      "2018-08-10T17:23:18.781699: step 208, loss 0.12881, acc 0.983871\n",
      "2018-08-10T17:23:18.929561: step 209, loss 0.0964404, acc 0.96875\n",
      "2018-08-10T17:23:19.075859: step 210, loss 0.160909, acc 0.953125\n",
      "2018-08-10T17:23:19.221835: step 211, loss 0.0839287, acc 0.953125\n",
      "2018-08-10T17:23:19.366257: step 212, loss 0.158134, acc 0.953125\n",
      "2018-08-10T17:23:19.514031: step 213, loss 0.0542743, acc 0.96875\n",
      "2018-08-10T17:23:19.660571: step 214, loss 0.105081, acc 0.953125\n",
      "2018-08-10T17:23:19.805278: step 215, loss 0.129346, acc 0.96875\n",
      "2018-08-10T17:23:19.950824: step 216, loss 0.0509261, acc 0.984375\n",
      "2018-08-10T17:23:20.097222: step 217, loss 0.0937956, acc 0.96875\n",
      "2018-08-10T17:23:20.241260: step 218, loss 0.116321, acc 0.953125\n",
      "2018-08-10T17:23:20.386698: step 219, loss 0.101524, acc 0.953125\n",
      "2018-08-10T17:23:20.531631: step 220, loss 0.0573555, acc 0.984375\n",
      "2018-08-10T17:23:20.671451: step 221, loss 0.129229, acc 0.967742\n",
      "2018-08-10T17:23:20.818581: step 222, loss 0.172061, acc 0.96875\n",
      "2018-08-10T17:23:20.961903: step 223, loss 0.0422195, acc 0.984375\n",
      "2018-08-10T17:23:21.108749: step 224, loss 0.047248, acc 0.984375\n",
      "2018-08-10T17:23:21.255132: step 225, loss 0.0440037, acc 0.984375\n",
      "2018-08-10T17:23:21.400169: step 226, loss 0.0186222, acc 0.984375\n",
      "2018-08-10T17:23:21.549951: step 227, loss 0.0906208, acc 0.953125\n",
      "2018-08-10T17:23:21.699373: step 228, loss 0.12688, acc 0.953125\n",
      "2018-08-10T17:23:21.845422: step 229, loss 0.0397361, acc 0.984375\n",
      "2018-08-10T17:23:21.992872: step 230, loss 0.1984, acc 0.96875\n",
      "2018-08-10T17:23:22.139174: step 231, loss 0.0374887, acc 0.984375\n",
      "2018-08-10T17:23:22.283033: step 232, loss 0.0395578, acc 0.984375\n",
      "2018-08-10T17:23:22.430364: step 233, loss 0.0616726, acc 0.96875\n",
      "2018-08-10T17:23:22.570446: step 234, loss 0.067832, acc 0.967742\n",
      "2018-08-10T17:23:22.715059: step 235, loss 0.0525091, acc 0.96875\n",
      "2018-08-10T17:23:22.862141: step 236, loss 0.0610979, acc 0.984375\n",
      "2018-08-10T17:23:23.008613: step 237, loss 0.0924766, acc 0.96875\n",
      "2018-08-10T17:23:23.154204: step 238, loss 0.229443, acc 0.953125\n",
      "2018-08-10T17:23:23.298094: step 239, loss 0.0252583, acc 0.984375\n",
      "2018-08-10T17:23:23.447264: step 240, loss 0.0685455, acc 0.96875\n",
      "2018-08-10T17:23:23.593729: step 241, loss 0.0275187, acc 1\n",
      "2018-08-10T17:23:23.740445: step 242, loss 0.0698902, acc 0.96875\n",
      "2018-08-10T17:23:23.879932: step 243, loss 0.0679109, acc 0.984375\n",
      "2018-08-10T17:23:24.018906: step 244, loss 0.083249, acc 0.953125\n",
      "2018-08-10T17:23:24.163405: step 245, loss 0.0766372, acc 0.96875\n",
      "2018-08-10T17:23:24.308167: step 246, loss 0.0237689, acc 0.984375\n",
      "2018-08-10T17:23:24.451249: step 247, loss 0.0259756, acc 1\n",
      "2018-08-10T17:23:24.598826: step 248, loss 0.0277382, acc 1\n",
      "2018-08-10T17:23:24.745972: step 249, loss 0.0326424, acc 0.984375\n",
      "2018-08-10T17:23:24.893081: step 250, loss 0.117673, acc 0.96875\n",
      "2018-08-10T17:23:25.040822: step 251, loss 0.208718, acc 0.90625\n",
      "2018-08-10T17:23:25.188032: step 252, loss 0.0760099, acc 0.96875\n",
      "2018-08-10T17:23:25.338168: step 253, loss 0.0624913, acc 0.984375\n",
      "2018-08-10T17:23:25.488129: step 254, loss 0.159024, acc 0.90625\n",
      "2018-08-10T17:23:25.638076: step 255, loss 0.0261271, acc 1\n",
      "2018-08-10T17:23:25.785355: step 256, loss 0.0637723, acc 0.96875\n",
      "2018-08-10T17:23:25.932557: step 257, loss 0.123185, acc 0.9375\n",
      "2018-08-10T17:23:26.078837: step 258, loss 0.0989151, acc 0.953125\n",
      "2018-08-10T17:23:26.227436: step 259, loss 0.0497332, acc 0.96875\n",
      "2018-08-10T17:23:26.377565: step 260, loss 0.0324686, acc 0.983871\n",
      "2018-08-10T17:23:26.529747: step 261, loss 0.114568, acc 0.96875\n",
      "2018-08-10T17:23:26.678005: step 262, loss 0.116551, acc 0.953125\n",
      "2018-08-10T17:23:26.825986: step 263, loss 0.148723, acc 0.96875\n",
      "2018-08-10T17:23:26.974387: step 264, loss 0.0472116, acc 0.96875\n",
      "2018-08-10T17:23:27.120725: step 265, loss 0.0291087, acc 0.984375\n",
      "2018-08-10T17:23:27.273447: step 266, loss 0.120838, acc 0.953125\n",
      "2018-08-10T17:23:27.423365: step 267, loss 0.00448736, acc 1\n",
      "2018-08-10T17:23:27.574758: step 268, loss 0.136026, acc 0.953125\n",
      "2018-08-10T17:23:27.723063: step 269, loss 0.105589, acc 0.953125\n",
      "2018-08-10T17:23:27.870951: step 270, loss 0.181447, acc 0.9375\n",
      "2018-08-10T17:23:28.019566: step 271, loss 0.0899354, acc 0.96875\n",
      "2018-08-10T17:23:28.166096: step 272, loss 0.0484015, acc 0.984375\n",
      "2018-08-10T17:23:28.312333: step 273, loss 0.0300974, acc 0.983871\n",
      "2018-08-10T17:23:28.460298: step 274, loss 0.107801, acc 0.9375\n",
      "2018-08-10T17:23:28.607616: step 275, loss 0.0122845, acc 1\n",
      "2018-08-10T17:23:28.753574: step 276, loss 0.0849801, acc 0.96875\n",
      "2018-08-10T17:23:28.901767: step 277, loss 0.0440431, acc 0.96875\n",
      "2018-08-10T17:23:29.049617: step 278, loss 0.0543434, acc 0.984375\n",
      "2018-08-10T17:23:29.197567: step 279, loss 0.0501417, acc 0.96875\n",
      "2018-08-10T17:23:29.348344: step 280, loss 0.0873307, acc 0.953125\n",
      "2018-08-10T17:23:29.505419: step 281, loss 0.0193065, acc 0.984375\n",
      "2018-08-10T17:23:29.655608: step 282, loss 0.0788957, acc 0.96875\n",
      "2018-08-10T17:23:29.799705: step 283, loss 0.174884, acc 0.96875\n",
      "2018-08-10T17:23:29.941376: step 284, loss 0.0241767, acc 1\n",
      "2018-08-10T17:23:30.082784: step 285, loss 0.161535, acc 0.96875\n",
      "2018-08-10T17:23:30.217657: step 286, loss 0.0860237, acc 0.935484\n",
      "2018-08-10T17:23:30.358504: step 287, loss 0.052193, acc 0.984375\n",
      "2018-08-10T17:23:30.499220: step 288, loss 0.112603, acc 0.96875\n",
      "2018-08-10T17:23:30.637916: step 289, loss 0.0568621, acc 0.984375\n",
      "2018-08-10T17:23:30.775168: step 290, loss 0.0130994, acc 1\n",
      "2018-08-10T17:23:30.913774: step 291, loss 0.0808225, acc 0.96875\n",
      "2018-08-10T17:23:31.053217: step 292, loss 0.0522188, acc 0.984375\n",
      "2018-08-10T17:23:31.192510: step 293, loss 0.0422861, acc 0.96875\n",
      "2018-08-10T17:23:31.335557: step 294, loss 0.0800481, acc 0.96875\n",
      "2018-08-10T17:23:31.476593: step 295, loss 0.113383, acc 0.96875\n",
      "2018-08-10T17:23:31.616252: step 296, loss 0.017301, acc 1\n",
      "2018-08-10T17:23:31.753435: step 297, loss 0.0155092, acc 0.984375\n",
      "2018-08-10T17:23:31.892999: step 298, loss 0.0782537, acc 0.96875\n",
      "2018-08-10T17:23:32.030215: step 299, loss 0.112304, acc 0.951613\n",
      "2018-08-10T17:23:32.169896: step 300, loss 0.0204538, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:23:32.305337: step 300, loss 0.641576, acc 0.842105\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533921767/checkpoints/model-300\n",
      "\n",
      "2018-08-10T17:23:32.492141: step 301, loss 0.00591619, acc 1\n",
      "2018-08-10T17:23:32.633289: step 302, loss 0.0299296, acc 0.984375\n",
      "2018-08-10T17:23:32.771474: step 303, loss 0.082709, acc 0.96875\n",
      "2018-08-10T17:23:32.912201: step 304, loss 0.00685296, acc 1\n",
      "2018-08-10T17:23:33.052699: step 305, loss 0.0065426, acc 1\n",
      "2018-08-10T17:23:33.192485: step 306, loss 0.150924, acc 0.953125\n",
      "2018-08-10T17:23:33.331196: step 307, loss 0.0223436, acc 1\n",
      "2018-08-10T17:23:33.471720: step 308, loss 0.0403012, acc 0.96875\n",
      "2018-08-10T17:23:33.612984: step 309, loss 0.0747747, acc 0.96875\n",
      "2018-08-10T17:23:33.751827: step 310, loss 0.0519537, acc 0.96875\n",
      "2018-08-10T17:23:33.890784: step 311, loss 0.038595, acc 0.984375\n",
      "2018-08-10T17:23:34.027912: step 312, loss 0.084378, acc 0.951613\n",
      "2018-08-10T17:23:34.169378: step 313, loss 0.0216066, acc 1\n",
      "2018-08-10T17:23:34.314280: step 314, loss 0.0352202, acc 0.984375\n",
      "2018-08-10T17:23:34.460884: step 315, loss 0.0480193, acc 0.984375\n",
      "2018-08-10T17:23:34.606115: step 316, loss 0.0208509, acc 0.984375\n",
      "2018-08-10T17:23:34.750777: step 317, loss 0.0111265, acc 1\n",
      "2018-08-10T17:23:34.897034: step 318, loss 0.0483454, acc 0.96875\n",
      "2018-08-10T17:23:35.042616: step 319, loss 0.169562, acc 0.953125\n",
      "2018-08-10T17:23:35.189965: step 320, loss 0.0503093, acc 0.96875\n",
      "2018-08-10T17:23:35.335038: step 321, loss 0.0208363, acc 1\n",
      "2018-08-10T17:23:35.485448: step 322, loss 0.0430712, acc 0.984375\n",
      "2018-08-10T17:23:35.629779: step 323, loss 0.0339909, acc 0.984375\n",
      "2018-08-10T17:23:35.778017: step 324, loss 0.0356285, acc 0.984375\n",
      "2018-08-10T17:23:35.920060: step 325, loss 0.0392521, acc 0.983871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:23:36.067999: step 326, loss 0.0332118, acc 0.96875\n",
      "2018-08-10T17:23:36.216288: step 327, loss 0.0262106, acc 1\n",
      "2018-08-10T17:23:36.362472: step 328, loss 0.0736995, acc 0.984375\n",
      "2018-08-10T17:23:36.507713: step 329, loss 0.0563345, acc 0.984375\n",
      "2018-08-10T17:23:36.651347: step 330, loss 0.0330186, acc 0.984375\n",
      "2018-08-10T17:23:36.800402: step 331, loss 0.0219713, acc 1\n",
      "2018-08-10T17:23:36.949080: step 332, loss 0.027766, acc 0.984375\n",
      "2018-08-10T17:23:37.094419: step 333, loss 0.0538668, acc 0.984375\n",
      "2018-08-10T17:23:37.239483: step 334, loss 0.00811313, acc 1\n",
      "2018-08-10T17:23:37.389980: step 335, loss 0.0583028, acc 0.984375\n",
      "2018-08-10T17:23:37.555818: step 336, loss 0.0512831, acc 0.96875\n",
      "2018-08-10T17:23:37.705890: step 337, loss 0.00685048, acc 1\n",
      "2018-08-10T17:23:37.848636: step 338, loss 0.0117536, acc 1\n",
      "2018-08-10T17:23:37.995786: step 339, loss 0.0342913, acc 0.96875\n",
      "2018-08-10T17:23:38.141942: step 340, loss 0.0575896, acc 0.984375\n",
      "2018-08-10T17:23:38.287423: step 341, loss 0.00587943, acc 1\n",
      "2018-08-10T17:23:38.434444: step 342, loss 0.11188, acc 0.953125\n",
      "2018-08-10T17:23:38.581971: step 343, loss 0.0233109, acc 1\n",
      "2018-08-10T17:23:38.728411: step 344, loss 0.0597954, acc 0.984375\n",
      "2018-08-10T17:23:38.874167: step 345, loss 0.0422054, acc 0.96875\n",
      "2018-08-10T17:23:39.021711: step 346, loss 0.0260545, acc 0.984375\n",
      "2018-08-10T17:23:39.166958: step 347, loss 0.00425652, acc 1\n",
      "2018-08-10T17:23:39.315666: step 348, loss 0.0380586, acc 0.96875\n",
      "2018-08-10T17:23:39.464005: step 349, loss 0.0116679, acc 1\n",
      "2018-08-10T17:23:39.613636: step 350, loss 0.0175381, acc 1\n",
      "2018-08-10T17:23:39.755262: step 351, loss 0.0247162, acc 1\n",
      "2018-08-10T17:23:39.901053: step 352, loss 0.0122271, acc 1\n",
      "2018-08-10T17:23:40.045117: step 353, loss 0.0421715, acc 0.96875\n",
      "2018-08-10T17:23:40.193146: step 354, loss 0.0629755, acc 0.984375\n",
      "2018-08-10T17:23:40.337681: step 355, loss 0.00823954, acc 1\n",
      "2018-08-10T17:23:40.483740: step 356, loss 0.0692836, acc 0.96875\n",
      "2018-08-10T17:23:40.628382: step 357, loss 0.0283323, acc 0.984375\n",
      "2018-08-10T17:23:40.775682: step 358, loss 0.0255931, acc 0.984375\n",
      "2018-08-10T17:23:40.920077: step 359, loss 0.0277013, acc 1\n",
      "2018-08-10T17:23:41.065433: step 360, loss 0.115313, acc 0.953125\n",
      "2018-08-10T17:23:41.208291: step 361, loss 0.0756179, acc 0.96875\n",
      "2018-08-10T17:23:41.353868: step 362, loss 0.023258, acc 0.984375\n",
      "2018-08-10T17:23:41.503217: step 363, loss 0.0311551, acc 0.984375\n",
      "2018-08-10T17:23:41.648747: step 364, loss 0.0127126, acc 1\n",
      "2018-08-10T17:23:41.794667: step 365, loss 0.0369813, acc 0.984375\n",
      "2018-08-10T17:23:41.940760: step 366, loss 0.00647721, acc 1\n",
      "2018-08-10T17:23:42.085179: step 367, loss 0.0628195, acc 0.96875\n",
      "2018-08-10T17:23:42.233083: step 368, loss 0.0212249, acc 0.984375\n",
      "2018-08-10T17:23:42.377647: step 369, loss 0.0382143, acc 0.984375\n",
      "2018-08-10T17:23:42.524519: step 370, loss 0.0235878, acc 1\n",
      "2018-08-10T17:23:42.671960: step 371, loss 0.025045, acc 1\n",
      "2018-08-10T17:23:42.817157: step 372, loss 0.00303001, acc 1\n",
      "2018-08-10T17:23:42.963608: step 373, loss 0.038837, acc 0.96875\n",
      "2018-08-10T17:23:43.109235: step 374, loss 0.0726772, acc 0.96875\n",
      "2018-08-10T17:23:43.254815: step 375, loss 0.0100234, acc 1\n",
      "2018-08-10T17:23:43.400640: step 376, loss 0.0192941, acc 1\n",
      "2018-08-10T17:23:43.542420: step 377, loss 0.0516983, acc 0.967742\n",
      "2018-08-10T17:23:43.688038: step 378, loss 0.0163884, acc 1\n",
      "2018-08-10T17:23:43.832551: step 379, loss 0.00800732, acc 1\n",
      "2018-08-10T17:23:43.977874: step 380, loss 0.0449989, acc 0.984375\n",
      "2018-08-10T17:23:44.125337: step 381, loss 0.0161527, acc 1\n",
      "2018-08-10T17:23:44.272723: step 382, loss 0.00532665, acc 1\n",
      "2018-08-10T17:23:44.418817: step 383, loss 0.0703141, acc 0.96875\n",
      "2018-08-10T17:23:44.561574: step 384, loss 0.0153789, acc 1\n",
      "2018-08-10T17:23:44.701710: step 385, loss 0.0166514, acc 0.984375\n",
      "2018-08-10T17:23:44.841773: step 386, loss 0.00885343, acc 1\n",
      "2018-08-10T17:23:44.983054: step 387, loss 0.0497495, acc 0.984375\n",
      "2018-08-10T17:23:45.125329: step 388, loss 0.0171819, acc 0.984375\n",
      "2018-08-10T17:23:45.270763: step 389, loss 0.0130379, acc 1\n",
      "2018-08-10T17:23:45.427681: step 390, loss 0.0199644, acc 1\n",
      "2018-08-10T17:23:45.577392: step 391, loss 0.138153, acc 0.9375\n",
      "2018-08-10T17:23:45.725397: step 392, loss 0.0260932, acc 0.984375\n",
      "2018-08-10T17:23:45.873468: step 393, loss 0.0196117, acc 0.984375\n",
      "2018-08-10T17:23:46.021315: step 394, loss 0.0141023, acc 1\n",
      "2018-08-10T17:23:46.170273: step 395, loss 0.00629873, acc 1\n",
      "2018-08-10T17:23:46.321989: step 396, loss 0.0178179, acc 0.984375\n",
      "2018-08-10T17:23:46.468173: step 397, loss 0.0101563, acc 1\n",
      "2018-08-10T17:23:46.616551: step 398, loss 0.0188417, acc 0.984375\n",
      "2018-08-10T17:23:46.765535: step 399, loss 0.0115122, acc 1\n",
      "2018-08-10T17:23:46.912065: step 400, loss 0.0156286, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:23:47.052622: step 400, loss 0.653855, acc 0.84689\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533921767/checkpoints/model-400\n",
      "\n",
      "2018-08-10T17:23:47.253613: step 401, loss 0.0111116, acc 1\n",
      "2018-08-10T17:23:47.401683: step 402, loss 0.0146919, acc 1\n",
      "2018-08-10T17:23:47.549035: step 403, loss 0.00680233, acc 1\n",
      "2018-08-10T17:23:47.696631: step 404, loss 0.00955731, acc 1\n",
      "2018-08-10T17:23:47.843718: step 405, loss 0.0175467, acc 1\n",
      "2018-08-10T17:23:47.988895: step 406, loss 0.0124794, acc 1\n",
      "2018-08-10T17:23:48.132700: step 407, loss 0.00314173, acc 1\n",
      "2018-08-10T17:23:48.277907: step 408, loss 0.00913749, acc 1\n",
      "2018-08-10T17:23:48.426354: step 409, loss 0.0165452, acc 0.984375\n",
      "2018-08-10T17:23:48.569468: step 410, loss 0.109556, acc 0.9375\n",
      "2018-08-10T17:23:48.715433: step 411, loss 0.0154074, acc 1\n",
      "2018-08-10T17:23:48.863892: step 412, loss 0.0257574, acc 1\n",
      "2018-08-10T17:23:49.011299: step 413, loss 0.0137196, acc 1\n",
      "2018-08-10T17:23:49.159584: step 414, loss 0.0107366, acc 1\n",
      "2018-08-10T17:23:49.307150: step 415, loss 0.0691634, acc 0.96875\n",
      "2018-08-10T17:23:49.453970: step 416, loss 0.00687758, acc 1\n",
      "2018-08-10T17:23:49.603838: step 417, loss 0.0156481, acc 0.984375\n",
      "2018-08-10T17:23:49.749789: step 418, loss 0.0110025, acc 1\n",
      "2018-08-10T17:23:49.894886: step 419, loss 0.0954945, acc 0.96875\n",
      "2018-08-10T17:23:50.042144: step 420, loss 0.00294041, acc 1\n",
      "2018-08-10T17:23:50.187715: step 421, loss 0.0059164, acc 1\n",
      "2018-08-10T17:23:50.333257: step 422, loss 0.0104191, acc 1\n",
      "2018-08-10T17:23:50.483770: step 423, loss 0.0337154, acc 0.984375\n",
      "2018-08-10T17:23:50.630348: step 424, loss 0.00539183, acc 1\n",
      "2018-08-10T17:23:50.774082: step 425, loss 0.0724578, acc 0.984375\n",
      "2018-08-10T17:23:50.913461: step 426, loss 0.0080575, acc 1\n",
      "2018-08-10T17:23:51.053174: step 427, loss 0.10452, acc 0.96875\n",
      "2018-08-10T17:23:51.191288: step 428, loss 0.0137257, acc 1\n",
      "2018-08-10T17:23:51.328148: step 429, loss 0.041539, acc 0.983871\n",
      "2018-08-10T17:23:51.472448: step 430, loss 0.0306313, acc 0.984375\n",
      "2018-08-10T17:23:51.613940: step 431, loss 0.0100263, acc 1\n",
      "2018-08-10T17:23:51.752509: step 432, loss 0.0631396, acc 0.96875\n",
      "2018-08-10T17:23:51.893684: step 433, loss 0.00708235, acc 1\n",
      "2018-08-10T17:23:52.033156: step 434, loss 0.0108131, acc 1\n",
      "2018-08-10T17:23:52.170807: step 435, loss 0.0108073, acc 1\n",
      "2018-08-10T17:23:52.308587: step 436, loss 0.0569133, acc 0.96875\n",
      "2018-08-10T17:23:52.450219: step 437, loss 0.00584449, acc 1\n",
      "2018-08-10T17:23:52.588851: step 438, loss 0.0220229, acc 1\n",
      "2018-08-10T17:23:52.732143: step 439, loss 0.0399128, acc 0.984375\n",
      "2018-08-10T17:23:52.871527: step 440, loss 0.0252819, acc 1\n",
      "2018-08-10T17:23:53.012226: step 441, loss 0.02055, acc 1\n",
      "2018-08-10T17:23:53.146388: step 442, loss 0.0112974, acc 1\n",
      "2018-08-10T17:23:53.283897: step 443, loss 0.00541834, acc 1\n",
      "2018-08-10T17:23:53.424051: step 444, loss 0.00822226, acc 1\n",
      "2018-08-10T17:23:53.566500: step 445, loss 0.00697932, acc 1\n",
      "2018-08-10T17:23:53.707125: step 446, loss 0.0197869, acc 1\n",
      "2018-08-10T17:23:53.846454: step 447, loss 0.00348914, acc 1\n",
      "2018-08-10T17:23:53.987782: step 448, loss 0.00434219, acc 1\n",
      "2018-08-10T17:23:54.127731: step 449, loss 0.13249, acc 0.984375\n",
      "2018-08-10T17:23:54.267200: step 450, loss 0.047917, acc 0.984375\n",
      "2018-08-10T17:23:54.406985: step 451, loss 0.0132297, acc 0.984375\n",
      "2018-08-10T17:23:54.547126: step 452, loss 0.00847414, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:23:54.687218: step 453, loss 0.0315499, acc 0.984375\n",
      "2018-08-10T17:23:54.828351: step 454, loss 0.0123577, acc 1\n",
      "2018-08-10T17:23:54.965400: step 455, loss 0.0347902, acc 0.983871\n",
      "2018-08-10T17:23:55.108780: step 456, loss 0.0195752, acc 1\n",
      "2018-08-10T17:23:55.253479: step 457, loss 0.00678563, acc 1\n",
      "2018-08-10T17:23:55.400627: step 458, loss 0.016621, acc 1\n",
      "2018-08-10T17:23:55.549114: step 459, loss 0.0427313, acc 0.96875\n",
      "2018-08-10T17:23:55.697059: step 460, loss 0.0206861, acc 1\n",
      "2018-08-10T17:23:55.846967: step 461, loss 0.0179563, acc 1\n",
      "2018-08-10T17:23:55.994099: step 462, loss 0.038201, acc 0.984375\n",
      "2018-08-10T17:23:56.141767: step 463, loss 0.0172008, acc 1\n",
      "2018-08-10T17:23:56.289428: step 464, loss 0.0038178, acc 1\n",
      "2018-08-10T17:23:56.439291: step 465, loss 0.0195297, acc 1\n",
      "2018-08-10T17:23:56.590715: step 466, loss 0.0110514, acc 1\n",
      "2018-08-10T17:23:56.740793: step 467, loss 0.017281, acc 0.984375\n",
      "2018-08-10T17:23:56.884740: step 468, loss 0.00589238, acc 1\n",
      "2018-08-10T17:23:57.030476: step 469, loss 0.0219388, acc 0.984375\n",
      "2018-08-10T17:23:57.178743: step 470, loss 0.00500425, acc 1\n",
      "2018-08-10T17:23:57.327466: step 471, loss 0.0195363, acc 1\n",
      "2018-08-10T17:23:57.479446: step 472, loss 0.0122653, acc 1\n",
      "2018-08-10T17:23:57.628599: step 473, loss 0.0141841, acc 1\n",
      "2018-08-10T17:23:57.776448: step 474, loss 0.0197612, acc 1\n",
      "2018-08-10T17:23:57.923784: step 475, loss 0.0748492, acc 0.984375\n",
      "2018-08-10T17:23:58.070888: step 476, loss 0.0184443, acc 1\n",
      "2018-08-10T17:23:58.219795: step 477, loss 0.0858527, acc 0.96875\n",
      "2018-08-10T17:23:58.369712: step 478, loss 0.00612084, acc 1\n",
      "2018-08-10T17:23:58.517735: step 479, loss 0.0175619, acc 0.984375\n",
      "2018-08-10T17:23:58.665291: step 480, loss 0.0142245, acc 0.984375\n",
      "2018-08-10T17:23:58.808640: step 481, loss 0.0189026, acc 1\n",
      "2018-08-10T17:23:58.956972: step 482, loss 0.00655056, acc 1\n",
      "2018-08-10T17:23:59.104225: step 483, loss 0.0251974, acc 0.984375\n",
      "2018-08-10T17:23:59.249862: step 484, loss 0.0131746, acc 1\n",
      "2018-08-10T17:23:59.397573: step 485, loss 0.024241, acc 1\n",
      "2018-08-10T17:23:59.545230: step 486, loss 0.00618237, acc 1\n",
      "2018-08-10T17:23:59.696010: step 487, loss 0.0511784, acc 0.984375\n",
      "2018-08-10T17:23:59.845349: step 488, loss 0.0269181, acc 0.984375\n",
      "2018-08-10T17:23:59.996057: step 489, loss 0.00334292, acc 1\n",
      "2018-08-10T17:24:00.144074: step 490, loss 0.0650691, acc 0.96875\n",
      "2018-08-10T17:24:00.294073: step 491, loss 0.0126824, acc 1\n",
      "2018-08-10T17:24:00.445584: step 492, loss 0.0205246, acc 0.984375\n",
      "2018-08-10T17:24:00.595448: step 493, loss 0.0433003, acc 0.984375\n",
      "2018-08-10T17:24:00.739347: step 494, loss 0.00756323, acc 1\n",
      "2018-08-10T17:24:00.888402: step 495, loss 0.0018031, acc 1\n",
      "2018-08-10T17:24:01.036265: step 496, loss 0.0129271, acc 1\n",
      "2018-08-10T17:24:01.186844: step 497, loss 0.0938151, acc 0.984375\n",
      "2018-08-10T17:24:01.338979: step 498, loss 0.0151847, acc 1\n",
      "2018-08-10T17:24:01.492767: step 499, loss 0.0338347, acc 0.984375\n",
      "2018-08-10T17:24:01.648048: step 500, loss 0.0141183, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:24:01.789825: step 500, loss 0.76737, acc 0.827751\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533921767/checkpoints/model-500\n",
      "\n",
      "2018-08-10T17:24:01.992304: step 501, loss 0.0174853, acc 0.984375\n",
      "2018-08-10T17:24:02.139927: step 502, loss 0.0456175, acc 0.96875\n",
      "2018-08-10T17:24:02.288175: step 503, loss 0.0200092, acc 1\n",
      "2018-08-10T17:24:02.437333: step 504, loss 0.0056079, acc 1\n",
      "2018-08-10T17:24:02.586965: step 505, loss 0.0118068, acc 1\n",
      "2018-08-10T17:24:02.734124: step 506, loss 0.0246774, acc 0.984375\n",
      "2018-08-10T17:24:02.878024: step 507, loss 0.009783, acc 1\n",
      "2018-08-10T17:24:03.025177: step 508, loss 0.0013607, acc 1\n",
      "2018-08-10T17:24:03.173786: step 509, loss 0.0253289, acc 0.984375\n",
      "2018-08-10T17:24:03.322800: step 510, loss 0.0129445, acc 1\n",
      "2018-08-10T17:24:03.476173: step 511, loss 0.0100615, acc 1\n",
      "2018-08-10T17:24:03.625523: step 512, loss 0.0218237, acc 0.984375\n",
      "2018-08-10T17:24:03.774098: step 513, loss 0.127758, acc 0.984375\n",
      "2018-08-10T17:24:03.924106: step 514, loss 0.00501306, acc 1\n",
      "2018-08-10T17:24:04.076980: step 515, loss 0.035754, acc 0.96875\n",
      "2018-08-10T17:24:04.225474: step 516, loss 0.00474376, acc 1\n",
      "2018-08-10T17:24:04.377191: step 517, loss 0.0177523, acc 0.984375\n",
      "2018-08-10T17:24:04.526688: step 518, loss 0.0455204, acc 0.984375\n",
      "2018-08-10T17:24:04.675783: step 519, loss 0.0226957, acc 1\n",
      "2018-08-10T17:24:04.818748: step 520, loss 0.000519563, acc 1\n",
      "2018-08-10T17:24:04.964340: step 521, loss 0.00743415, acc 1\n",
      "2018-08-10T17:24:05.111430: step 522, loss 0.00233243, acc 1\n",
      "2018-08-10T17:24:05.256959: step 523, loss 0.0106676, acc 1\n",
      "2018-08-10T17:24:05.404529: step 524, loss 0.00314163, acc 1\n",
      "2018-08-10T17:24:05.551955: step 525, loss 0.00346592, acc 1\n",
      "2018-08-10T17:24:05.694844: step 526, loss 0.0201683, acc 0.984375\n",
      "2018-08-10T17:24:05.834719: step 527, loss 0.0965959, acc 0.953125\n",
      "2018-08-10T17:24:05.977715: step 528, loss 0.0053889, acc 1\n",
      "2018-08-10T17:24:06.122053: step 529, loss 0.0677786, acc 0.984375\n",
      "2018-08-10T17:24:06.270584: step 530, loss 0.0153367, acc 1\n",
      "2018-08-10T17:24:06.419306: step 531, loss 0.00270228, acc 1\n",
      "2018-08-10T17:24:06.568065: step 532, loss 0.0241152, acc 0.984375\n",
      "2018-08-10T17:24:06.713617: step 533, loss 0.00653761, acc 1\n",
      "2018-08-10T17:24:06.861977: step 534, loss 0.0106518, acc 1\n",
      "2018-08-10T17:24:07.011198: step 535, loss 0.0147955, acc 1\n",
      "2018-08-10T17:24:07.157859: step 536, loss 0.000842977, acc 1\n",
      "2018-08-10T17:24:07.306361: step 537, loss 0.00803598, acc 1\n",
      "2018-08-10T17:24:07.458110: step 538, loss 0.0226252, acc 0.984375\n",
      "2018-08-10T17:24:07.611844: step 539, loss 0.00384036, acc 1\n",
      "2018-08-10T17:24:07.760864: step 540, loss 0.0130958, acc 1\n",
      "2018-08-10T17:24:07.910426: step 541, loss 0.0166834, acc 0.984375\n",
      "2018-08-10T17:24:08.058782: step 542, loss 0.00314272, acc 1\n",
      "2018-08-10T17:24:08.209441: step 543, loss 0.0289242, acc 1\n",
      "2018-08-10T17:24:08.357900: step 544, loss 0.0118466, acc 1\n",
      "2018-08-10T17:24:08.507486: step 545, loss 0.0635558, acc 0.984375\n",
      "2018-08-10T17:24:08.651885: step 546, loss 0.0599314, acc 0.983871\n",
      "2018-08-10T17:24:08.798947: step 547, loss 0.00167817, acc 1\n",
      "2018-08-10T17:24:08.948448: step 548, loss 0.00296719, acc 1\n",
      "2018-08-10T17:24:09.104442: step 549, loss 0.00233512, acc 1\n",
      "2018-08-10T17:24:09.252068: step 550, loss 0.00421619, acc 1\n",
      "2018-08-10T17:24:09.403086: step 551, loss 0.0126189, acc 1\n",
      "2018-08-10T17:24:09.552668: step 552, loss 0.0095581, acc 1\n",
      "2018-08-10T17:24:09.700927: step 553, loss 0.0196089, acc 0.984375\n",
      "2018-08-10T17:24:09.847956: step 554, loss 0.00187029, acc 1\n",
      "2018-08-10T17:24:09.998110: step 555, loss 0.000996685, acc 1\n",
      "2018-08-10T17:24:10.144456: step 556, loss 0.00419848, acc 1\n",
      "2018-08-10T17:24:10.292106: step 557, loss 0.00618704, acc 1\n",
      "2018-08-10T17:24:10.440810: step 558, loss 0.00094501, acc 1\n",
      "2018-08-10T17:24:10.586251: step 559, loss 0.00773238, acc 1\n",
      "2018-08-10T17:24:10.735433: step 560, loss 0.00273146, acc 1\n",
      "2018-08-10T17:24:10.886553: step 561, loss 0.0301808, acc 0.984375\n",
      "2018-08-10T17:24:11.035870: step 562, loss 0.0136736, acc 1\n",
      "2018-08-10T17:24:11.184613: step 563, loss 0.00444672, acc 1\n",
      "2018-08-10T17:24:11.330988: step 564, loss 0.00164391, acc 1\n",
      "2018-08-10T17:24:11.483252: step 565, loss 0.0131857, acc 0.984375\n",
      "2018-08-10T17:24:11.633928: step 566, loss 0.0128932, acc 0.984375\n",
      "2018-08-10T17:24:11.777106: step 567, loss 0.00250998, acc 1\n",
      "2018-08-10T17:24:11.916060: step 568, loss 0.0216711, acc 0.984375\n",
      "2018-08-10T17:24:12.058636: step 569, loss 0.00740526, acc 1\n",
      "2018-08-10T17:24:12.196703: step 570, loss 0.00505176, acc 1\n",
      "2018-08-10T17:24:12.336448: step 571, loss 0.00470719, acc 1\n",
      "2018-08-10T17:24:12.472749: step 572, loss 0.00281365, acc 1\n",
      "2018-08-10T17:24:12.616875: step 573, loss 0.0167045, acc 1\n",
      "2018-08-10T17:24:12.756874: step 574, loss 0.0305773, acc 0.984375\n",
      "2018-08-10T17:24:12.895831: step 575, loss 0.0129868, acc 1\n",
      "2018-08-10T17:24:13.032699: step 576, loss 0.0229037, acc 0.984375\n",
      "2018-08-10T17:24:13.171244: step 577, loss 0.00594108, acc 1\n",
      "2018-08-10T17:24:13.311281: step 578, loss 0.000784065, acc 1\n",
      "2018-08-10T17:24:13.453921: step 579, loss 0.00352247, acc 1\n",
      "2018-08-10T17:24:13.593361: step 580, loss 0.00330885, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:24:13.733191: step 581, loss 0.0446434, acc 0.984375\n",
      "2018-08-10T17:24:13.871988: step 582, loss 0.0192323, acc 0.984375\n",
      "2018-08-10T17:24:14.012229: step 583, loss 0.00104721, acc 1\n",
      "2018-08-10T17:24:14.150507: step 584, loss 0.0172399, acc 1\n",
      "2018-08-10T17:24:14.286863: step 585, loss 0.00171017, acc 1\n",
      "2018-08-10T17:24:14.426374: step 586, loss 0.0113474, acc 1\n",
      "2018-08-10T17:24:14.565402: step 587, loss 0.0144144, acc 0.984375\n",
      "2018-08-10T17:24:14.704010: step 588, loss 0.0298129, acc 0.984375\n",
      "2018-08-10T17:24:14.843714: step 589, loss 0.00597754, acc 1\n",
      "2018-08-10T17:24:14.983496: step 590, loss 0.00162368, acc 1\n",
      "2018-08-10T17:24:15.124085: step 591, loss 0.0253051, acc 0.984375\n",
      "2018-08-10T17:24:15.261408: step 592, loss 0.0140662, acc 1\n",
      "2018-08-10T17:24:15.404600: step 593, loss 0.0212317, acc 0.984375\n",
      "2018-08-10T17:24:15.546854: step 594, loss 0.0172381, acc 0.984375\n",
      "2018-08-10T17:24:15.687549: step 595, loss 0.029429, acc 0.984375\n",
      "2018-08-10T17:24:15.826323: step 596, loss 0.00777014, acc 1\n",
      "2018-08-10T17:24:15.969324: step 597, loss 0.00387827, acc 1\n",
      "2018-08-10T17:24:16.108039: step 598, loss 0.00378987, acc 1\n",
      "2018-08-10T17:24:16.253271: step 599, loss 0.0292791, acc 0.984375\n",
      "2018-08-10T17:24:16.400119: step 600, loss 0.0102072, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:24:16.559243: step 600, loss 0.727028, acc 0.842105\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533921767/checkpoints/model-600\n",
      "\n",
      "2018-08-10T17:24:16.765112: step 601, loss 0.00816357, acc 1\n",
      "2018-08-10T17:24:16.915719: step 602, loss 0.0025652, acc 1\n",
      "2018-08-10T17:24:17.062868: step 603, loss 0.00367091, acc 1\n",
      "2018-08-10T17:24:17.208157: step 604, loss 0.0109135, acc 1\n",
      "2018-08-10T17:24:17.359941: step 605, loss 0.0134238, acc 1\n",
      "2018-08-10T17:24:17.508992: step 606, loss 0.0080714, acc 1\n",
      "2018-08-10T17:24:17.656907: step 607, loss 0.0095308, acc 1\n",
      "2018-08-10T17:24:17.805246: step 608, loss 0.00421146, acc 1\n",
      "2018-08-10T17:24:17.951883: step 609, loss 0.00893754, acc 1\n",
      "2018-08-10T17:24:18.098007: step 610, loss 0.0376613, acc 0.984375\n",
      "2018-08-10T17:24:18.242527: step 611, loss 0.0287388, acc 1\n",
      "2018-08-10T17:24:18.390746: step 612, loss 0.00706037, acc 1\n",
      "2018-08-10T17:24:18.532809: step 613, loss 0.00225417, acc 1\n",
      "2018-08-10T17:24:18.680981: step 614, loss 0.0172941, acc 1\n",
      "2018-08-10T17:24:18.825550: step 615, loss 0.0514102, acc 0.984375\n",
      "2018-08-10T17:24:18.974323: step 616, loss 0.106951, acc 0.96875\n",
      "2018-08-10T17:24:19.118040: step 617, loss 0.00378374, acc 1\n",
      "2018-08-10T17:24:19.262925: step 618, loss 0.00694804, acc 1\n",
      "2018-08-10T17:24:19.410976: step 619, loss 0.00460428, acc 1\n",
      "2018-08-10T17:24:19.558721: step 620, loss 0.000963381, acc 1\n",
      "2018-08-10T17:24:19.707033: step 621, loss 0.0045268, acc 1\n",
      "2018-08-10T17:24:19.852624: step 622, loss 0.00462319, acc 1\n",
      "2018-08-10T17:24:19.998099: step 623, loss 0.00174243, acc 1\n",
      "2018-08-10T17:24:20.140971: step 624, loss 0.00451322, acc 1\n",
      "2018-08-10T17:24:20.288818: step 625, loss 0.0408483, acc 0.984375\n",
      "2018-08-10T17:24:20.439270: step 626, loss 0.00365168, acc 1\n",
      "2018-08-10T17:24:20.587334: step 627, loss 0.00189487, acc 1\n",
      "2018-08-10T17:24:20.737053: step 628, loss 0.00322119, acc 1\n",
      "2018-08-10T17:24:20.882191: step 629, loss 0.00385296, acc 1\n",
      "2018-08-10T17:24:21.029400: step 630, loss 0.00539107, acc 1\n",
      "2018-08-10T17:24:21.177583: step 631, loss 0.00103803, acc 1\n",
      "2018-08-10T17:24:21.323678: step 632, loss 0.00368243, acc 1\n",
      "2018-08-10T17:24:21.475694: step 633, loss 0.0408093, acc 0.984375\n",
      "2018-08-10T17:24:21.623906: step 634, loss 0.00465451, acc 1\n",
      "2018-08-10T17:24:21.771293: step 635, loss 0.00186367, acc 1\n",
      "2018-08-10T17:24:21.919686: step 636, loss 0.00718775, acc 1\n",
      "2018-08-10T17:24:22.062151: step 637, loss 0.0129226, acc 1\n",
      "2018-08-10T17:24:22.211329: step 638, loss 0.00344081, acc 1\n",
      "2018-08-10T17:24:22.359144: step 639, loss 0.00291948, acc 1\n",
      "2018-08-10T17:24:22.507330: step 640, loss 0.00222678, acc 1\n",
      "2018-08-10T17:24:22.652677: step 641, loss 0.00748353, acc 1\n",
      "2018-08-10T17:24:22.798809: step 642, loss 0.00195883, acc 1\n",
      "2018-08-10T17:24:22.945671: step 643, loss 0.00793169, acc 1\n",
      "2018-08-10T17:24:23.094378: step 644, loss 0.00831277, acc 1\n",
      "2018-08-10T17:24:23.242442: step 645, loss 0.00386759, acc 1\n",
      "2018-08-10T17:24:23.389296: step 646, loss 0.00277011, acc 1\n",
      "2018-08-10T17:24:23.538974: step 647, loss 0.00120761, acc 1\n",
      "2018-08-10T17:24:23.688817: step 648, loss 0.00288722, acc 1\n",
      "2018-08-10T17:24:23.835732: step 649, loss 0.0091355, acc 1\n",
      "2018-08-10T17:24:23.977816: step 650, loss 0.00410574, acc 1\n",
      "2018-08-10T17:24:24.129238: step 651, loss 0.0251518, acc 0.984375\n",
      "2018-08-10T17:24:24.277360: step 652, loss 0.00045936, acc 1\n",
      "2018-08-10T17:24:24.425282: step 653, loss 0.00802702, acc 1\n",
      "2018-08-10T17:24:24.568788: step 654, loss 0.0426406, acc 0.984375\n",
      "2018-08-10T17:24:24.715747: step 655, loss 0.000584406, acc 1\n",
      "2018-08-10T17:24:24.861879: step 656, loss 0.00218866, acc 1\n",
      "2018-08-10T17:24:25.010464: step 657, loss 0.00801777, acc 1\n",
      "2018-08-10T17:24:25.159891: step 658, loss 0.00200985, acc 1\n",
      "2018-08-10T17:24:25.307231: step 659, loss 0.00276139, acc 1\n",
      "2018-08-10T17:24:25.455894: step 660, loss 0.00338444, acc 1\n",
      "2018-08-10T17:24:25.604104: step 661, loss 0.00206293, acc 1\n",
      "2018-08-10T17:24:25.752464: step 662, loss 0.00959343, acc 1\n",
      "2018-08-10T17:24:25.895763: step 663, loss 0.0569381, acc 0.983871\n",
      "2018-08-10T17:24:26.042408: step 664, loss 0.00518916, acc 1\n",
      "2018-08-10T17:24:26.187832: step 665, loss 0.0187471, acc 1\n",
      "2018-08-10T17:24:26.335264: step 666, loss 0.00213496, acc 1\n",
      "2018-08-10T17:24:26.481272: step 667, loss 0.0279565, acc 0.984375\n",
      "2018-08-10T17:24:26.624960: step 668, loss 0.0177302, acc 1\n",
      "2018-08-10T17:24:26.765470: step 669, loss 0.0481495, acc 0.96875\n",
      "2018-08-10T17:24:26.906057: step 670, loss 0.000379942, acc 1\n",
      "2018-08-10T17:24:27.048206: step 671, loss 0.00111452, acc 1\n",
      "2018-08-10T17:24:27.192737: step 672, loss 0.00711671, acc 1\n",
      "2018-08-10T17:24:27.344775: step 673, loss 0.0185563, acc 0.984375\n",
      "2018-08-10T17:24:27.496918: step 674, loss 0.00842329, acc 1\n",
      "2018-08-10T17:24:27.648414: step 675, loss 0.00238663, acc 1\n",
      "2018-08-10T17:24:27.794890: step 676, loss 0.00180362, acc 1\n",
      "2018-08-10T17:24:27.943272: step 677, loss 0.0018262, acc 1\n",
      "2018-08-10T17:24:28.090174: step 678, loss 0.00361473, acc 1\n",
      "2018-08-10T17:24:28.238988: step 679, loss 0.00229794, acc 1\n",
      "2018-08-10T17:24:28.388417: step 680, loss 0.00222275, acc 1\n",
      "2018-08-10T17:24:28.534997: step 681, loss 0.0129672, acc 1\n",
      "2018-08-10T17:24:28.684811: step 682, loss 0.00361654, acc 1\n",
      "2018-08-10T17:24:28.833954: step 683, loss 0.0214215, acc 1\n",
      "2018-08-10T17:24:28.982571: step 684, loss 0.00183851, acc 1\n",
      "2018-08-10T17:24:29.129030: step 685, loss 0.00344453, acc 1\n",
      "2018-08-10T17:24:29.278437: step 686, loss 0.00389507, acc 1\n",
      "2018-08-10T17:24:29.430272: step 687, loss 0.0144675, acc 1\n",
      "2018-08-10T17:24:29.578972: step 688, loss 0.000788458, acc 1\n",
      "2018-08-10T17:24:29.728168: step 689, loss 0.0178347, acc 0.983871\n",
      "2018-08-10T17:24:29.879476: step 690, loss 0.00584041, acc 1\n",
      "2018-08-10T17:24:30.029074: step 691, loss 0.00539191, acc 1\n",
      "2018-08-10T17:24:30.176971: step 692, loss 0.0311732, acc 0.984375\n",
      "2018-08-10T17:24:30.331316: step 693, loss 0.00743298, acc 1\n",
      "2018-08-10T17:24:30.481742: step 694, loss 0.00146057, acc 1\n",
      "2018-08-10T17:24:30.632393: step 695, loss 0.00895658, acc 1\n",
      "2018-08-10T17:24:30.781608: step 696, loss 0.0051442, acc 1\n",
      "2018-08-10T17:24:30.935136: step 697, loss 0.00817647, acc 1\n",
      "2018-08-10T17:24:31.087234: step 698, loss 0.0110306, acc 1\n",
      "2018-08-10T17:24:31.237239: step 699, loss 0.00845994, acc 1\n",
      "2018-08-10T17:24:31.386053: step 700, loss 0.00543046, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:24:31.533015: step 700, loss 0.742304, acc 0.851675\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533921767/checkpoints/model-700\n",
      "\n",
      "2018-08-10T17:24:31.743706: step 701, loss 0.0317181, acc 0.984375\n",
      "2018-08-10T17:24:31.889951: step 702, loss 0.00440734, acc 1\n",
      "2018-08-10T17:24:32.040054: step 703, loss 0.00410363, acc 1\n",
      "2018-08-10T17:24:32.188692: step 704, loss 0.0791338, acc 0.984375\n",
      "2018-08-10T17:24:32.336119: step 705, loss 0.00796177, acc 1\n",
      "2018-08-10T17:24:32.490632: step 706, loss 0.0105487, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:24:32.641121: step 707, loss 0.0244211, acc 0.984375\n",
      "2018-08-10T17:24:32.784932: step 708, loss 0.00373259, acc 1\n",
      "2018-08-10T17:24:32.927064: step 709, loss 0.00346357, acc 1\n",
      "2018-08-10T17:24:33.065454: step 710, loss 0.00295538, acc 1\n",
      "2018-08-10T17:24:33.205568: step 711, loss 0.000839234, acc 1\n",
      "2018-08-10T17:24:33.346956: step 712, loss 0.000785156, acc 1\n",
      "2018-08-10T17:24:33.491465: step 713, loss 0.00889118, acc 1\n",
      "2018-08-10T17:24:33.631782: step 714, loss 0.0189477, acc 1\n",
      "2018-08-10T17:24:33.766855: step 715, loss 0.00578687, acc 1\n",
      "2018-08-10T17:24:33.905732: step 716, loss 0.00216303, acc 1\n",
      "2018-08-10T17:24:34.047096: step 717, loss 0.00570235, acc 1\n",
      "2018-08-10T17:24:34.197518: step 718, loss 0.0139929, acc 0.984375\n",
      "2018-08-10T17:24:34.339866: step 719, loss 0.00201081, acc 1\n",
      "2018-08-10T17:24:34.481634: step 720, loss 0.0225698, acc 0.984375\n",
      "2018-08-10T17:24:34.625337: step 721, loss 0.00355441, acc 1\n",
      "2018-08-10T17:24:34.763314: step 722, loss 0.00900586, acc 1\n",
      "2018-08-10T17:24:34.903956: step 723, loss 0.00109296, acc 1\n",
      "2018-08-10T17:24:35.044247: step 724, loss 0.0185459, acc 0.984375\n",
      "2018-08-10T17:24:35.184970: step 725, loss 0.000706146, acc 1\n",
      "2018-08-10T17:24:35.326084: step 726, loss 0.00424595, acc 1\n",
      "2018-08-10T17:24:35.471661: step 727, loss 0.000612532, acc 1\n",
      "2018-08-10T17:24:35.611047: step 728, loss 0.0211337, acc 0.983871\n",
      "2018-08-10T17:24:35.752407: step 729, loss 0.0314986, acc 0.984375\n",
      "2018-08-10T17:24:35.890104: step 730, loss 0.00346255, acc 1\n",
      "2018-08-10T17:24:36.029559: step 731, loss 0.0050448, acc 1\n",
      "2018-08-10T17:24:36.169912: step 732, loss 0.00168297, acc 1\n",
      "2018-08-10T17:24:36.318151: step 733, loss 0.00231427, acc 1\n",
      "2018-08-10T17:24:36.459977: step 734, loss 0.00500757, acc 1\n",
      "2018-08-10T17:24:36.601941: step 735, loss 0.0266513, acc 0.984375\n",
      "2018-08-10T17:24:36.741720: step 736, loss 0.0159852, acc 0.984375\n",
      "2018-08-10T17:24:36.885191: step 737, loss 0.00383892, acc 1\n",
      "2018-08-10T17:24:37.029768: step 738, loss 0.00110226, acc 1\n",
      "2018-08-10T17:24:37.173882: step 739, loss 0.00418507, acc 1\n",
      "2018-08-10T17:24:37.317723: step 740, loss 0.0125727, acc 1\n",
      "2018-08-10T17:24:37.466062: step 741, loss 0.01148, acc 1\n",
      "2018-08-10T17:24:37.615611: step 742, loss 0.00977361, acc 1\n",
      "2018-08-10T17:24:37.763636: step 743, loss 0.0091626, acc 1\n",
      "2018-08-10T17:24:37.906393: step 744, loss 0.00928662, acc 1\n",
      "2018-08-10T17:24:38.053869: step 745, loss 0.0141642, acc 0.984375\n",
      "2018-08-10T17:24:38.199709: step 746, loss 0.00192214, acc 1\n",
      "2018-08-10T17:24:38.347748: step 747, loss 0.00073917, acc 1\n",
      "2018-08-10T17:24:38.493228: step 748, loss 0.000680084, acc 1\n",
      "2018-08-10T17:24:38.643413: step 749, loss 0.00268612, acc 1\n",
      "2018-08-10T17:24:38.792706: step 750, loss 0.000824888, acc 1\n",
      "2018-08-10T17:24:38.940796: step 751, loss 0.00452453, acc 1\n",
      "2018-08-10T17:24:39.089899: step 752, loss 0.00226137, acc 1\n",
      "2018-08-10T17:24:39.237072: step 753, loss 0.0038019, acc 1\n",
      "2018-08-10T17:24:39.380673: step 754, loss 0.00682851, acc 1\n",
      "2018-08-10T17:24:39.530610: step 755, loss 0.00123133, acc 1\n",
      "2018-08-10T17:24:39.677349: step 756, loss 0.0035178, acc 1\n",
      "2018-08-10T17:24:39.825703: step 757, loss 0.0448973, acc 0.984375\n",
      "2018-08-10T17:24:39.974312: step 758, loss 0.00869975, acc 1\n",
      "2018-08-10T17:24:40.123190: step 759, loss 0.0104285, acc 1\n",
      "2018-08-10T17:24:40.268473: step 760, loss 0.00186869, acc 1\n",
      "2018-08-10T17:24:40.421650: step 761, loss 0.00569167, acc 1\n",
      "2018-08-10T17:24:40.566916: step 762, loss 0.0148765, acc 0.984375\n",
      "2018-08-10T17:24:40.713308: step 763, loss 0.00276614, acc 1\n",
      "2018-08-10T17:24:40.860266: step 764, loss 0.0178617, acc 1\n",
      "2018-08-10T17:24:41.007612: step 765, loss 0.000971478, acc 1\n",
      "2018-08-10T17:24:41.155319: step 766, loss 0.019216, acc 0.984375\n",
      "2018-08-10T17:24:41.299363: step 767, loss 0.00216108, acc 1\n",
      "2018-08-10T17:24:41.446745: step 768, loss 0.00697501, acc 1\n",
      "2018-08-10T17:24:41.592831: step 769, loss 0.00157902, acc 1\n",
      "2018-08-10T17:24:41.737744: step 770, loss 0.0106722, acc 1\n",
      "2018-08-10T17:24:41.887651: step 771, loss 0.0106526, acc 1\n",
      "2018-08-10T17:24:42.033474: step 772, loss 0.00280038, acc 1\n",
      "2018-08-10T17:24:42.180249: step 773, loss 0.00832183, acc 1\n",
      "2018-08-10T17:24:42.326827: step 774, loss 0.00214911, acc 1\n",
      "2018-08-10T17:24:42.473349: step 775, loss 0.00332954, acc 1\n",
      "2018-08-10T17:24:42.619745: step 776, loss 0.00180522, acc 1\n",
      "2018-08-10T17:24:42.766018: step 777, loss 0.00299308, acc 1\n",
      "2018-08-10T17:24:42.910071: step 778, loss 0.000456364, acc 1\n",
      "2018-08-10T17:24:43.055930: step 779, loss 0.0012921, acc 1\n",
      "2018-08-10T17:24:43.194997: step 780, loss 0.00151747, acc 1\n",
      "2018-08-10T17:24:43.342627: step 781, loss 0.052348, acc 0.984375\n",
      "2018-08-10T17:24:43.490314: step 782, loss 0.00336521, acc 1\n",
      "2018-08-10T17:24:43.635832: step 783, loss 0.00270694, acc 1\n",
      "2018-08-10T17:24:43.780489: step 784, loss 0.0276329, acc 0.984375\n",
      "2018-08-10T17:24:43.927475: step 785, loss 0.00567929, acc 1\n",
      "2018-08-10T17:24:44.076189: step 786, loss 0.00982858, acc 1\n",
      "2018-08-10T17:24:44.224004: step 787, loss 0.00091171, acc 1\n",
      "2018-08-10T17:24:44.369885: step 788, loss 0.00197858, acc 1\n",
      "2018-08-10T17:24:44.517793: step 789, loss 0.0191422, acc 0.984375\n",
      "2018-08-10T17:24:44.666971: step 790, loss 0.000817448, acc 1\n",
      "2018-08-10T17:24:44.813441: step 791, loss 0.00230504, acc 1\n",
      "2018-08-10T17:24:44.960608: step 792, loss 0.0473457, acc 0.96875\n",
      "2018-08-10T17:24:45.103984: step 793, loss 0.0833782, acc 0.983871\n",
      "2018-08-10T17:24:45.250047: step 794, loss 0.00222683, acc 1\n",
      "2018-08-10T17:24:45.398108: step 795, loss 0.0020655, acc 1\n",
      "2018-08-10T17:24:45.546225: step 796, loss 0.00158386, acc 1\n",
      "2018-08-10T17:24:45.692484: step 797, loss 0.000752958, acc 1\n",
      "2018-08-10T17:24:45.838769: step 798, loss 0.00267085, acc 1\n",
      "2018-08-10T17:24:45.988030: step 799, loss 0.00228698, acc 1\n",
      "2018-08-10T17:24:46.136999: step 800, loss 0.000363239, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:24:46.282249: step 800, loss 0.659592, acc 0.851675\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533921767/checkpoints/model-800\n",
      "\n",
      "2018-08-10T17:24:46.490217: step 801, loss 0.00533671, acc 1\n",
      "2018-08-10T17:24:46.638473: step 802, loss 0.00464559, acc 1\n",
      "2018-08-10T17:24:46.787774: step 803, loss 0.00455072, acc 1\n",
      "2018-08-10T17:24:46.935161: step 804, loss 0.00749961, acc 1\n",
      "2018-08-10T17:24:47.083424: step 805, loss 0.0367961, acc 0.984375\n",
      "2018-08-10T17:24:47.224917: step 806, loss 0.000907177, acc 1\n",
      "2018-08-10T17:24:47.373800: step 807, loss 0.00190536, acc 1\n",
      "2018-08-10T17:24:47.522038: step 808, loss 0.000617432, acc 1\n",
      "2018-08-10T17:24:47.666357: step 809, loss 0.00364618, acc 1\n",
      "2018-08-10T17:24:47.807361: step 810, loss 0.00300283, acc 1\n",
      "2018-08-10T17:24:47.952804: step 811, loss 0.00370861, acc 1\n",
      "2018-08-10T17:24:48.097341: step 812, loss 0.000938145, acc 1\n",
      "2018-08-10T17:24:48.243618: step 813, loss 0.0439508, acc 0.984375\n",
      "2018-08-10T17:24:48.391572: step 814, loss 0.01006, acc 1\n",
      "2018-08-10T17:24:48.541655: step 815, loss 0.00735041, acc 1\n",
      "2018-08-10T17:24:48.689067: step 816, loss 0.00221709, acc 1\n",
      "2018-08-10T17:24:48.838822: step 817, loss 0.00172637, acc 1\n",
      "2018-08-10T17:24:48.986579: step 818, loss 0.00102364, acc 1\n",
      "2018-08-10T17:24:49.133743: step 819, loss 0.00747504, acc 1\n",
      "2018-08-10T17:24:49.285546: step 820, loss 0.0878882, acc 0.984375\n",
      "2018-08-10T17:24:49.438950: step 821, loss 0.00486984, acc 1\n",
      "2018-08-10T17:24:49.587926: step 822, loss 0.00245112, acc 1\n",
      "2018-08-10T17:24:49.740160: step 823, loss 0.00321197, acc 1\n",
      "2018-08-10T17:24:49.889105: step 824, loss 0.00413316, acc 1\n",
      "2018-08-10T17:24:50.039155: step 825, loss 0.000798947, acc 1\n",
      "2018-08-10T17:24:50.187931: step 826, loss 0.00104473, acc 1\n",
      "2018-08-10T17:24:50.336621: step 827, loss 0.00299568, acc 1\n",
      "2018-08-10T17:24:50.484832: step 828, loss 0.00195018, acc 1\n",
      "2018-08-10T17:24:50.638163: step 829, loss 0.0040237, acc 1\n",
      "2018-08-10T17:24:50.789079: step 830, loss 0.000506609, acc 1\n",
      "2018-08-10T17:24:50.940953: step 831, loss 0.00212899, acc 1\n",
      "2018-08-10T17:24:51.087089: step 832, loss 0.0111778, acc 1\n",
      "2018-08-10T17:24:51.237280: step 833, loss 0.00218211, acc 1\n",
      "2018-08-10T17:24:51.386383: step 834, loss 0.00661397, acc 1\n",
      "2018-08-10T17:24:51.540173: step 835, loss 0.00169629, acc 1\n",
      "2018-08-10T17:24:51.691378: step 836, loss 0.00082631, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:24:51.846003: step 837, loss 0.0849945, acc 0.984375\n",
      "2018-08-10T17:24:51.999228: step 838, loss 0.00219173, acc 1\n",
      "2018-08-10T17:24:52.152710: step 839, loss 0.00352016, acc 1\n",
      "2018-08-10T17:24:52.301783: step 840, loss 0.00118254, acc 1\n",
      "2018-08-10T17:24:52.453427: step 841, loss 0.00215957, acc 1\n",
      "2018-08-10T17:24:52.602969: step 842, loss 0.00526626, acc 1\n",
      "2018-08-10T17:24:52.753012: step 843, loss 0.01892, acc 0.984375\n",
      "2018-08-10T17:24:52.903347: step 844, loss 0.000409026, acc 1\n",
      "2018-08-10T17:24:53.048989: step 845, loss 0.00488655, acc 1\n",
      "2018-08-10T17:24:53.196461: step 846, loss 0.0146839, acc 1\n",
      "2018-08-10T17:24:53.344570: step 847, loss 0.0014767, acc 1\n",
      "2018-08-10T17:24:53.493471: step 848, loss 0.00230529, acc 1\n",
      "2018-08-10T17:24:53.647897: step 849, loss 0.00571589, acc 1\n",
      "2018-08-10T17:24:53.788511: step 850, loss 0.0168248, acc 1\n",
      "2018-08-10T17:24:53.929068: step 851, loss 0.00673994, acc 1\n",
      "2018-08-10T17:24:54.069813: step 852, loss 0.00259712, acc 1\n",
      "2018-08-10T17:24:54.208016: step 853, loss 0.00286884, acc 1\n",
      "2018-08-10T17:24:54.347212: step 854, loss 0.00268019, acc 1\n",
      "2018-08-10T17:24:54.488217: step 855, loss 0.00308646, acc 1\n",
      "2018-08-10T17:24:54.629090: step 856, loss 0.00102759, acc 1\n",
      "2018-08-10T17:24:54.769473: step 857, loss 0.00497633, acc 1\n",
      "2018-08-10T17:24:54.907200: step 858, loss 0.0113881, acc 1\n",
      "2018-08-10T17:24:55.047876: step 859, loss 0.00227526, acc 1\n",
      "2018-08-10T17:24:55.189166: step 860, loss 0.00400374, acc 1\n",
      "2018-08-10T17:24:55.330426: step 861, loss 0.00344332, acc 1\n",
      "2018-08-10T17:24:55.472414: step 862, loss 0.00249203, acc 1\n",
      "2018-08-10T17:24:55.614132: step 863, loss 0.00358323, acc 1\n",
      "2018-08-10T17:24:55.752297: step 864, loss 0.000112342, acc 1\n",
      "2018-08-10T17:24:55.892591: step 865, loss 0.0163677, acc 0.984375\n",
      "2018-08-10T17:24:56.033078: step 866, loss 0.0018618, acc 1\n",
      "2018-08-10T17:24:56.173187: step 867, loss 0.00216843, acc 1\n",
      "2018-08-10T17:24:56.312628: step 868, loss 0.00301752, acc 1\n",
      "2018-08-10T17:24:56.453506: step 869, loss 0.00127593, acc 1\n",
      "2018-08-10T17:24:56.597073: step 870, loss 0.00626116, acc 1\n",
      "2018-08-10T17:24:56.734606: step 871, loss 0.0187176, acc 0.983871\n",
      "2018-08-10T17:24:56.874958: step 872, loss 0.0101169, acc 1\n",
      "2018-08-10T17:24:57.012200: step 873, loss 0.012102, acc 0.984375\n",
      "2018-08-10T17:24:57.152843: step 874, loss 0.00111057, acc 1\n",
      "2018-08-10T17:24:57.293351: step 875, loss 0.00253766, acc 1\n",
      "2018-08-10T17:24:57.435773: step 876, loss 0.000415009, acc 1\n",
      "2018-08-10T17:24:57.575868: step 877, loss 0.00165192, acc 1\n",
      "2018-08-10T17:24:57.716436: step 878, loss 0.00229946, acc 1\n",
      "2018-08-10T17:24:57.856629: step 879, loss 0.00320997, acc 1\n",
      "2018-08-10T17:24:57.998961: step 880, loss 0.000259638, acc 1\n",
      "2018-08-10T17:24:58.143094: step 881, loss 0.00238509, acc 1\n",
      "2018-08-10T17:24:58.288711: step 882, loss 0.000637993, acc 1\n",
      "2018-08-10T17:24:58.435795: step 883, loss 0.024757, acc 0.984375\n",
      "2018-08-10T17:24:58.579236: step 884, loss 0.00519457, acc 1\n",
      "2018-08-10T17:24:58.727263: step 885, loss 0.00601731, acc 1\n",
      "2018-08-10T17:24:58.874108: step 886, loss 0.00166245, acc 1\n",
      "2018-08-10T17:24:59.021225: step 887, loss 0.000275264, acc 1\n",
      "2018-08-10T17:24:59.166184: step 888, loss 0.0247205, acc 0.984375\n",
      "2018-08-10T17:24:59.312646: step 889, loss 0.00608525, acc 1\n",
      "2018-08-10T17:24:59.464355: step 890, loss 0.00388662, acc 1\n",
      "2018-08-10T17:24:59.613576: step 891, loss 0.00467135, acc 1\n",
      "2018-08-10T17:24:59.759796: step 892, loss 0.00152082, acc 1\n",
      "2018-08-10T17:24:59.906652: step 893, loss 0.00128363, acc 1\n",
      "2018-08-10T17:25:00.053652: step 894, loss 0.000620524, acc 1\n",
      "2018-08-10T17:25:00.200948: step 895, loss 0.00353789, acc 1\n",
      "2018-08-10T17:25:00.346588: step 896, loss 0.00267932, acc 1\n",
      "2018-08-10T17:25:00.490751: step 897, loss 0.000842842, acc 1\n",
      "2018-08-10T17:25:00.639664: step 898, loss 0.000553816, acc 1\n",
      "2018-08-10T17:25:00.785693: step 899, loss 0.00716641, acc 1\n",
      "2018-08-10T17:25:00.933265: step 900, loss 0.00111315, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:25:01.074832: step 900, loss 0.756358, acc 0.856459\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533921767/checkpoints/model-900\n",
      "\n",
      "2018-08-10T17:25:01.280121: step 901, loss 0.0394522, acc 0.984375\n",
      "2018-08-10T17:25:01.427816: step 902, loss 0.00148334, acc 1\n",
      "2018-08-10T17:25:01.573885: step 903, loss 0.000956996, acc 1\n",
      "2018-08-10T17:25:01.724601: step 904, loss 0.00129691, acc 1\n",
      "2018-08-10T17:25:01.871862: step 905, loss 0.00227721, acc 1\n",
      "2018-08-10T17:25:02.020614: step 906, loss 0.0024156, acc 1\n",
      "2018-08-10T17:25:02.166407: step 907, loss 0.0090767, acc 1\n",
      "2018-08-10T17:25:02.313165: step 908, loss 0.00258934, acc 1\n",
      "2018-08-10T17:25:02.458077: step 909, loss 0.000545494, acc 1\n",
      "2018-08-10T17:25:02.600384: step 910, loss 0.00315137, acc 1\n",
      "2018-08-10T17:25:02.748826: step 911, loss 0.0381176, acc 0.984375\n",
      "2018-08-10T17:25:02.895726: step 912, loss 0.0035821, acc 1\n",
      "2018-08-10T17:25:03.045274: step 913, loss 0.00125063, acc 1\n",
      "2018-08-10T17:25:03.193048: step 914, loss 0.00161697, acc 1\n",
      "2018-08-10T17:25:03.341653: step 915, loss 0.00463301, acc 1\n",
      "2018-08-10T17:25:03.491019: step 916, loss 0.00251995, acc 1\n",
      "2018-08-10T17:25:03.640431: step 917, loss 0.00254299, acc 1\n",
      "2018-08-10T17:25:03.788784: step 918, loss 0.00188313, acc 1\n",
      "2018-08-10T17:25:03.936792: step 919, loss 0.101056, acc 0.96875\n",
      "2018-08-10T17:25:04.083115: step 920, loss 0.00904569, acc 1\n",
      "2018-08-10T17:25:04.231647: step 921, loss 0.00171057, acc 1\n",
      "2018-08-10T17:25:04.378582: step 922, loss 0.00183115, acc 1\n",
      "2018-08-10T17:25:04.520259: step 923, loss 0.000536, acc 1\n",
      "2018-08-10T17:25:04.668367: step 924, loss 0.00291175, acc 1\n",
      "2018-08-10T17:25:04.816992: step 925, loss 0.00463226, acc 1\n",
      "2018-08-10T17:25:04.961639: step 926, loss 0.00165214, acc 1\n",
      "2018-08-10T17:25:05.114137: step 927, loss 0.000282921, acc 1\n",
      "2018-08-10T17:25:05.258251: step 928, loss 0.000819711, acc 1\n",
      "2018-08-10T17:25:05.405840: step 929, loss 0.00355082, acc 1\n",
      "2018-08-10T17:25:05.557719: step 930, loss 0.00120319, acc 1\n",
      "2018-08-10T17:25:05.707213: step 931, loss 0.00213338, acc 1\n",
      "2018-08-10T17:25:05.854141: step 932, loss 0.0116669, acc 1\n",
      "2018-08-10T17:25:06.004080: step 933, loss 0.00131742, acc 1\n",
      "2018-08-10T17:25:06.150844: step 934, loss 0.000415757, acc 1\n",
      "2018-08-10T17:25:06.308679: step 935, loss 0.0182403, acc 0.984375\n",
      "2018-08-10T17:25:06.452195: step 936, loss 0.00166804, acc 1\n",
      "2018-08-10T17:25:06.601370: step 937, loss 0.00720437, acc 1\n",
      "2018-08-10T17:25:06.746537: step 938, loss 0.00118893, acc 1\n",
      "2018-08-10T17:25:06.892910: step 939, loss 0.00708586, acc 1\n",
      "2018-08-10T17:25:07.037727: step 940, loss 0.0050279, acc 1\n",
      "2018-08-10T17:25:07.185255: step 941, loss 0.00138759, acc 1\n",
      "2018-08-10T17:25:07.335272: step 942, loss 0.00567958, acc 1\n",
      "2018-08-10T17:25:07.485443: step 943, loss 0.00490249, acc 1\n",
      "2018-08-10T17:25:07.633309: step 944, loss 0.00322256, acc 1\n",
      "2018-08-10T17:25:07.780697: step 945, loss 0.00282398, acc 1\n",
      "2018-08-10T17:25:07.927960: step 946, loss 0.0031345, acc 1\n",
      "2018-08-10T17:25:08.074469: step 947, loss 0.00192608, acc 1\n",
      "2018-08-10T17:25:08.223854: step 948, loss 0.00101626, acc 1\n",
      "2018-08-10T17:25:08.369600: step 949, loss 0.00170192, acc 1\n",
      "2018-08-10T17:25:08.517677: step 950, loss 0.000861677, acc 1\n",
      "2018-08-10T17:25:08.658403: step 951, loss 0.00474748, acc 1\n",
      "2018-08-10T17:25:08.799648: step 952, loss 0.000759024, acc 1\n",
      "2018-08-10T17:25:08.942768: step 953, loss 0.000852247, acc 1\n",
      "2018-08-10T17:25:09.087302: step 954, loss 0.0111895, acc 1\n",
      "2018-08-10T17:25:09.235202: step 955, loss 0.00245212, acc 1\n",
      "2018-08-10T17:25:09.385991: step 956, loss 0.00102136, acc 1\n",
      "2018-08-10T17:25:09.535998: step 957, loss 0.00624342, acc 1\n",
      "2018-08-10T17:25:09.685237: step 958, loss 0.00182776, acc 1\n",
      "2018-08-10T17:25:09.833546: step 959, loss 0.000836586, acc 1\n",
      "2018-08-10T17:25:09.981612: step 960, loss 0.0183446, acc 0.984375\n",
      "2018-08-10T17:25:10.131362: step 961, loss 0.00494908, acc 1\n",
      "2018-08-10T17:25:10.276300: step 962, loss 0.00197178, acc 1\n",
      "2018-08-10T17:25:10.425424: step 963, loss 0.0141591, acc 1\n",
      "2018-08-10T17:25:10.573175: step 964, loss 0.000597083, acc 1\n",
      "2018-08-10T17:25:10.726560: step 965, loss 0.00115372, acc 1\n",
      "2018-08-10T17:25:10.875992: step 966, loss 0.00022592, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:25:11.026989: step 967, loss 0.00155038, acc 1\n",
      "2018-08-10T17:25:11.173475: step 968, loss 0.00565337, acc 1\n",
      "2018-08-10T17:25:11.322888: step 969, loss 0.00549612, acc 1\n",
      "2018-08-10T17:25:11.476597: step 970, loss 0.00903613, acc 1\n",
      "2018-08-10T17:25:11.627125: step 971, loss 0.00132258, acc 1\n",
      "2018-08-10T17:25:11.778206: step 972, loss 0.00129533, acc 1\n",
      "2018-08-10T17:25:11.931426: step 973, loss 0.00468142, acc 1\n",
      "2018-08-10T17:25:12.079599: step 974, loss 0.000242134, acc 1\n",
      "2018-08-10T17:25:12.224729: step 975, loss 0.0366352, acc 0.983871\n",
      "2018-08-10T17:25:12.371250: step 976, loss 0.000821832, acc 1\n",
      "2018-08-10T17:25:12.522366: step 977, loss 0.0019389, acc 1\n",
      "2018-08-10T17:25:12.668623: step 978, loss 0.000432616, acc 1\n",
      "2018-08-10T17:25:12.817606: step 979, loss 0.00240224, acc 1\n",
      "2018-08-10T17:25:12.965016: step 980, loss 0.0537746, acc 0.984375\n",
      "2018-08-10T17:25:13.111276: step 981, loss 0.010519, acc 1\n",
      "2018-08-10T17:25:13.259675: step 982, loss 0.00642138, acc 1\n",
      "2018-08-10T17:25:13.407575: step 983, loss 0.00142721, acc 1\n",
      "2018-08-10T17:25:13.557061: step 984, loss 0.000243048, acc 1\n",
      "2018-08-10T17:25:13.706779: step 985, loss 0.000913711, acc 1\n",
      "2018-08-10T17:25:13.856850: step 986, loss 0.00459898, acc 1\n",
      "2018-08-10T17:25:14.005359: step 987, loss 0.0022193, acc 1\n",
      "2018-08-10T17:25:14.148486: step 988, loss 0.0013325, acc 1\n",
      "2018-08-10T17:25:14.299130: step 989, loss 0.0053925, acc 1\n",
      "2018-08-10T17:25:14.448820: step 990, loss 0.00521144, acc 1\n",
      "2018-08-10T17:25:14.599959: step 991, loss 0.00337868, acc 1\n",
      "2018-08-10T17:25:14.742067: step 992, loss 0.00140916, acc 1\n",
      "2018-08-10T17:25:14.881081: step 993, loss 0.00241607, acc 1\n",
      "2018-08-10T17:25:15.021941: step 994, loss 0.00302973, acc 1\n",
      "2018-08-10T17:25:15.161164: step 995, loss 0.000256951, acc 1\n",
      "2018-08-10T17:25:15.299604: step 996, loss 0.00083483, acc 1\n",
      "2018-08-10T17:25:15.441712: step 997, loss 0.000805995, acc 1\n",
      "2018-08-10T17:25:15.579993: step 998, loss 0.00223189, acc 1\n",
      "2018-08-10T17:25:15.720505: step 999, loss 0.00274864, acc 1\n",
      "2018-08-10T17:25:15.858959: step 1000, loss 0.00306473, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:25:15.994633: step 1000, loss 0.728381, acc 0.861244\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533921767/checkpoints/model-1000\n",
      "\n",
      "2018-08-10T17:25:16.184267: step 1001, loss 0.00638046, acc 1\n",
      "2018-08-10T17:25:16.323496: step 1002, loss 0.00210328, acc 1\n",
      "2018-08-10T17:25:16.462131: step 1003, loss 0.00146266, acc 1\n",
      "2018-08-10T17:25:16.600965: step 1004, loss 0.00950935, acc 1\n",
      "2018-08-10T17:25:16.738917: step 1005, loss 0.00263171, acc 1\n",
      "2018-08-10T17:25:16.879926: step 1006, loss 0.00614552, acc 1\n",
      "2018-08-10T17:25:17.016459: step 1007, loss 0.0101789, acc 1\n",
      "2018-08-10T17:25:17.156310: step 1008, loss 0.0158827, acc 1\n",
      "2018-08-10T17:25:17.296077: step 1009, loss 0.00349782, acc 1\n",
      "2018-08-10T17:25:17.438563: step 1010, loss 0.000706897, acc 1\n",
      "2018-08-10T17:25:17.579138: step 1011, loss 0.00264506, acc 1\n",
      "2018-08-10T17:25:17.719789: step 1012, loss 0.00245097, acc 1\n",
      "2018-08-10T17:25:17.861062: step 1013, loss 0.000528176, acc 1\n",
      "2018-08-10T17:25:17.998483: step 1014, loss 0.00393388, acc 1\n",
      "2018-08-10T17:25:18.137476: step 1015, loss 0.00105028, acc 1\n",
      "2018-08-10T17:25:18.275010: step 1016, loss 0.0039803, acc 1\n",
      "2018-08-10T17:25:18.413956: step 1017, loss 0.000829983, acc 1\n",
      "2018-08-10T17:25:18.550901: step 1018, loss 0.00082269, acc 1\n",
      "2018-08-10T17:25:18.691159: step 1019, loss 0.00126768, acc 1\n",
      "2018-08-10T17:25:18.828919: step 1020, loss 0.003901, acc 1\n",
      "2018-08-10T17:25:18.969813: step 1021, loss 0.0204425, acc 0.984375\n",
      "2018-08-10T17:25:19.110535: step 1022, loss 0.00116665, acc 1\n",
      "2018-08-10T17:25:19.256307: step 1023, loss 0.00669936, acc 1\n",
      "2018-08-10T17:25:19.404242: step 1024, loss 0.00106867, acc 1\n",
      "2018-08-10T17:25:19.553201: step 1025, loss 0.00550947, acc 1\n",
      "2018-08-10T17:25:19.703268: step 1026, loss 0.0134209, acc 1\n",
      "2018-08-10T17:25:19.844815: step 1027, loss 0.02236, acc 0.983871\n",
      "2018-08-10T17:25:19.991769: step 1028, loss 0.00033674, acc 1\n",
      "2018-08-10T17:25:20.141017: step 1029, loss 0.00017784, acc 1\n",
      "2018-08-10T17:25:20.287425: step 1030, loss 0.00540179, acc 1\n",
      "2018-08-10T17:25:20.433866: step 1031, loss 0.00298962, acc 1\n",
      "2018-08-10T17:25:20.583627: step 1032, loss 0.0109221, acc 1\n",
      "2018-08-10T17:25:20.732149: step 1033, loss 0.00671631, acc 1\n",
      "2018-08-10T17:25:20.879241: step 1034, loss 0.00153065, acc 1\n",
      "2018-08-10T17:25:21.024894: step 1035, loss 0.00257931, acc 1\n",
      "2018-08-10T17:25:21.173092: step 1036, loss 0.00549996, acc 1\n",
      "2018-08-10T17:25:21.318882: step 1037, loss 0.000308433, acc 1\n",
      "2018-08-10T17:25:21.470086: step 1038, loss 0.000726744, acc 1\n",
      "2018-08-10T17:25:21.620253: step 1039, loss 0.000279255, acc 1\n",
      "2018-08-10T17:25:21.762552: step 1040, loss 0.017874, acc 0.983871\n",
      "2018-08-10T17:25:21.909195: step 1041, loss 0.00712662, acc 1\n",
      "2018-08-10T17:25:22.056614: step 1042, loss 0.00480519, acc 1\n",
      "2018-08-10T17:25:22.203588: step 1043, loss 0.0352587, acc 0.984375\n",
      "2018-08-10T17:25:22.349821: step 1044, loss 0.000733822, acc 1\n",
      "2018-08-10T17:25:22.494339: step 1045, loss 0.000909861, acc 1\n",
      "2018-08-10T17:25:22.643000: step 1046, loss 0.0038827, acc 1\n",
      "2018-08-10T17:25:22.789237: step 1047, loss 0.000736161, acc 1\n",
      "2018-08-10T17:25:22.935956: step 1048, loss 0.00915402, acc 1\n",
      "2018-08-10T17:25:23.082199: step 1049, loss 0.00204264, acc 1\n",
      "2018-08-10T17:25:23.227751: step 1050, loss 0.00119247, acc 1\n",
      "2018-08-10T17:25:23.380797: step 1051, loss 0.00429351, acc 1\n",
      "2018-08-10T17:25:23.532180: step 1052, loss 0.000190385, acc 1\n",
      "2018-08-10T17:25:23.677338: step 1053, loss 0.0014845, acc 1\n",
      "2018-08-10T17:25:23.823562: step 1054, loss 0.000234098, acc 1\n",
      "2018-08-10T17:25:23.968682: step 1055, loss 0.00608114, acc 1\n",
      "2018-08-10T17:25:24.116063: step 1056, loss 0.00125079, acc 1\n",
      "2018-08-10T17:25:24.263079: step 1057, loss 0.000953565, acc 1\n",
      "2018-08-10T17:25:24.412630: step 1058, loss 0.0100122, acc 1\n",
      "2018-08-10T17:25:24.556380: step 1059, loss 0.000746221, acc 1\n",
      "2018-08-10T17:25:24.705699: step 1060, loss 0.00285719, acc 1\n",
      "2018-08-10T17:25:24.852043: step 1061, loss 0.00132943, acc 1\n",
      "2018-08-10T17:25:24.998256: step 1062, loss 0.00119431, acc 1\n",
      "2018-08-10T17:25:25.145773: step 1063, loss 0.000892875, acc 1\n",
      "2018-08-10T17:25:25.291727: step 1064, loss 0.00040839, acc 1\n",
      "2018-08-10T17:25:25.436353: step 1065, loss 0.00296691, acc 1\n",
      "2018-08-10T17:25:25.577321: step 1066, loss 0.00147721, acc 1\n",
      "2018-08-10T17:25:25.725754: step 1067, loss 0.0185889, acc 1\n",
      "2018-08-10T17:25:25.874791: step 1068, loss 0.00238993, acc 1\n",
      "2018-08-10T17:25:26.022366: step 1069, loss 0.000444164, acc 1\n",
      "2018-08-10T17:25:26.168169: step 1070, loss 0.0265783, acc 0.984375\n",
      "2018-08-10T17:25:26.315573: step 1071, loss 0.0114185, acc 1\n",
      "2018-08-10T17:25:26.460828: step 1072, loss 0.000226233, acc 1\n",
      "2018-08-10T17:25:26.607792: step 1073, loss 0.00136841, acc 1\n",
      "2018-08-10T17:25:26.764418: step 1074, loss 0.000794141, acc 1\n",
      "2018-08-10T17:25:26.914617: step 1075, loss 0.00687571, acc 1\n",
      "2018-08-10T17:25:27.060883: step 1076, loss 0.000240575, acc 1\n",
      "2018-08-10T17:25:27.205940: step 1077, loss 0.00125959, acc 1\n",
      "2018-08-10T17:25:27.350101: step 1078, loss 0.000838157, acc 1\n",
      "2018-08-10T17:25:27.493925: step 1079, loss 0.00326733, acc 1\n",
      "2018-08-10T17:25:27.644722: step 1080, loss 0.00305513, acc 1\n",
      "2018-08-10T17:25:27.788667: step 1081, loss 0.000524019, acc 1\n",
      "2018-08-10T17:25:27.934903: step 1082, loss 0.00532679, acc 1\n",
      "2018-08-10T17:25:28.082016: step 1083, loss 0.000337644, acc 1\n",
      "2018-08-10T17:25:28.227945: step 1084, loss 0.0106223, acc 1\n",
      "2018-08-10T17:25:28.373597: step 1085, loss 0.00173771, acc 1\n",
      "2018-08-10T17:25:28.521242: step 1086, loss 0.00105181, acc 1\n",
      "2018-08-10T17:25:28.667003: step 1087, loss 0.00602745, acc 1\n",
      "2018-08-10T17:25:28.815068: step 1088, loss 0.00126829, acc 1\n",
      "2018-08-10T17:25:28.961303: step 1089, loss 0.00398343, acc 1\n",
      "2018-08-10T17:25:29.108954: step 1090, loss 0.000436011, acc 1\n",
      "2018-08-10T17:25:29.253260: step 1091, loss 0.00195027, acc 1\n",
      "2018-08-10T17:25:29.394626: step 1092, loss 0.000214017, acc 1\n",
      "2018-08-10T17:25:29.537332: step 1093, loss 0.00161362, acc 1\n",
      "2018-08-10T17:25:29.680406: step 1094, loss 0.00239767, acc 1\n",
      "2018-08-10T17:25:29.823156: step 1095, loss 0.000977628, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:25:29.967369: step 1096, loss 0.00284458, acc 1\n",
      "2018-08-10T17:25:30.116403: step 1097, loss 0.000984323, acc 1\n",
      "2018-08-10T17:25:30.263870: step 1098, loss 0.000673429, acc 1\n",
      "2018-08-10T17:25:30.410252: step 1099, loss 0.00137088, acc 1\n",
      "2018-08-10T17:25:30.561193: step 1100, loss 0.0537747, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:25:30.704069: step 1100, loss 0.768863, acc 0.851675\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533921767/checkpoints/model-1100\n",
      "\n",
      "2018-08-10T17:25:30.912323: step 1101, loss 0.0108119, acc 1\n",
      "2018-08-10T17:25:31.064330: step 1102, loss 0.0002894, acc 1\n",
      "2018-08-10T17:25:31.213538: step 1103, loss 0.00320844, acc 1\n",
      "2018-08-10T17:25:31.364729: step 1104, loss 0.00170706, acc 1\n",
      "2018-08-10T17:25:31.517408: step 1105, loss 0.000134837, acc 1\n",
      "2018-08-10T17:25:31.668790: step 1106, loss 0.00999733, acc 1\n",
      "2018-08-10T17:25:31.819418: step 1107, loss 0.0172541, acc 1\n",
      "2018-08-10T17:25:31.968559: step 1108, loss 0.00847936, acc 1\n",
      "2018-08-10T17:25:32.116720: step 1109, loss 0.0153285, acc 0.984375\n",
      "2018-08-10T17:25:32.263629: step 1110, loss 0.00186112, acc 1\n",
      "2018-08-10T17:25:32.412742: step 1111, loss 0.000703456, acc 1\n",
      "2018-08-10T17:25:32.561613: step 1112, loss 0.00923467, acc 1\n",
      "2018-08-10T17:25:32.712099: step 1113, loss 0.000611754, acc 1\n",
      "2018-08-10T17:25:32.858953: step 1114, loss 0.00040543, acc 1\n",
      "2018-08-10T17:25:33.009724: step 1115, loss 0.00170364, acc 1\n",
      "2018-08-10T17:25:33.158553: step 1116, loss 0.000478482, acc 1\n",
      "2018-08-10T17:25:33.306300: step 1117, loss 0.00528408, acc 1\n",
      "2018-08-10T17:25:33.451598: step 1118, loss 0.00108999, acc 1\n",
      "2018-08-10T17:25:33.601242: step 1119, loss 0.000298749, acc 1\n",
      "2018-08-10T17:25:33.746088: step 1120, loss 0.000879299, acc 1\n",
      "2018-08-10T17:25:33.892798: step 1121, loss 0.000204565, acc 1\n",
      "2018-08-10T17:25:34.040739: step 1122, loss 0.000949929, acc 1\n",
      "2018-08-10T17:25:34.190242: step 1123, loss 0.0025488, acc 1\n",
      "2018-08-10T17:25:34.338103: step 1124, loss 0.0131577, acc 0.984375\n",
      "2018-08-10T17:25:34.487867: step 1125, loss 0.00110903, acc 1\n",
      "2018-08-10T17:25:34.636277: step 1126, loss 0.0261066, acc 0.984375\n",
      "2018-08-10T17:25:34.786266: step 1127, loss 0.00036715, acc 1\n",
      "2018-08-10T17:25:34.937042: step 1128, loss 0.0446315, acc 0.984375\n",
      "2018-08-10T17:25:35.088638: step 1129, loss 0.00095097, acc 1\n",
      "2018-08-10T17:25:35.238722: step 1130, loss 0.0019444, acc 1\n",
      "2018-08-10T17:25:35.386058: step 1131, loss 0.000324331, acc 1\n",
      "2018-08-10T17:25:35.534655: step 1132, loss 0.000193241, acc 1\n",
      "2018-08-10T17:25:35.676328: step 1133, loss 0.0153135, acc 1\n",
      "2018-08-10T17:25:35.816214: step 1134, loss 0.000245609, acc 1\n",
      "2018-08-10T17:25:35.959735: step 1135, loss 0.00128005, acc 1\n",
      "2018-08-10T17:25:36.099335: step 1136, loss 0.00359901, acc 1\n",
      "2018-08-10T17:25:36.240087: step 1137, loss 0.00537706, acc 1\n",
      "2018-08-10T17:25:36.380145: step 1138, loss 0.000364111, acc 1\n",
      "2018-08-10T17:25:36.520924: step 1139, loss 0.000200481, acc 1\n",
      "2018-08-10T17:25:36.660969: step 1140, loss 0.000523213, acc 1\n",
      "2018-08-10T17:25:36.800715: step 1141, loss 0.00180836, acc 1\n",
      "2018-08-10T17:25:36.942048: step 1142, loss 0.000598685, acc 1\n",
      "2018-08-10T17:25:37.085407: step 1143, loss 0.00271976, acc 1\n",
      "2018-08-10T17:25:37.222223: step 1144, loss 0.00813221, acc 1\n",
      "2018-08-10T17:25:37.366203: step 1145, loss 0.00333048, acc 1\n",
      "2018-08-10T17:25:37.511809: step 1146, loss 0.0208489, acc 0.984375\n",
      "2018-08-10T17:25:37.652267: step 1147, loss 0.0182464, acc 0.984375\n",
      "2018-08-10T17:25:37.791882: step 1148, loss 0.00031214, acc 1\n",
      "2018-08-10T17:25:37.931752: step 1149, loss 0.000551829, acc 1\n",
      "2018-08-10T17:25:38.073171: step 1150, loss 0.000740257, acc 1\n",
      "2018-08-10T17:25:38.212473: step 1151, loss 0.0117743, acc 1\n",
      "2018-08-10T17:25:38.354184: step 1152, loss 0.000368415, acc 1\n",
      "2018-08-10T17:25:38.495278: step 1153, loss 0.000576973, acc 1\n",
      "2018-08-10T17:25:38.635402: step 1154, loss 0.00275802, acc 1\n",
      "2018-08-10T17:25:38.777975: step 1155, loss 0.00118552, acc 1\n",
      "2018-08-10T17:25:38.916759: step 1156, loss 0.0409321, acc 0.984375\n",
      "2018-08-10T17:25:39.054573: step 1157, loss 0.00114616, acc 1\n",
      "2018-08-10T17:25:39.197086: step 1158, loss 0.0111578, acc 1\n",
      "2018-08-10T17:25:39.338226: step 1159, loss 0.000157769, acc 1\n",
      "2018-08-10T17:25:39.481381: step 1160, loss 0.0128651, acc 1\n",
      "2018-08-10T17:25:39.624173: step 1161, loss 0.0296271, acc 0.984375\n",
      "2018-08-10T17:25:39.764514: step 1162, loss 0.00239163, acc 1\n",
      "2018-08-10T17:25:39.907494: step 1163, loss 0.00180207, acc 1\n",
      "2018-08-10T17:25:40.053082: step 1164, loss 0.000282185, acc 1\n",
      "2018-08-10T17:25:40.199130: step 1165, loss 0.0306427, acc 0.984375\n",
      "2018-08-10T17:25:40.346684: step 1166, loss 0.000652885, acc 1\n",
      "2018-08-10T17:25:40.494782: step 1167, loss 0.00217988, acc 1\n",
      "2018-08-10T17:25:40.640796: step 1168, loss 0.0047866, acc 1\n",
      "2018-08-10T17:25:40.787011: step 1169, loss 0.0270446, acc 0.984375\n",
      "2018-08-10T17:25:40.932994: step 1170, loss 0.00128178, acc 1\n",
      "2018-08-10T17:25:41.078628: step 1171, loss 0.000297945, acc 1\n",
      "2018-08-10T17:25:41.224580: step 1172, loss 0.0012175, acc 1\n",
      "2018-08-10T17:25:41.372120: step 1173, loss 0.00303558, acc 1\n",
      "2018-08-10T17:25:41.526748: step 1174, loss 0.00453575, acc 1\n",
      "2018-08-10T17:25:41.674625: step 1175, loss 0.000342681, acc 1\n",
      "2018-08-10T17:25:41.824456: step 1176, loss 0.00119877, acc 1\n",
      "2018-08-10T17:25:41.976170: step 1177, loss 0.00747714, acc 1\n",
      "2018-08-10T17:25:42.122329: step 1178, loss 0.00231002, acc 1\n",
      "2018-08-10T17:25:42.267801: step 1179, loss 0.000227582, acc 1\n",
      "2018-08-10T17:25:42.413615: step 1180, loss 0.000274114, acc 1\n",
      "2018-08-10T17:25:42.565721: step 1181, loss 0.00183887, acc 1\n",
      "2018-08-10T17:25:42.711143: step 1182, loss 0.000165521, acc 1\n",
      "2018-08-10T17:25:42.855173: step 1183, loss 0.000681015, acc 1\n",
      "2018-08-10T17:25:43.001302: step 1184, loss 0.00163453, acc 1\n",
      "2018-08-10T17:25:43.152624: step 1185, loss 0.00924983, acc 1\n",
      "2018-08-10T17:25:43.302320: step 1186, loss 0.00119977, acc 1\n",
      "2018-08-10T17:25:43.456775: step 1187, loss 0.0184136, acc 0.984375\n",
      "2018-08-10T17:25:43.608753: step 1188, loss 0.000993453, acc 1\n",
      "2018-08-10T17:25:43.756307: step 1189, loss 0.00132315, acc 1\n",
      "2018-08-10T17:25:43.902210: step 1190, loss 0.000273871, acc 1\n",
      "2018-08-10T17:25:44.051734: step 1191, loss 0.00140435, acc 1\n",
      "2018-08-10T17:25:44.198261: step 1192, loss 0.00138335, acc 1\n",
      "2018-08-10T17:25:44.347196: step 1193, loss 0.000422416, acc 1\n",
      "2018-08-10T17:25:44.497246: step 1194, loss 0.0017042, acc 1\n",
      "2018-08-10T17:25:44.646767: step 1195, loss 0.000533338, acc 1\n",
      "2018-08-10T17:25:44.792265: step 1196, loss 0.00273084, acc 1\n",
      "2018-08-10T17:25:44.942496: step 1197, loss 0.00136585, acc 1\n",
      "2018-08-10T17:25:45.089689: step 1198, loss 0.00281604, acc 1\n",
      "2018-08-10T17:25:45.236460: step 1199, loss 0.000773529, acc 1\n",
      "2018-08-10T17:25:45.383229: step 1200, loss 0.00146158, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:25:45.527035: step 1200, loss 0.840546, acc 0.842105\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533921767/checkpoints/model-1200\n",
      "\n",
      "2018-08-10T17:25:45.736387: step 1201, loss 0.000520415, acc 1\n",
      "2018-08-10T17:25:45.887193: step 1202, loss 0.000237308, acc 1\n",
      "2018-08-10T17:25:46.035662: step 1203, loss 0.000913488, acc 1\n",
      "2018-08-10T17:25:46.182934: step 1204, loss 0.00011831, acc 1\n",
      "2018-08-10T17:25:46.332475: step 1205, loss 0.000438924, acc 1\n",
      "2018-08-10T17:25:46.478999: step 1206, loss 0.000126195, acc 1\n",
      "2018-08-10T17:25:46.629026: step 1207, loss 0.00128553, acc 1\n",
      "2018-08-10T17:25:46.773424: step 1208, loss 0.000192007, acc 1\n",
      "2018-08-10T17:25:46.918680: step 1209, loss 0.0174807, acc 0.983871\n",
      "2018-08-10T17:25:47.066305: step 1210, loss 0.0175986, acc 0.984375\n",
      "2018-08-10T17:25:47.212973: step 1211, loss 0.00238717, acc 1\n",
      "2018-08-10T17:25:47.362906: step 1212, loss 0.00135184, acc 1\n",
      "2018-08-10T17:25:47.513567: step 1213, loss 0.00192786, acc 1\n",
      "2018-08-10T17:25:47.661882: step 1214, loss 0.00264058, acc 1\n",
      "2018-08-10T17:25:47.812062: step 1215, loss 0.000481767, acc 1\n",
      "2018-08-10T17:25:47.958773: step 1216, loss 0.000264621, acc 1\n",
      "2018-08-10T17:25:48.102352: step 1217, loss 0.000358265, acc 1\n",
      "2018-08-10T17:25:48.248625: step 1218, loss 0.000554631, acc 1\n",
      "2018-08-10T17:25:48.395473: step 1219, loss 0.00240829, acc 1\n",
      "2018-08-10T17:25:48.543496: step 1220, loss 0.00103684, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:25:48.690304: step 1221, loss 0.000318218, acc 1\n",
      "2018-08-10T17:25:48.832619: step 1222, loss 9.69978e-05, acc 1\n",
      "2018-08-10T17:25:48.979956: step 1223, loss 0.000626904, acc 1\n",
      "2018-08-10T17:25:49.126750: step 1224, loss 0.00367111, acc 1\n",
      "2018-08-10T17:25:49.274326: step 1225, loss 0.000908447, acc 1\n",
      "2018-08-10T17:25:49.419474: step 1226, loss 9.43982e-05, acc 1\n",
      "2018-08-10T17:25:49.570837: step 1227, loss 0.00411709, acc 1\n",
      "2018-08-10T17:25:49.716561: step 1228, loss 0.000869476, acc 1\n",
      "2018-08-10T17:25:49.861083: step 1229, loss 0.000584074, acc 1\n",
      "2018-08-10T17:25:50.006064: step 1230, loss 0.000539206, acc 1\n",
      "2018-08-10T17:25:50.151652: step 1231, loss 0.00197025, acc 1\n",
      "2018-08-10T17:25:50.296738: step 1232, loss 0.000354987, acc 1\n",
      "2018-08-10T17:25:50.438804: step 1233, loss 0.000531066, acc 1\n",
      "2018-08-10T17:25:50.577826: step 1234, loss 0.00127996, acc 1\n",
      "2018-08-10T17:25:50.716828: step 1235, loss 0.001715, acc 1\n",
      "2018-08-10T17:25:50.860202: step 1236, loss 0.000147325, acc 1\n",
      "2018-08-10T17:25:51.005255: step 1237, loss 0.000483046, acc 1\n",
      "2018-08-10T17:25:51.153952: step 1238, loss 0.00056719, acc 1\n",
      "2018-08-10T17:25:51.305179: step 1239, loss 0.00293806, acc 1\n",
      "2018-08-10T17:25:51.457086: step 1240, loss 0.00026938, acc 1\n",
      "2018-08-10T17:25:51.609225: step 1241, loss 0.000325573, acc 1\n",
      "2018-08-10T17:25:51.756338: step 1242, loss 0.000749788, acc 1\n",
      "2018-08-10T17:25:51.907115: step 1243, loss 0.000503355, acc 1\n",
      "2018-08-10T17:25:52.057731: step 1244, loss 0.000338562, acc 1\n",
      "2018-08-10T17:25:52.207609: step 1245, loss 0.00101985, acc 1\n",
      "2018-08-10T17:25:52.359701: step 1246, loss 0.000696962, acc 1\n",
      "2018-08-10T17:25:52.510401: step 1247, loss 0.000296799, acc 1\n",
      "2018-08-10T17:25:52.657090: step 1248, loss 0.00383683, acc 1\n",
      "2018-08-10T17:25:52.808337: step 1249, loss 0.000384381, acc 1\n",
      "2018-08-10T17:25:52.956336: step 1250, loss 0.00038057, acc 1\n",
      "2018-08-10T17:25:53.108577: step 1251, loss 0.000542123, acc 1\n",
      "2018-08-10T17:25:53.257985: step 1252, loss 0.00247454, acc 1\n",
      "2018-08-10T17:25:53.410099: step 1253, loss 0.00103199, acc 1\n",
      "2018-08-10T17:25:53.561155: step 1254, loss 0.000648684, acc 1\n",
      "2018-08-10T17:25:53.713193: step 1255, loss 0.00126221, acc 1\n",
      "2018-08-10T17:25:53.866887: step 1256, loss 0.00040363, acc 1\n",
      "2018-08-10T17:25:54.018784: step 1257, loss 0.000467136, acc 1\n",
      "2018-08-10T17:25:54.168411: step 1258, loss 0.00446186, acc 1\n",
      "2018-08-10T17:25:54.320059: step 1259, loss 0.00396044, acc 1\n",
      "2018-08-10T17:25:54.467341: step 1260, loss 0.00152416, acc 1\n",
      "2018-08-10T17:25:54.612304: step 1261, loss 0.00798612, acc 1\n",
      "2018-08-10T17:25:54.762631: step 1262, loss 0.00048079, acc 1\n",
      "2018-08-10T17:25:54.914565: step 1263, loss 0.00135592, acc 1\n",
      "2018-08-10T17:25:55.063149: step 1264, loss 0.0198053, acc 0.984375\n",
      "2018-08-10T17:25:55.213780: step 1265, loss 0.00254301, acc 1\n",
      "2018-08-10T17:25:55.366094: step 1266, loss 0.000389753, acc 1\n",
      "2018-08-10T17:25:55.517672: step 1267, loss 0.0238183, acc 0.984375\n",
      "2018-08-10T17:25:55.667497: step 1268, loss 0.000709556, acc 1\n",
      "2018-08-10T17:25:55.816734: step 1269, loss 0.000234021, acc 1\n",
      "2018-08-10T17:25:55.966120: step 1270, loss 0.000315273, acc 1\n",
      "2018-08-10T17:25:56.114686: step 1271, loss 0.000257497, acc 1\n",
      "2018-08-10T17:25:56.262802: step 1272, loss 0.00725316, acc 1\n",
      "2018-08-10T17:25:56.413466: step 1273, loss 0.000229716, acc 1\n",
      "2018-08-10T17:25:56.554715: step 1274, loss 8.8782e-05, acc 1\n",
      "2018-08-10T17:25:56.695482: step 1275, loss 0.0002445, acc 1\n",
      "2018-08-10T17:25:56.835265: step 1276, loss 0.00164941, acc 1\n",
      "2018-08-10T17:25:56.977252: step 1277, loss 0.000439057, acc 1\n",
      "2018-08-10T17:25:57.119690: step 1278, loss 0.000417978, acc 1\n",
      "2018-08-10T17:25:57.258898: step 1279, loss 0.000951225, acc 1\n",
      "2018-08-10T17:25:57.398568: step 1280, loss 0.00134329, acc 1\n",
      "2018-08-10T17:25:57.541160: step 1281, loss 0.000572808, acc 1\n",
      "2018-08-10T17:25:57.681231: step 1282, loss 0.000442598, acc 1\n",
      "2018-08-10T17:25:57.820501: step 1283, loss 0.00411604, acc 1\n",
      "2018-08-10T17:25:57.959268: step 1284, loss 0.00735274, acc 1\n",
      "2018-08-10T17:25:58.100698: step 1285, loss 0.000123565, acc 1\n",
      "2018-08-10T17:25:58.241858: step 1286, loss 0.00336188, acc 1\n",
      "2018-08-10T17:25:58.376849: step 1287, loss 0.0089163, acc 1\n",
      "2018-08-10T17:25:58.518605: step 1288, loss 0.000580572, acc 1\n",
      "2018-08-10T17:25:58.659193: step 1289, loss 0.001807, acc 1\n",
      "2018-08-10T17:25:58.798853: step 1290, loss 0.00389467, acc 1\n",
      "2018-08-10T17:25:58.940734: step 1291, loss 0.00292735, acc 1\n",
      "2018-08-10T17:25:59.098589: step 1292, loss 0.0219968, acc 0.984375\n",
      "2018-08-10T17:25:59.241441: step 1293, loss 0.00614417, acc 1\n",
      "2018-08-10T17:25:59.390077: step 1294, loss 0.00386573, acc 1\n",
      "2018-08-10T17:25:59.533738: step 1295, loss 0.000325936, acc 1\n",
      "2018-08-10T17:25:59.673542: step 1296, loss 0.00117757, acc 1\n",
      "2018-08-10T17:25:59.811405: step 1297, loss 0.0231958, acc 0.984375\n",
      "2018-08-10T17:25:59.951198: step 1298, loss 0.00070461, acc 1\n",
      "2018-08-10T17:26:00.091684: step 1299, loss 0.0303303, acc 0.984375\n",
      "2018-08-10T17:26:00.227674: step 1300, loss 0.000484828, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:26:00.377613: step 1300, loss 0.784794, acc 0.861244\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533921767/checkpoints/model-1300\n",
      "\n",
      "2018-08-10T17:26:00.572141: step 1301, loss 0.00322786, acc 1\n",
      "2018-08-10T17:26:00.715135: step 1302, loss 0.000350158, acc 1\n",
      "2018-08-10T17:26:00.857768: step 1303, loss 0.000255608, acc 1\n",
      "2018-08-10T17:26:01.004091: step 1304, loss 0.00318611, acc 1\n",
      "2018-08-10T17:26:01.146786: step 1305, loss 0.00129628, acc 1\n",
      "2018-08-10T17:26:01.292493: step 1306, loss 0.00213898, acc 1\n",
      "2018-08-10T17:26:01.443933: step 1307, loss 0.00402154, acc 1\n",
      "2018-08-10T17:26:01.591788: step 1308, loss 0.000189769, acc 1\n",
      "2018-08-10T17:26:01.741030: step 1309, loss 0.00320112, acc 1\n",
      "2018-08-10T17:26:01.888965: step 1310, loss 0.000175515, acc 1\n",
      "2018-08-10T17:26:02.034993: step 1311, loss 0.000320004, acc 1\n",
      "2018-08-10T17:26:02.179120: step 1312, loss 0.00183592, acc 1\n",
      "2018-08-10T17:26:02.320653: step 1313, loss 0.0436677, acc 0.983871\n",
      "2018-08-10T17:26:02.467209: step 1314, loss 0.011593, acc 0.984375\n",
      "2018-08-10T17:26:02.615035: step 1315, loss 0.0023393, acc 1\n",
      "2018-08-10T17:26:02.762452: step 1316, loss 0.00695368, acc 1\n",
      "2018-08-10T17:26:02.909201: step 1317, loss 0.000406054, acc 1\n",
      "2018-08-10T17:26:03.055460: step 1318, loss 0.000953321, acc 1\n",
      "2018-08-10T17:26:03.199761: step 1319, loss 0.0513342, acc 0.984375\n",
      "2018-08-10T17:26:03.345937: step 1320, loss 0.000699302, acc 1\n",
      "2018-08-10T17:26:03.493111: step 1321, loss 0.0139574, acc 0.984375\n",
      "2018-08-10T17:26:03.639661: step 1322, loss 0.000213962, acc 1\n",
      "2018-08-10T17:26:03.784494: step 1323, loss 0.000360889, acc 1\n",
      "2018-08-10T17:26:03.930703: step 1324, loss 0.000539009, acc 1\n",
      "2018-08-10T17:26:04.077347: step 1325, loss 9.86268e-05, acc 1\n",
      "2018-08-10T17:26:04.221084: step 1326, loss 0.000804457, acc 1\n",
      "2018-08-10T17:26:04.368695: step 1327, loss 0.001935, acc 1\n",
      "2018-08-10T17:26:04.516307: step 1328, loss 0.00429608, acc 1\n",
      "2018-08-10T17:26:04.665402: step 1329, loss 0.000525903, acc 1\n",
      "2018-08-10T17:26:04.812424: step 1330, loss 0.000884548, acc 1\n",
      "2018-08-10T17:26:04.963181: step 1331, loss 0.00730315, acc 1\n",
      "2018-08-10T17:26:05.111623: step 1332, loss 0.000260366, acc 1\n",
      "2018-08-10T17:26:05.257856: step 1333, loss 0.00431733, acc 1\n",
      "2018-08-10T17:26:05.409148: step 1334, loss 0.000271496, acc 1\n",
      "2018-08-10T17:26:05.561307: step 1335, loss 0.0109781, acc 1\n",
      "2018-08-10T17:26:05.710752: step 1336, loss 0.0019063, acc 1\n",
      "2018-08-10T17:26:05.857433: step 1337, loss 0.000152135, acc 1\n",
      "2018-08-10T17:26:06.007272: step 1338, loss 0.00210751, acc 1\n",
      "2018-08-10T17:26:06.153019: step 1339, loss 0.000205446, acc 1\n",
      "2018-08-10T17:26:06.300218: step 1340, loss 0.00272536, acc 1\n",
      "2018-08-10T17:26:06.449549: step 1341, loss 0.00148594, acc 1\n",
      "2018-08-10T17:26:06.600347: step 1342, loss 0.000292065, acc 1\n",
      "2018-08-10T17:26:06.748390: step 1343, loss 0.000906284, acc 1\n",
      "2018-08-10T17:26:06.896344: step 1344, loss 0.00021496, acc 1\n",
      "2018-08-10T17:26:07.047826: step 1345, loss 0.000190856, acc 1\n",
      "2018-08-10T17:26:07.195579: step 1346, loss 0.000318534, acc 1\n",
      "2018-08-10T17:26:07.342462: step 1347, loss 0.00171293, acc 1\n",
      "2018-08-10T17:26:07.494776: step 1348, loss 0.00090677, acc 1\n",
      "2018-08-10T17:26:07.644202: step 1349, loss 0.0250111, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:26:07.793181: step 1350, loss 0.000347554, acc 1\n",
      "2018-08-10T17:26:07.941579: step 1351, loss 0.000614874, acc 1\n",
      "2018-08-10T17:26:08.087148: step 1352, loss 0.000208718, acc 1\n",
      "2018-08-10T17:26:08.236352: step 1353, loss 0.00166126, acc 1\n",
      "2018-08-10T17:26:08.384995: step 1354, loss 0.000136231, acc 1\n",
      "2018-08-10T17:26:08.535746: step 1355, loss 0.00331621, acc 1\n",
      "2018-08-10T17:26:08.685335: step 1356, loss 0.000385331, acc 1\n",
      "2018-08-10T17:26:08.832635: step 1357, loss 0.000969821, acc 1\n",
      "2018-08-10T17:26:08.982140: step 1358, loss 0.0022209, acc 1\n",
      "2018-08-10T17:26:09.131848: step 1359, loss 0.0183109, acc 0.984375\n",
      "2018-08-10T17:26:09.281953: step 1360, loss 0.00252881, acc 1\n",
      "2018-08-10T17:26:09.430646: step 1361, loss 0.0172823, acc 0.984375\n",
      "2018-08-10T17:26:09.576960: step 1362, loss 0.00028799, acc 1\n",
      "2018-08-10T17:26:09.725953: step 1363, loss 0.00107276, acc 1\n",
      "2018-08-10T17:26:09.872318: step 1364, loss 0.00229678, acc 1\n",
      "2018-08-10T17:26:10.015503: step 1365, loss 0.00060742, acc 1\n",
      "2018-08-10T17:26:10.164685: step 1366, loss 0.000192029, acc 1\n",
      "2018-08-10T17:26:10.311949: step 1367, loss 0.000144393, acc 1\n",
      "2018-08-10T17:26:10.464624: step 1368, loss 0.00175487, acc 1\n",
      "2018-08-10T17:26:10.612657: step 1369, loss 0.000163592, acc 1\n",
      "2018-08-10T17:26:10.759734: step 1370, loss 0.00170561, acc 1\n",
      "2018-08-10T17:26:10.908840: step 1371, loss 0.00130339, acc 1\n",
      "2018-08-10T17:26:11.059272: step 1372, loss 0.0381753, acc 0.984375\n",
      "2018-08-10T17:26:11.202887: step 1373, loss 0.00134627, acc 1\n",
      "2018-08-10T17:26:11.347095: step 1374, loss 0.00173393, acc 1\n",
      "2018-08-10T17:26:11.492339: step 1375, loss 0.000175521, acc 1\n",
      "2018-08-10T17:26:11.636064: step 1376, loss 0.00138872, acc 1\n",
      "2018-08-10T17:26:11.780452: step 1377, loss 0.000597582, acc 1\n",
      "2018-08-10T17:26:11.924557: step 1378, loss 0.000606592, acc 1\n",
      "2018-08-10T17:26:12.075218: step 1379, loss 0.0026433, acc 1\n",
      "2018-08-10T17:26:12.223599: step 1380, loss 0.000293042, acc 1\n",
      "2018-08-10T17:26:12.371114: step 1381, loss 0.000331832, acc 1\n",
      "2018-08-10T17:26:12.520448: step 1382, loss 0.00413307, acc 1\n",
      "2018-08-10T17:26:12.667091: step 1383, loss 0.000496027, acc 1\n",
      "2018-08-10T17:26:12.813835: step 1384, loss 0.00900387, acc 1\n",
      "2018-08-10T17:26:12.962959: step 1385, loss 0.000204094, acc 1\n",
      "2018-08-10T17:26:13.112851: step 1386, loss 0.000510834, acc 1\n",
      "2018-08-10T17:26:13.261027: step 1387, loss 0.000342077, acc 1\n",
      "2018-08-10T17:26:13.410098: step 1388, loss 0.0241684, acc 0.984375\n",
      "2018-08-10T17:26:13.560473: step 1389, loss 0.000425098, acc 1\n",
      "2018-08-10T17:26:13.711278: step 1390, loss 0.00109325, acc 1\n",
      "2018-08-10T17:26:13.854989: step 1391, loss 0.0037342, acc 1\n",
      "2018-08-10T17:26:14.004856: step 1392, loss 0.000348086, acc 1\n",
      "2018-08-10T17:26:14.157344: step 1393, loss 0.000528497, acc 1\n",
      "2018-08-10T17:26:14.311170: step 1394, loss 0.000710207, acc 1\n",
      "2018-08-10T17:26:14.461113: step 1395, loss 0.000810582, acc 1\n",
      "2018-08-10T17:26:14.611265: step 1396, loss 0.00212803, acc 1\n",
      "2018-08-10T17:26:14.761967: step 1397, loss 0.00178645, acc 1\n",
      "2018-08-10T17:26:14.912460: step 1398, loss 0.00028472, acc 1\n",
      "2018-08-10T17:26:15.063971: step 1399, loss 0.00078385, acc 1\n",
      "2018-08-10T17:26:15.215660: step 1400, loss 0.000904755, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:26:15.358058: step 1400, loss 0.829409, acc 0.851675\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533921767/checkpoints/model-1400\n",
      "\n",
      "2018-08-10T17:26:15.569298: step 1401, loss 0.041446, acc 0.984375\n",
      "2018-08-10T17:26:15.718130: step 1402, loss 0.000509575, acc 1\n",
      "2018-08-10T17:26:15.865191: step 1403, loss 0.00374051, acc 1\n",
      "2018-08-10T17:26:16.011128: step 1404, loss 0.00373879, acc 1\n",
      "2018-08-10T17:26:16.163978: step 1405, loss 0.00775971, acc 1\n",
      "2018-08-10T17:26:16.316823: step 1406, loss 0.00344655, acc 1\n",
      "2018-08-10T17:26:16.466908: step 1407, loss 0.000943794, acc 1\n",
      "2018-08-10T17:26:16.616299: step 1408, loss 0.000257653, acc 1\n",
      "2018-08-10T17:26:16.764149: step 1409, loss 0.000414012, acc 1\n",
      "2018-08-10T17:26:16.915146: step 1410, loss 0.000250166, acc 1\n",
      "2018-08-10T17:26:17.064337: step 1411, loss 0.000500897, acc 1\n",
      "2018-08-10T17:26:17.210664: step 1412, loss 0.000396539, acc 1\n",
      "2018-08-10T17:26:17.361146: step 1413, loss 0.000401266, acc 1\n",
      "2018-08-10T17:26:17.512456: step 1414, loss 0.0068709, acc 1\n",
      "2018-08-10T17:26:17.652652: step 1415, loss 0.00393462, acc 1\n",
      "2018-08-10T17:26:17.795692: step 1416, loss 0.00054963, acc 1\n",
      "2018-08-10T17:26:17.933676: step 1417, loss 0.0010851, acc 1\n",
      "2018-08-10T17:26:18.071643: step 1418, loss 0.000346456, acc 1\n",
      "2018-08-10T17:26:18.211975: step 1419, loss 0.000623314, acc 1\n",
      "2018-08-10T17:26:18.350721: step 1420, loss 0.00348044, acc 1\n",
      "2018-08-10T17:26:18.493864: step 1421, loss 0.000159188, acc 1\n",
      "2018-08-10T17:26:18.633075: step 1422, loss 0.000222155, acc 1\n",
      "2018-08-10T17:26:18.774503: step 1423, loss 0.00353849, acc 1\n",
      "2018-08-10T17:26:18.917280: step 1424, loss 0.00162304, acc 1\n",
      "2018-08-10T17:26:19.059974: step 1425, loss 0.000420457, acc 1\n",
      "2018-08-10T17:26:19.200201: step 1426, loss 0.000630536, acc 1\n",
      "2018-08-10T17:26:19.342496: step 1427, loss 0.00317115, acc 1\n",
      "2018-08-10T17:26:19.484668: step 1428, loss 0.00934212, acc 1\n",
      "2018-08-10T17:26:19.629160: step 1429, loss 0.00210706, acc 1\n",
      "2018-08-10T17:26:19.764137: step 1430, loss 0.000334667, acc 1\n",
      "2018-08-10T17:26:19.913152: step 1431, loss 0.000340644, acc 1\n",
      "2018-08-10T17:26:20.051306: step 1432, loss 0.000434104, acc 1\n",
      "2018-08-10T17:26:20.193277: step 1433, loss 0.000508394, acc 1\n",
      "2018-08-10T17:26:20.334499: step 1434, loss 0.00116012, acc 1\n",
      "2018-08-10T17:26:20.476123: step 1435, loss 0.00206064, acc 1\n",
      "2018-08-10T17:26:20.615306: step 1436, loss 0.00222571, acc 1\n",
      "2018-08-10T17:26:20.755885: step 1437, loss 0.000785648, acc 1\n",
      "2018-08-10T17:26:20.898928: step 1438, loss 0.00152075, acc 1\n",
      "2018-08-10T17:26:21.042440: step 1439, loss 0.000404089, acc 1\n",
      "2018-08-10T17:26:21.181445: step 1440, loss 0.000403396, acc 1\n",
      "2018-08-10T17:26:21.326353: step 1441, loss 0.000584128, acc 1\n",
      "2018-08-10T17:26:21.471719: step 1442, loss 0.000519069, acc 1\n",
      "2018-08-10T17:26:21.612769: step 1443, loss 0.000291227, acc 1\n",
      "2018-08-10T17:26:21.756058: step 1444, loss 0.000315212, acc 1\n",
      "2018-08-10T17:26:21.901946: step 1445, loss 0.000145763, acc 1\n",
      "2018-08-10T17:26:22.051123: step 1446, loss 0.00356816, acc 1\n",
      "2018-08-10T17:26:22.198483: step 1447, loss 0.000256399, acc 1\n",
      "2018-08-10T17:26:22.348006: step 1448, loss 0.00011453, acc 1\n",
      "2018-08-10T17:26:22.496988: step 1449, loss 0.00852336, acc 1\n",
      "2018-08-10T17:26:22.645159: step 1450, loss 0.00165993, acc 1\n",
      "2018-08-10T17:26:22.793365: step 1451, loss 0.0228352, acc 0.984375\n",
      "2018-08-10T17:26:22.937787: step 1452, loss 9.60244e-05, acc 1\n",
      "2018-08-10T17:26:23.087211: step 1453, loss 0.000566112, acc 1\n",
      "2018-08-10T17:26:23.234676: step 1454, loss 0.0011144, acc 1\n",
      "2018-08-10T17:26:23.383713: step 1455, loss 0.0014321, acc 1\n",
      "2018-08-10T17:26:23.532822: step 1456, loss 0.000536078, acc 1\n",
      "2018-08-10T17:26:23.682845: step 1457, loss 0.00764542, acc 1\n",
      "2018-08-10T17:26:23.829223: step 1458, loss 0.00111702, acc 1\n",
      "2018-08-10T17:26:23.980022: step 1459, loss 0.000687232, acc 1\n",
      "2018-08-10T17:26:24.128912: step 1460, loss 0.000759574, acc 1\n",
      "2018-08-10T17:26:24.277278: step 1461, loss 0.000866823, acc 1\n",
      "2018-08-10T17:26:24.423859: step 1462, loss 0.00100143, acc 1\n",
      "2018-08-10T17:26:24.572370: step 1463, loss 0.00026882, acc 1\n",
      "2018-08-10T17:26:24.720222: step 1464, loss 0.00208857, acc 1\n",
      "2018-08-10T17:26:24.867004: step 1465, loss 0.00285639, acc 1\n",
      "2018-08-10T17:26:25.010160: step 1466, loss 0.00046595, acc 1\n",
      "2018-08-10T17:26:25.158822: step 1467, loss 0.0228425, acc 0.984375\n",
      "2018-08-10T17:26:25.305199: step 1468, loss 0.00039804, acc 1\n",
      "2018-08-10T17:26:25.447755: step 1469, loss 0.000528781, acc 1\n",
      "2018-08-10T17:26:25.594859: step 1470, loss 0.00292984, acc 1\n",
      "2018-08-10T17:26:25.746073: step 1471, loss 0.000717844, acc 1\n",
      "2018-08-10T17:26:25.893706: step 1472, loss 0.000694711, acc 1\n",
      "2018-08-10T17:26:26.039713: step 1473, loss 0.000245804, acc 1\n",
      "2018-08-10T17:26:26.183906: step 1474, loss 0.00011203, acc 1\n",
      "2018-08-10T17:26:26.330900: step 1475, loss 0.0007289, acc 1\n",
      "2018-08-10T17:26:26.476928: step 1476, loss 0.00138522, acc 1\n",
      "2018-08-10T17:26:26.626566: step 1477, loss 0.000413237, acc 1\n",
      "2018-08-10T17:26:26.773411: step 1478, loss 2.2797e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:26:26.921658: step 1479, loss 0.00106126, acc 1\n",
      "2018-08-10T17:26:27.069023: step 1480, loss 0.000326746, acc 1\n",
      "2018-08-10T17:26:27.219313: step 1481, loss 0.00921251, acc 1\n",
      "2018-08-10T17:26:27.362116: step 1482, loss 0.00031672, acc 1\n",
      "2018-08-10T17:26:27.508900: step 1483, loss 0.000332869, acc 1\n",
      "2018-08-10T17:26:27.657046: step 1484, loss 0.000585376, acc 1\n",
      "2018-08-10T17:26:27.803828: step 1485, loss 0.000752721, acc 1\n",
      "2018-08-10T17:26:27.949275: step 1486, loss 0.000326332, acc 1\n",
      "2018-08-10T17:26:28.099968: step 1487, loss 0.00119413, acc 1\n",
      "2018-08-10T17:26:28.246020: step 1488, loss 0.00150601, acc 1\n",
      "2018-08-10T17:26:28.394207: step 1489, loss 0.00318483, acc 1\n",
      "2018-08-10T17:26:28.538927: step 1490, loss 0.00627199, acc 1\n",
      "2018-08-10T17:26:28.686570: step 1491, loss 0.00248861, acc 1\n",
      "2018-08-10T17:26:28.832087: step 1492, loss 0.000400822, acc 1\n",
      "2018-08-10T17:26:28.977859: step 1493, loss 0.000785351, acc 1\n",
      "2018-08-10T17:26:29.123254: step 1494, loss 0.000460085, acc 1\n",
      "2018-08-10T17:26:29.268405: step 1495, loss 0.00114434, acc 1\n",
      "2018-08-10T17:26:29.415326: step 1496, loss 0.000371256, acc 1\n",
      "2018-08-10T17:26:29.565893: step 1497, loss 0.000363633, acc 1\n",
      "2018-08-10T17:26:29.714803: step 1498, loss 0.0228241, acc 0.984375\n",
      "2018-08-10T17:26:29.865897: step 1499, loss 0.000257396, acc 1\n",
      "2018-08-10T17:26:30.015008: step 1500, loss 0.00432215, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:26:30.156851: step 1500, loss 0.802773, acc 0.861244\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533921767/checkpoints/model-1500\n",
      "\n",
      "2018-08-10T17:26:30.363467: step 1501, loss 0.00103475, acc 1\n",
      "2018-08-10T17:26:30.511660: step 1502, loss 5.93753e-05, acc 1\n",
      "2018-08-10T17:26:30.661443: step 1503, loss 0.00237997, acc 1\n",
      "2018-08-10T17:26:30.808811: step 1504, loss 0.00209709, acc 1\n",
      "2018-08-10T17:26:30.954408: step 1505, loss 0.000218854, acc 1\n",
      "2018-08-10T17:26:31.104766: step 1506, loss 0.000828077, acc 1\n",
      "2018-08-10T17:26:31.254000: step 1507, loss 0.00157678, acc 1\n",
      "2018-08-10T17:26:31.398939: step 1508, loss 0.000781788, acc 1\n",
      "2018-08-10T17:26:31.549952: step 1509, loss 0.00284799, acc 1\n",
      "2018-08-10T17:26:31.702149: step 1510, loss 0.000404056, acc 1\n",
      "2018-08-10T17:26:31.848784: step 1511, loss 0.00470681, acc 1\n",
      "2018-08-10T17:26:31.999581: step 1512, loss 0.0237637, acc 0.984375\n",
      "2018-08-10T17:26:32.147979: step 1513, loss 0.00138588, acc 1\n",
      "2018-08-10T17:26:32.288096: step 1514, loss 0.00206217, acc 1\n",
      "2018-08-10T17:26:32.427958: step 1515, loss 0.000317442, acc 1\n",
      "2018-08-10T17:26:32.568784: step 1516, loss 0.000298112, acc 1\n",
      "2018-08-10T17:26:32.711687: step 1517, loss 0.000702373, acc 1\n",
      "2018-08-10T17:26:32.856876: step 1518, loss 0.00112554, acc 1\n",
      "2018-08-10T17:26:33.005316: step 1519, loss 0.000643135, acc 1\n",
      "2018-08-10T17:26:33.154198: step 1520, loss 0.000743382, acc 1\n",
      "2018-08-10T17:26:33.299935: step 1521, loss 0.00409151, acc 1\n",
      "2018-08-10T17:26:33.450818: step 1522, loss 0.000109292, acc 1\n",
      "2018-08-10T17:26:33.603644: step 1523, loss 0.0151704, acc 0.984375\n",
      "2018-08-10T17:26:33.749766: step 1524, loss 0.000813586, acc 1\n",
      "2018-08-10T17:26:33.898261: step 1525, loss 0.000394428, acc 1\n",
      "2018-08-10T17:26:34.044677: step 1526, loss 0.000219089, acc 1\n",
      "2018-08-10T17:26:34.193955: step 1527, loss 0.00222874, acc 1\n",
      "2018-08-10T17:26:34.342664: step 1528, loss 0.000571769, acc 1\n",
      "2018-08-10T17:26:34.494283: step 1529, loss 0.00403976, acc 1\n",
      "2018-08-10T17:26:34.644218: step 1530, loss 0.00129657, acc 1\n",
      "2018-08-10T17:26:34.798582: step 1531, loss 0.000312325, acc 1\n",
      "2018-08-10T17:26:34.948444: step 1532, loss 0.0008638, acc 1\n",
      "2018-08-10T17:26:35.102016: step 1533, loss 0.00060706, acc 1\n",
      "2018-08-10T17:26:35.248305: step 1534, loss 0.000923862, acc 1\n",
      "2018-08-10T17:26:35.402367: step 1535, loss 0.00824852, acc 1\n",
      "2018-08-10T17:26:35.558952: step 1536, loss 0.000243102, acc 1\n",
      "2018-08-10T17:26:35.710478: step 1537, loss 0.000441231, acc 1\n",
      "2018-08-10T17:26:35.859654: step 1538, loss 0.000541972, acc 1\n",
      "2018-08-10T17:26:36.009665: step 1539, loss 0.0419291, acc 0.984375\n",
      "2018-08-10T17:26:36.161142: step 1540, loss 0.00307447, acc 1\n",
      "2018-08-10T17:26:36.314420: step 1541, loss 0.000420305, acc 1\n",
      "2018-08-10T17:26:36.466785: step 1542, loss 0.00084717, acc 1\n",
      "2018-08-10T17:26:36.617267: step 1543, loss 0.000119025, acc 1\n",
      "2018-08-10T17:26:36.766478: step 1544, loss 0.0084018, acc 1\n",
      "2018-08-10T17:26:36.919444: step 1545, loss 0.00157564, acc 1\n",
      "2018-08-10T17:26:37.070632: step 1546, loss 0.000469172, acc 1\n",
      "2018-08-10T17:26:37.219609: step 1547, loss 0.000360162, acc 1\n",
      "2018-08-10T17:26:37.371738: step 1548, loss 0.000726642, acc 1\n",
      "2018-08-10T17:26:37.528175: step 1549, loss 0.000114587, acc 1\n",
      "2018-08-10T17:26:37.680673: step 1550, loss 0.00060892, acc 1\n",
      "2018-08-10T17:26:37.829387: step 1551, loss 0.000916865, acc 1\n",
      "2018-08-10T17:26:37.978770: step 1552, loss 0.000760646, acc 1\n",
      "2018-08-10T17:26:38.131410: step 1553, loss 0.0174534, acc 0.984375\n",
      "2018-08-10T17:26:38.280615: step 1554, loss 0.000139833, acc 1\n",
      "2018-08-10T17:26:38.426059: step 1555, loss 0.00237216, acc 1\n",
      "2018-08-10T17:26:38.564751: step 1556, loss 0.000206254, acc 1\n",
      "2018-08-10T17:26:38.706371: step 1557, loss 0.000347623, acc 1\n",
      "2018-08-10T17:26:38.845138: step 1558, loss 0.000141633, acc 1\n",
      "2018-08-10T17:26:38.986291: step 1559, loss 0.000538634, acc 1\n",
      "2018-08-10T17:26:39.121910: step 1560, loss 0.000999587, acc 1\n",
      "2018-08-10T17:26:39.263134: step 1561, loss 0.00116095, acc 1\n",
      "2018-08-10T17:26:39.403186: step 1562, loss 0.000384384, acc 1\n",
      "2018-08-10T17:26:39.546530: step 1563, loss 0.00764882, acc 1\n",
      "2018-08-10T17:26:39.694328: step 1564, loss 0.0213766, acc 0.984375\n",
      "2018-08-10T17:26:39.833977: step 1565, loss 0.000212291, acc 1\n",
      "2018-08-10T17:26:39.973008: step 1566, loss 0.000369777, acc 1\n",
      "2018-08-10T17:26:40.112146: step 1567, loss 0.000353582, acc 1\n",
      "2018-08-10T17:26:40.252921: step 1568, loss 0.00011267, acc 1\n",
      "2018-08-10T17:26:40.393996: step 1569, loss 0.0145596, acc 0.984375\n",
      "2018-08-10T17:26:40.533188: step 1570, loss 0.00531417, acc 1\n",
      "2018-08-10T17:26:40.674870: step 1571, loss 0.00147453, acc 1\n",
      "2018-08-10T17:26:40.813992: step 1572, loss 0.000437709, acc 1\n",
      "2018-08-10T17:26:40.948932: step 1573, loss 0.00144804, acc 1\n",
      "2018-08-10T17:26:41.087408: step 1574, loss 0.000716293, acc 1\n",
      "2018-08-10T17:26:41.226446: step 1575, loss 0.000775607, acc 1\n",
      "2018-08-10T17:26:41.363329: step 1576, loss 0.00201666, acc 1\n",
      "2018-08-10T17:26:41.504845: step 1577, loss 0.000385918, acc 1\n",
      "2018-08-10T17:26:41.648702: step 1578, loss 0.000374628, acc 1\n",
      "2018-08-10T17:26:41.788590: step 1579, loss 0.000657396, acc 1\n",
      "2018-08-10T17:26:41.926359: step 1580, loss 0.0199605, acc 0.984375\n",
      "2018-08-10T17:26:42.067831: step 1581, loss 0.000347751, acc 1\n",
      "2018-08-10T17:26:42.208271: step 1582, loss 0.000401409, acc 1\n",
      "2018-08-10T17:26:42.347735: step 1583, loss 0.00337305, acc 1\n",
      "2018-08-10T17:26:42.490193: step 1584, loss 0.000457186, acc 1\n",
      "2018-08-10T17:26:42.631490: step 1585, loss 0.0023877, acc 1\n",
      "2018-08-10T17:26:42.767942: step 1586, loss 0.000208192, acc 1\n",
      "2018-08-10T17:26:42.910592: step 1587, loss 0.000902509, acc 1\n",
      "2018-08-10T17:26:43.056946: step 1588, loss 0.00124309, acc 1\n",
      "2018-08-10T17:26:43.204384: step 1589, loss 9.05569e-05, acc 1\n",
      "2018-08-10T17:26:43.355802: step 1590, loss 0.000390721, acc 1\n",
      "2018-08-10T17:26:43.505341: step 1591, loss 0.000434647, acc 1\n",
      "2018-08-10T17:26:43.654719: step 1592, loss 0.00343663, acc 1\n",
      "2018-08-10T17:26:43.799821: step 1593, loss 0.00597274, acc 1\n",
      "2018-08-10T17:26:43.943921: step 1594, loss 0.00663134, acc 1\n",
      "2018-08-10T17:26:44.092240: step 1595, loss 0.006158, acc 1\n",
      "2018-08-10T17:26:44.237939: step 1596, loss 0.0123429, acc 1\n",
      "2018-08-10T17:26:44.384449: step 1597, loss 0.00150032, acc 1\n",
      "2018-08-10T17:26:44.540982: step 1598, loss 0.00113032, acc 1\n",
      "2018-08-10T17:26:44.686860: step 1599, loss 0.000573469, acc 1\n",
      "2018-08-10T17:26:44.831996: step 1600, loss 0.00281007, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:26:44.972752: step 1600, loss 0.881598, acc 0.842105\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533921767/checkpoints/model-1600\n",
      "\n",
      "2018-08-10T17:26:45.173753: step 1601, loss 0.000792311, acc 1\n",
      "2018-08-10T17:26:45.324121: step 1602, loss 0.00851949, acc 1\n",
      "2018-08-10T17:26:45.474120: step 1603, loss 0.000312811, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:26:45.623647: step 1604, loss 0.000120695, acc 1\n",
      "2018-08-10T17:26:45.769419: step 1605, loss 6.27236e-05, acc 1\n",
      "2018-08-10T17:26:45.917666: step 1606, loss 0.00417451, acc 1\n",
      "2018-08-10T17:26:46.062843: step 1607, loss 0.00103548, acc 1\n",
      "2018-08-10T17:26:46.212720: step 1608, loss 5.21339e-05, acc 1\n",
      "2018-08-10T17:26:46.360338: step 1609, loss 0.00222868, acc 1\n",
      "2018-08-10T17:26:46.509459: step 1610, loss 0.00405565, acc 1\n",
      "2018-08-10T17:26:46.658763: step 1611, loss 0.00069507, acc 1\n",
      "2018-08-10T17:26:46.804671: step 1612, loss 0.000428204, acc 1\n",
      "2018-08-10T17:26:46.952045: step 1613, loss 0.000134064, acc 1\n",
      "2018-08-10T17:26:47.102145: step 1614, loss 0.00187225, acc 1\n",
      "2018-08-10T17:26:47.250095: step 1615, loss 0.000257645, acc 1\n",
      "2018-08-10T17:26:47.402923: step 1616, loss 0.00122208, acc 1\n",
      "2018-08-10T17:26:47.554652: step 1617, loss 0.00654847, acc 1\n",
      "2018-08-10T17:26:47.701574: step 1618, loss 0.000805073, acc 1\n",
      "2018-08-10T17:26:47.850105: step 1619, loss 0.00200085, acc 1\n",
      "2018-08-10T17:26:47.997348: step 1620, loss 0.00333553, acc 1\n",
      "2018-08-10T17:26:48.142639: step 1621, loss 0.000412118, acc 1\n",
      "2018-08-10T17:26:48.289705: step 1622, loss 0.000180932, acc 1\n",
      "2018-08-10T17:26:48.448407: step 1623, loss 0.00013856, acc 1\n",
      "2018-08-10T17:26:48.600070: step 1624, loss 0.000209294, acc 1\n",
      "2018-08-10T17:26:48.742950: step 1625, loss 0.00207717, acc 1\n",
      "2018-08-10T17:26:48.890694: step 1626, loss 9.95916e-05, acc 1\n",
      "2018-08-10T17:26:49.034070: step 1627, loss 0.00281712, acc 1\n",
      "2018-08-10T17:26:49.191966: step 1628, loss 0.000988594, acc 1\n",
      "2018-08-10T17:26:49.342472: step 1629, loss 0.000722393, acc 1\n",
      "2018-08-10T17:26:49.494450: step 1630, loss 0.000153672, acc 1\n",
      "2018-08-10T17:26:49.643601: step 1631, loss 0.000399655, acc 1\n",
      "2018-08-10T17:26:49.795286: step 1632, loss 0.000617626, acc 1\n",
      "2018-08-10T17:26:49.944718: step 1633, loss 0.000276257, acc 1\n",
      "2018-08-10T17:26:50.091828: step 1634, loss 0.000629438, acc 1\n",
      "2018-08-10T17:26:50.237190: step 1635, loss 0.000171106, acc 1\n",
      "2018-08-10T17:26:50.384748: step 1636, loss 0.000228522, acc 1\n",
      "2018-08-10T17:26:50.532331: step 1637, loss 0.000212005, acc 1\n",
      "2018-08-10T17:26:50.677422: step 1638, loss 0.00273928, acc 1\n",
      "2018-08-10T17:26:50.822080: step 1639, loss 0.000382132, acc 1\n",
      "2018-08-10T17:26:50.971005: step 1640, loss 0.000183946, acc 1\n",
      "2018-08-10T17:26:51.120409: step 1641, loss 3.87014e-05, acc 1\n",
      "2018-08-10T17:26:51.269598: step 1642, loss 0.00200077, acc 1\n",
      "2018-08-10T17:26:51.417097: step 1643, loss 0.00255927, acc 1\n",
      "2018-08-10T17:26:51.567198: step 1644, loss 0.000144586, acc 1\n",
      "2018-08-10T17:26:51.714811: step 1645, loss 0.000244623, acc 1\n",
      "2018-08-10T17:26:51.864056: step 1646, loss 0.000370463, acc 1\n",
      "2018-08-10T17:26:52.012987: step 1647, loss 0.00198631, acc 1\n",
      "2018-08-10T17:26:52.161472: step 1648, loss 0.00100965, acc 1\n",
      "2018-08-10T17:26:52.304926: step 1649, loss 0.000635873, acc 1\n",
      "2018-08-10T17:26:52.454073: step 1650, loss 0.000159389, acc 1\n",
      "2018-08-10T17:26:52.597662: step 1651, loss 0.0120831, acc 0.983871\n",
      "2018-08-10T17:26:52.747356: step 1652, loss 0.00202303, acc 1\n",
      "2018-08-10T17:26:52.894151: step 1653, loss 0.00128703, acc 1\n",
      "2018-08-10T17:26:53.041623: step 1654, loss 0.00012995, acc 1\n",
      "2018-08-10T17:26:53.185180: step 1655, loss 0.000308535, acc 1\n",
      "2018-08-10T17:26:53.324497: step 1656, loss 0.00336247, acc 1\n",
      "2018-08-10T17:26:53.466680: step 1657, loss 0.000424995, acc 1\n",
      "2018-08-10T17:26:53.612168: step 1658, loss 0.0008737, acc 1\n",
      "2018-08-10T17:26:53.759117: step 1659, loss 0.000130163, acc 1\n",
      "2018-08-10T17:26:53.906970: step 1660, loss 0.000196294, acc 1\n",
      "2018-08-10T17:26:54.054243: step 1661, loss 0.000732987, acc 1\n",
      "2018-08-10T17:26:54.204080: step 1662, loss 0.000107491, acc 1\n",
      "2018-08-10T17:26:54.352712: step 1663, loss 0.000118475, acc 1\n",
      "2018-08-10T17:26:54.498373: step 1664, loss 0.000439611, acc 1\n",
      "2018-08-10T17:26:54.644173: step 1665, loss 0.000799413, acc 1\n",
      "2018-08-10T17:26:54.791288: step 1666, loss 0.000648521, acc 1\n",
      "2018-08-10T17:26:54.936660: step 1667, loss 0.00123129, acc 1\n",
      "2018-08-10T17:26:55.081643: step 1668, loss 0.0001704, acc 1\n",
      "2018-08-10T17:26:55.228622: step 1669, loss 0.000153939, acc 1\n",
      "2018-08-10T17:26:55.374877: step 1670, loss 0.000194927, acc 1\n",
      "2018-08-10T17:26:55.527396: step 1671, loss 9.6519e-05, acc 1\n",
      "2018-08-10T17:26:55.676232: step 1672, loss 0.000257729, acc 1\n",
      "2018-08-10T17:26:55.822353: step 1673, loss 0.000520229, acc 1\n",
      "2018-08-10T17:26:55.972277: step 1674, loss 8.47648e-05, acc 1\n",
      "2018-08-10T17:26:56.120094: step 1675, loss 0.000891925, acc 1\n",
      "2018-08-10T17:26:56.264363: step 1676, loss 4.08786e-05, acc 1\n",
      "2018-08-10T17:26:56.408939: step 1677, loss 0.0110442, acc 1\n",
      "2018-08-10T17:26:56.556194: step 1678, loss 0.000292923, acc 1\n",
      "2018-08-10T17:26:56.701691: step 1679, loss 0.000255445, acc 1\n",
      "2018-08-10T17:26:56.852002: step 1680, loss 0.00105677, acc 1\n",
      "2018-08-10T17:26:57.002152: step 1681, loss 0.00335393, acc 1\n",
      "2018-08-10T17:26:57.148843: step 1682, loss 0.000164137, acc 1\n",
      "2018-08-10T17:26:57.298103: step 1683, loss 0.000419106, acc 1\n",
      "2018-08-10T17:26:57.446885: step 1684, loss 0.000217519, acc 1\n",
      "2018-08-10T17:26:57.592297: step 1685, loss 8.44228e-05, acc 1\n",
      "2018-08-10T17:26:57.740612: step 1686, loss 0.000163607, acc 1\n",
      "2018-08-10T17:26:57.888216: step 1687, loss 0.000464228, acc 1\n",
      "2018-08-10T17:26:58.031647: step 1688, loss 0.000646452, acc 1\n",
      "2018-08-10T17:26:58.179921: step 1689, loss 0.00013392, acc 1\n",
      "2018-08-10T17:26:58.323153: step 1690, loss 0.000232849, acc 1\n",
      "2018-08-10T17:26:58.470246: step 1691, loss 0.00136794, acc 1\n",
      "2018-08-10T17:26:58.622164: step 1692, loss 0.000110343, acc 1\n",
      "2018-08-10T17:26:58.773266: step 1693, loss 0.0272172, acc 0.984375\n",
      "2018-08-10T17:26:58.920789: step 1694, loss 0.000351379, acc 1\n",
      "2018-08-10T17:26:59.065073: step 1695, loss 0.00128958, acc 1\n",
      "2018-08-10T17:26:59.210567: step 1696, loss 0.000127223, acc 1\n",
      "2018-08-10T17:26:59.348642: step 1697, loss 9.56858e-05, acc 1\n",
      "2018-08-10T17:26:59.491246: step 1698, loss 3.00609e-05, acc 1\n",
      "2018-08-10T17:26:59.631032: step 1699, loss 0.000696456, acc 1\n",
      "2018-08-10T17:26:59.771361: step 1700, loss 0.000334376, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:26:59.904870: step 1700, loss 0.808121, acc 0.870813\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533921767/checkpoints/model-1700\n",
      "\n",
      "2018-08-10T17:27:00.093872: step 1701, loss 0.000291178, acc 1\n",
      "2018-08-10T17:27:00.233224: step 1702, loss 0.00017364, acc 1\n",
      "2018-08-10T17:27:00.371960: step 1703, loss 0.000216626, acc 1\n",
      "2018-08-10T17:27:00.510283: step 1704, loss 0.00011401, acc 1\n",
      "2018-08-10T17:27:00.650659: step 1705, loss 0.00207516, acc 1\n",
      "2018-08-10T17:27:00.787910: step 1706, loss 0.000547809, acc 1\n",
      "2018-08-10T17:27:00.927735: step 1707, loss 0.000294155, acc 1\n",
      "2018-08-10T17:27:01.065542: step 1708, loss 0.00122384, acc 1\n",
      "2018-08-10T17:27:01.204434: step 1709, loss 5.12141e-05, acc 1\n",
      "2018-08-10T17:27:01.344570: step 1710, loss 0.000586985, acc 1\n",
      "2018-08-10T17:27:01.486494: step 1711, loss 0.000182538, acc 1\n",
      "2018-08-10T17:27:01.624183: step 1712, loss 0.00274596, acc 1\n",
      "2018-08-10T17:27:01.764965: step 1713, loss 0.000198822, acc 1\n",
      "2018-08-10T17:27:01.903678: step 1714, loss 0.000929994, acc 1\n",
      "2018-08-10T17:27:02.044889: step 1715, loss 0.000502789, acc 1\n",
      "2018-08-10T17:27:02.177809: step 1716, loss 0.000852635, acc 1\n",
      "2018-08-10T17:27:02.316456: step 1717, loss 0.00114445, acc 1\n",
      "2018-08-10T17:27:02.455823: step 1718, loss 0.000197523, acc 1\n",
      "2018-08-10T17:27:02.597181: step 1719, loss 0.000209331, acc 1\n",
      "2018-08-10T17:27:02.735023: step 1720, loss 5.8536e-05, acc 1\n",
      "2018-08-10T17:27:02.876832: step 1721, loss 0.000353167, acc 1\n",
      "2018-08-10T17:27:03.015183: step 1722, loss 0.00152361, acc 1\n",
      "2018-08-10T17:27:03.154502: step 1723, loss 0.00651778, acc 1\n",
      "2018-08-10T17:27:03.292496: step 1724, loss 0.00752357, acc 1\n",
      "2018-08-10T17:27:03.434909: step 1725, loss 0.000428517, acc 1\n",
      "2018-08-10T17:27:03.575320: step 1726, loss 0.000352035, acc 1\n",
      "2018-08-10T17:27:03.720673: step 1727, loss 0.000351009, acc 1\n",
      "2018-08-10T17:27:03.866880: step 1728, loss 0.00068286, acc 1\n",
      "2018-08-10T17:27:04.009799: step 1729, loss 0.00201885, acc 1\n",
      "2018-08-10T17:27:04.160433: step 1730, loss 7.02359e-05, acc 1\n",
      "2018-08-10T17:27:04.308827: step 1731, loss 0.000248255, acc 1\n",
      "2018-08-10T17:27:04.457681: step 1732, loss 0.00256273, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:27:04.607487: step 1733, loss 0.00288291, acc 1\n",
      "2018-08-10T17:27:04.753985: step 1734, loss 0.000141554, acc 1\n",
      "2018-08-10T17:27:04.902753: step 1735, loss 0.0122466, acc 1\n",
      "2018-08-10T17:27:05.050453: step 1736, loss 0.000216754, acc 1\n",
      "2018-08-10T17:27:05.199922: step 1737, loss 0.000225863, acc 1\n",
      "2018-08-10T17:27:05.348900: step 1738, loss 0.0113611, acc 0.984375\n",
      "2018-08-10T17:27:05.501712: step 1739, loss 0.000298849, acc 1\n",
      "2018-08-10T17:27:05.650624: step 1740, loss 0.000218163, acc 1\n",
      "2018-08-10T17:27:05.798812: step 1741, loss 0.000236652, acc 1\n",
      "2018-08-10T17:27:05.945126: step 1742, loss 0.000939935, acc 1\n",
      "2018-08-10T17:27:06.090344: step 1743, loss 0.000359607, acc 1\n",
      "2018-08-10T17:27:06.235719: step 1744, loss 0.000243236, acc 1\n",
      "2018-08-10T17:27:06.381177: step 1745, loss 0.000280113, acc 1\n",
      "2018-08-10T17:27:06.527010: step 1746, loss 0.000421567, acc 1\n",
      "2018-08-10T17:27:06.676360: step 1747, loss 7.3085e-05, acc 1\n",
      "2018-08-10T17:27:06.823461: step 1748, loss 0.000252191, acc 1\n",
      "2018-08-10T17:27:06.974645: step 1749, loss 0.000396674, acc 1\n",
      "2018-08-10T17:27:07.122613: step 1750, loss 0.00121182, acc 1\n",
      "2018-08-10T17:27:07.271132: step 1751, loss 0.000946273, acc 1\n",
      "2018-08-10T17:27:07.420611: step 1752, loss 9.28761e-05, acc 1\n",
      "2018-08-10T17:27:07.571158: step 1753, loss 0.000129056, acc 1\n",
      "2018-08-10T17:27:07.718397: step 1754, loss 0.00196472, acc 1\n",
      "2018-08-10T17:27:07.861877: step 1755, loss 0.00313985, acc 1\n",
      "2018-08-10T17:27:08.009609: step 1756, loss 0.00032909, acc 1\n",
      "2018-08-10T17:27:08.155965: step 1757, loss 0.000104457, acc 1\n",
      "2018-08-10T17:27:08.304742: step 1758, loss 0.00161091, acc 1\n",
      "2018-08-10T17:27:08.456082: step 1759, loss 0.00032682, acc 1\n",
      "2018-08-10T17:27:08.605052: step 1760, loss 0.0106473, acc 1\n",
      "2018-08-10T17:27:08.752298: step 1761, loss 8.4956e-05, acc 1\n",
      "2018-08-10T17:27:08.897011: step 1762, loss 0.00126553, acc 1\n",
      "2018-08-10T17:27:09.045678: step 1763, loss 0.000133267, acc 1\n",
      "2018-08-10T17:27:09.194666: step 1764, loss 0.00369375, acc 1\n",
      "2018-08-10T17:27:09.346272: step 1765, loss 0.00146339, acc 1\n",
      "2018-08-10T17:27:09.496654: step 1766, loss 0.000112589, acc 1\n",
      "2018-08-10T17:27:09.648417: step 1767, loss 9.13358e-05, acc 1\n",
      "2018-08-10T17:27:09.790751: step 1768, loss 0.00137085, acc 1\n",
      "2018-08-10T17:27:09.941352: step 1769, loss 0.00441227, acc 1\n",
      "2018-08-10T17:27:10.086589: step 1770, loss 0.000197427, acc 1\n",
      "2018-08-10T17:27:10.236708: step 1771, loss 0.0012032, acc 1\n",
      "2018-08-10T17:27:10.384359: step 1772, loss 8.40617e-05, acc 1\n",
      "2018-08-10T17:27:10.534032: step 1773, loss 0.00128787, acc 1\n",
      "2018-08-10T17:27:10.680506: step 1774, loss 0.000354723, acc 1\n",
      "2018-08-10T17:27:10.831143: step 1775, loss 0.00012668, acc 1\n",
      "2018-08-10T17:27:10.977397: step 1776, loss 0.00017553, acc 1\n",
      "2018-08-10T17:27:11.127577: step 1777, loss 0.0180648, acc 0.984375\n",
      "2018-08-10T17:27:11.274911: step 1778, loss 0.000206278, acc 1\n",
      "2018-08-10T17:27:11.428326: step 1779, loss 0.000167074, acc 1\n",
      "2018-08-10T17:27:11.581990: step 1780, loss 0.000196136, acc 1\n",
      "2018-08-10T17:27:11.731003: step 1781, loss 6.90784e-05, acc 1\n",
      "2018-08-10T17:27:11.881744: step 1782, loss 0.000394493, acc 1\n",
      "2018-08-10T17:27:12.031640: step 1783, loss 9.47219e-05, acc 1\n",
      "2018-08-10T17:27:12.178031: step 1784, loss 0.00085097, acc 1\n",
      "2018-08-10T17:27:12.327181: step 1785, loss 0.000152182, acc 1\n",
      "2018-08-10T17:27:12.473374: step 1786, loss 0.00515697, acc 1\n",
      "2018-08-10T17:27:12.617913: step 1787, loss 0.00220308, acc 1\n",
      "2018-08-10T17:27:12.763099: step 1788, loss 0.000461042, acc 1\n",
      "2018-08-10T17:27:12.908769: step 1789, loss 0.00141463, acc 1\n",
      "2018-08-10T17:27:13.054332: step 1790, loss 0.00134744, acc 1\n",
      "2018-08-10T17:27:13.202987: step 1791, loss 0.000119078, acc 1\n",
      "2018-08-10T17:27:13.350107: step 1792, loss 0.000106051, acc 1\n",
      "2018-08-10T17:27:13.500943: step 1793, loss 0.000332704, acc 1\n",
      "2018-08-10T17:27:13.646306: step 1794, loss 0.000140795, acc 1\n",
      "2018-08-10T17:27:13.792809: step 1795, loss 0.000344853, acc 1\n",
      "2018-08-10T17:27:13.937741: step 1796, loss 0.000147117, acc 1\n",
      "2018-08-10T17:27:14.086032: step 1797, loss 0.00025833, acc 1\n",
      "2018-08-10T17:27:14.227945: step 1798, loss 0.00293564, acc 1\n",
      "2018-08-10T17:27:14.370218: step 1799, loss 0.000123778, acc 1\n",
      "2018-08-10T17:27:14.511091: step 1800, loss 0.0007362, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:27:14.648671: step 1800, loss 0.814893, acc 0.866029\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533921767/checkpoints/model-1800\n",
      "\n",
      "2018-08-10T17:27:14.851593: step 1801, loss 0.0232283, acc 0.984375\n",
      "2018-08-10T17:27:15.001759: step 1802, loss 0.000311234, acc 1\n",
      "2018-08-10T17:27:15.149161: step 1803, loss 9.42315e-05, acc 1\n",
      "2018-08-10T17:27:15.296221: step 1804, loss 0.000114299, acc 1\n",
      "2018-08-10T17:27:15.444640: step 1805, loss 0.000207142, acc 1\n",
      "2018-08-10T17:27:15.591499: step 1806, loss 8.67023e-05, acc 1\n",
      "2018-08-10T17:27:15.737711: step 1807, loss 0.000349598, acc 1\n",
      "2018-08-10T17:27:15.883498: step 1808, loss 0.00111615, acc 1\n",
      "2018-08-10T17:27:16.028237: step 1809, loss 0.000188565, acc 1\n",
      "2018-08-10T17:27:16.173985: step 1810, loss 0.00243596, acc 1\n",
      "2018-08-10T17:27:16.319639: step 1811, loss 0.0002106, acc 1\n",
      "2018-08-10T17:27:16.466596: step 1812, loss 0.000163052, acc 1\n",
      "2018-08-10T17:27:16.613903: step 1813, loss 0.000355768, acc 1\n",
      "2018-08-10T17:27:16.760870: step 1814, loss 0.000321059, acc 1\n",
      "2018-08-10T17:27:16.910185: step 1815, loss 0.000204002, acc 1\n",
      "2018-08-10T17:27:17.060121: step 1816, loss 0.00130847, acc 1\n",
      "2018-08-10T17:27:17.211475: step 1817, loss 0.0417058, acc 0.984375\n",
      "2018-08-10T17:27:17.358828: step 1818, loss 0.00104752, acc 1\n",
      "2018-08-10T17:27:17.509214: step 1819, loss 0.0091534, acc 1\n",
      "2018-08-10T17:27:17.654310: step 1820, loss 0.000292475, acc 1\n",
      "2018-08-10T17:27:17.806036: step 1821, loss 0.000222148, acc 1\n",
      "2018-08-10T17:27:17.952679: step 1822, loss 0.000573934, acc 1\n",
      "2018-08-10T17:27:18.100610: step 1823, loss 0.000427137, acc 1\n",
      "2018-08-10T17:27:18.248373: step 1824, loss 0.000172107, acc 1\n",
      "2018-08-10T17:27:18.395884: step 1825, loss 0.000210605, acc 1\n",
      "2018-08-10T17:27:18.543402: step 1826, loss 0.000515728, acc 1\n",
      "2018-08-10T17:27:18.691717: step 1827, loss 0.000534963, acc 1\n",
      "2018-08-10T17:27:18.840833: step 1828, loss 0.000837925, acc 1\n",
      "2018-08-10T17:27:18.991704: step 1829, loss 0.000274587, acc 1\n",
      "2018-08-10T17:27:19.138699: step 1830, loss 0.000926046, acc 1\n",
      "2018-08-10T17:27:19.286985: step 1831, loss 0.00055226, acc 1\n",
      "2018-08-10T17:27:19.436872: step 1832, loss 0.000752307, acc 1\n",
      "2018-08-10T17:27:19.582929: step 1833, loss 0.000349638, acc 1\n",
      "2018-08-10T17:27:19.732777: step 1834, loss 0.00119841, acc 1\n",
      "2018-08-10T17:27:19.879488: step 1835, loss 0.00175695, acc 1\n",
      "2018-08-10T17:27:20.026903: step 1836, loss 0.00109617, acc 1\n",
      "2018-08-10T17:27:20.175009: step 1837, loss 0.000154294, acc 1\n",
      "2018-08-10T17:27:20.320173: step 1838, loss 0.00513435, acc 1\n",
      "2018-08-10T17:27:20.461062: step 1839, loss 0.000402901, acc 1\n",
      "2018-08-10T17:27:20.601516: step 1840, loss 0.000927707, acc 1\n",
      "2018-08-10T17:27:20.743388: step 1841, loss 0.00148703, acc 1\n",
      "2018-08-10T17:27:20.881724: step 1842, loss 0.00140429, acc 1\n",
      "2018-08-10T17:27:21.020118: step 1843, loss 0.000376107, acc 1\n",
      "2018-08-10T17:27:21.158354: step 1844, loss 0.000550264, acc 1\n",
      "2018-08-10T17:27:21.299397: step 1845, loss 0.00150296, acc 1\n",
      "2018-08-10T17:27:21.441750: step 1846, loss 0.000201476, acc 1\n",
      "2018-08-10T17:27:21.581210: step 1847, loss 0.00078149, acc 1\n",
      "2018-08-10T17:27:21.724837: step 1848, loss 0.000141793, acc 1\n",
      "2018-08-10T17:27:21.864850: step 1849, loss 0.000624965, acc 1\n",
      "2018-08-10T17:27:22.002253: step 1850, loss 0.00097107, acc 1\n",
      "2018-08-10T17:27:22.141770: step 1851, loss 0.00139833, acc 1\n",
      "2018-08-10T17:27:22.279287: step 1852, loss 0.101416, acc 0.984375\n",
      "2018-08-10T17:27:22.420365: step 1853, loss 0.000581523, acc 1\n",
      "2018-08-10T17:27:22.561779: step 1854, loss 0.000341996, acc 1\n",
      "2018-08-10T17:27:22.702229: step 1855, loss 0.0123579, acc 0.984375\n",
      "2018-08-10T17:27:22.843617: step 1856, loss 0.0102837, acc 1\n",
      "2018-08-10T17:27:22.982997: step 1857, loss 0.000103592, acc 1\n",
      "2018-08-10T17:27:23.121800: step 1858, loss 0.00031488, acc 1\n",
      "2018-08-10T17:27:23.261529: step 1859, loss 0.000435999, acc 1\n",
      "2018-08-10T17:27:23.401719: step 1860, loss 0.0214439, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:27:23.544356: step 1861, loss 7.27092e-05, acc 1\n",
      "2018-08-10T17:27:23.687675: step 1862, loss 0.00478862, acc 1\n",
      "2018-08-10T17:27:23.829164: step 1863, loss 0.0003252, acc 1\n",
      "2018-08-10T17:27:23.969134: step 1864, loss 0.000532766, acc 1\n",
      "2018-08-10T17:27:24.109143: step 1865, loss 0.00041352, acc 1\n",
      "2018-08-10T17:27:24.248135: step 1866, loss 0.02428, acc 0.984375\n",
      "2018-08-10T17:27:24.388057: step 1867, loss 0.000744314, acc 1\n",
      "2018-08-10T17:27:24.529945: step 1868, loss 0.0372311, acc 0.984375\n",
      "2018-08-10T17:27:24.672483: step 1869, loss 4.70821e-05, acc 1\n",
      "2018-08-10T17:27:24.816491: step 1870, loss 0.000654955, acc 1\n",
      "2018-08-10T17:27:24.963025: step 1871, loss 0.00553422, acc 1\n",
      "2018-08-10T17:27:25.107074: step 1872, loss 0.000458461, acc 1\n",
      "2018-08-10T17:27:25.257677: step 1873, loss 0.00178123, acc 1\n",
      "2018-08-10T17:27:25.406642: step 1874, loss 0.000527342, acc 1\n",
      "2018-08-10T17:27:25.555930: step 1875, loss 0.000382097, acc 1\n",
      "2018-08-10T17:27:25.704447: step 1876, loss 0.000179229, acc 1\n",
      "2018-08-10T17:27:25.856821: step 1877, loss 5.4455e-05, acc 1\n",
      "2018-08-10T17:27:26.000762: step 1878, loss 0.0120783, acc 0.984375\n",
      "2018-08-10T17:27:26.150882: step 1879, loss 0.00267328, acc 1\n",
      "2018-08-10T17:27:26.305061: step 1880, loss 0.000205392, acc 1\n",
      "2018-08-10T17:27:26.453822: step 1881, loss 0.000212935, acc 1\n",
      "2018-08-10T17:27:26.602950: step 1882, loss 0.000234361, acc 1\n",
      "2018-08-10T17:27:26.762427: step 1883, loss 0.000288283, acc 1\n",
      "2018-08-10T17:27:26.910452: step 1884, loss 0.000588962, acc 1\n",
      "2018-08-10T17:27:27.057041: step 1885, loss 0.000416522, acc 1\n",
      "2018-08-10T17:27:27.206314: step 1886, loss 8.31576e-05, acc 1\n",
      "2018-08-10T17:27:27.354829: step 1887, loss 0.000220155, acc 1\n",
      "2018-08-10T17:27:27.505177: step 1888, loss 9.73374e-05, acc 1\n",
      "2018-08-10T17:27:27.660266: step 1889, loss 0.00183088, acc 1\n",
      "2018-08-10T17:27:27.806310: step 1890, loss 0.0046522, acc 1\n",
      "2018-08-10T17:27:27.954001: step 1891, loss 0.000294927, acc 1\n",
      "2018-08-10T17:27:28.100896: step 1892, loss 0.000294759, acc 1\n",
      "2018-08-10T17:27:28.250185: step 1893, loss 0.010092, acc 1\n",
      "2018-08-10T17:27:28.397623: step 1894, loss 0.0175276, acc 0.984375\n",
      "2018-08-10T17:27:28.546696: step 1895, loss 0.000831803, acc 1\n",
      "2018-08-10T17:27:28.698196: step 1896, loss 0.00498867, acc 1\n",
      "2018-08-10T17:27:28.850028: step 1897, loss 0.000155058, acc 1\n",
      "2018-08-10T17:27:28.995242: step 1898, loss 0.000246155, acc 1\n",
      "2018-08-10T17:27:29.142883: step 1899, loss 0.000305707, acc 1\n",
      "2018-08-10T17:27:29.291328: step 1900, loss 0.000173935, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:27:29.438442: step 1900, loss 0.780987, acc 0.856459\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533921767/checkpoints/model-1900\n",
      "\n",
      "2018-08-10T17:27:29.648956: step 1901, loss 0.000103896, acc 1\n",
      "2018-08-10T17:27:29.796803: step 1902, loss 0.000557251, acc 1\n",
      "2018-08-10T17:27:29.943189: step 1903, loss 0.000792668, acc 1\n",
      "2018-08-10T17:27:30.092098: step 1904, loss 0.00591265, acc 1\n",
      "2018-08-10T17:27:30.238689: step 1905, loss 0.00138822, acc 1\n",
      "2018-08-10T17:27:30.387288: step 1906, loss 0.00139692, acc 1\n",
      "2018-08-10T17:27:30.536667: step 1907, loss 0.000346472, acc 1\n",
      "2018-08-10T17:27:30.682434: step 1908, loss 0.000226449, acc 1\n",
      "2018-08-10T17:27:30.831230: step 1909, loss 0.000502139, acc 1\n",
      "2018-08-10T17:27:30.981346: step 1910, loss 0.00044938, acc 1\n",
      "2018-08-10T17:27:31.126014: step 1911, loss 0.000721054, acc 1\n",
      "2018-08-10T17:27:31.275427: step 1912, loss 0.00106616, acc 1\n",
      "2018-08-10T17:27:31.423261: step 1913, loss 0.000174327, acc 1\n",
      "2018-08-10T17:27:31.575218: step 1914, loss 0.00158192, acc 1\n",
      "2018-08-10T17:27:31.726711: step 1915, loss 0.000132366, acc 1\n",
      "2018-08-10T17:27:31.876479: step 1916, loss 9.80884e-05, acc 1\n",
      "2018-08-10T17:27:32.025964: step 1917, loss 0.00109476, acc 1\n",
      "2018-08-10T17:27:32.175442: step 1918, loss 0.00129489, acc 1\n",
      "2018-08-10T17:27:32.335395: step 1919, loss 0.000870553, acc 1\n",
      "2018-08-10T17:27:32.485785: step 1920, loss 0.000366588, acc 1\n",
      "2018-08-10T17:27:32.631650: step 1921, loss 0.000747758, acc 1\n",
      "2018-08-10T17:27:32.779422: step 1922, loss 0.000237981, acc 1\n",
      "2018-08-10T17:27:32.928633: step 1923, loss 0.00033381, acc 1\n",
      "2018-08-10T17:27:33.072304: step 1924, loss 0.00540127, acc 1\n",
      "2018-08-10T17:27:33.221850: step 1925, loss 0.000291489, acc 1\n",
      "2018-08-10T17:27:33.369598: step 1926, loss 0.0010816, acc 1\n",
      "2018-08-10T17:27:33.518462: step 1927, loss 0.000248253, acc 1\n",
      "2018-08-10T17:27:33.666012: step 1928, loss 0.00191068, acc 1\n",
      "2018-08-10T17:27:33.813078: step 1929, loss 0.00993037, acc 1\n",
      "2018-08-10T17:27:33.959718: step 1930, loss 0.0160197, acc 0.984375\n",
      "2018-08-10T17:27:34.107074: step 1931, loss 0.00326964, acc 1\n",
      "2018-08-10T17:27:34.255146: step 1932, loss 0.000244325, acc 1\n",
      "2018-08-10T17:27:34.400834: step 1933, loss 0.000614745, acc 1\n",
      "2018-08-10T17:27:34.547097: step 1934, loss 0.000267558, acc 1\n",
      "2018-08-10T17:27:34.697892: step 1935, loss 0.000333166, acc 1\n",
      "2018-08-10T17:27:34.844705: step 1936, loss 0.00041397, acc 1\n",
      "2018-08-10T17:27:34.988119: step 1937, loss 0.00051309, acc 1\n",
      "2018-08-10T17:27:35.136178: step 1938, loss 0.00231964, acc 1\n",
      "2018-08-10T17:27:35.275951: step 1939, loss 0.00588202, acc 1\n",
      "2018-08-10T17:27:35.416696: step 1940, loss 0.000445566, acc 1\n",
      "2018-08-10T17:27:35.562753: step 1941, loss 6.41666e-05, acc 1\n",
      "2018-08-10T17:27:35.710715: step 1942, loss 0.00031134, acc 1\n",
      "2018-08-10T17:27:35.857627: step 1943, loss 0.00410141, acc 1\n",
      "2018-08-10T17:27:36.005444: step 1944, loss 9.83215e-05, acc 1\n",
      "2018-08-10T17:27:36.152712: step 1945, loss 0.000229403, acc 1\n",
      "2018-08-10T17:27:36.301758: step 1946, loss 0.000185148, acc 1\n",
      "2018-08-10T17:27:36.449791: step 1947, loss 0.00022209, acc 1\n",
      "2018-08-10T17:27:36.596879: step 1948, loss 0.000296989, acc 1\n",
      "2018-08-10T17:27:36.747054: step 1949, loss 3.84955e-05, acc 1\n",
      "2018-08-10T17:27:36.891725: step 1950, loss 0.000647812, acc 1\n",
      "2018-08-10T17:27:37.044233: step 1951, loss 0.00061014, acc 1\n",
      "2018-08-10T17:27:37.197300: step 1952, loss 0.000256113, acc 1\n",
      "2018-08-10T17:27:37.350071: step 1953, loss 0.000152514, acc 1\n",
      "2018-08-10T17:27:37.502768: step 1954, loss 0.000617723, acc 1\n",
      "2018-08-10T17:27:37.654030: step 1955, loss 0.000228247, acc 1\n",
      "2018-08-10T17:27:37.803217: step 1956, loss 0.000427178, acc 1\n",
      "2018-08-10T17:27:37.951003: step 1957, loss 8.02346e-05, acc 1\n",
      "2018-08-10T17:27:38.096978: step 1958, loss 0.000109752, acc 1\n",
      "2018-08-10T17:27:38.245547: step 1959, loss 0.00341514, acc 1\n",
      "2018-08-10T17:27:38.394325: step 1960, loss 0.0153317, acc 0.984375\n",
      "2018-08-10T17:27:38.543039: step 1961, loss 6.67531e-05, acc 1\n",
      "2018-08-10T17:27:38.692414: step 1962, loss 0.000125165, acc 1\n",
      "2018-08-10T17:27:38.842141: step 1963, loss 5.95092e-05, acc 1\n",
      "2018-08-10T17:27:38.994438: step 1964, loss 0.000202105, acc 1\n",
      "2018-08-10T17:27:39.142978: step 1965, loss 4.38551e-05, acc 1\n",
      "2018-08-10T17:27:39.292949: step 1966, loss 0.0024425, acc 1\n",
      "2018-08-10T17:27:39.448212: step 1967, loss 0.000426161, acc 1\n",
      "2018-08-10T17:27:39.597764: step 1968, loss 0.000408533, acc 1\n",
      "2018-08-10T17:27:39.754155: step 1969, loss 4.49276e-05, acc 1\n",
      "2018-08-10T17:27:39.900651: step 1970, loss 0.000296221, acc 1\n",
      "2018-08-10T17:27:40.046774: step 1971, loss 0.000513911, acc 1\n",
      "2018-08-10T17:27:40.194497: step 1972, loss 0.00352626, acc 1\n",
      "2018-08-10T17:27:40.344319: step 1973, loss 8.70386e-05, acc 1\n",
      "2018-08-10T17:27:40.489853: step 1974, loss 0.000169913, acc 1\n",
      "2018-08-10T17:27:40.635746: step 1975, loss 0.000751659, acc 1\n",
      "2018-08-10T17:27:40.776743: step 1976, loss 0.00030382, acc 1\n",
      "2018-08-10T17:27:40.924397: step 1977, loss 0.00123832, acc 1\n",
      "2018-08-10T17:27:41.070185: step 1978, loss 0.000295571, acc 1\n",
      "2018-08-10T17:27:41.216302: step 1979, loss 0.000194898, acc 1\n",
      "2018-08-10T17:27:41.363468: step 1980, loss 0.000195293, acc 1\n",
      "2018-08-10T17:27:41.506631: step 1981, loss 0.000436692, acc 1\n",
      "2018-08-10T17:27:41.645221: step 1982, loss 0.000881087, acc 1\n",
      "2018-08-10T17:27:41.786266: step 1983, loss 0.00166746, acc 1\n",
      "2018-08-10T17:27:41.926004: step 1984, loss 0.00201003, acc 1\n",
      "2018-08-10T17:27:42.065109: step 1985, loss 0.000146178, acc 1\n",
      "2018-08-10T17:27:42.201839: step 1986, loss 7.54571e-05, acc 1\n",
      "2018-08-10T17:27:42.342324: step 1987, loss 0.00442943, acc 1\n",
      "2018-08-10T17:27:42.482092: step 1988, loss 0.000276968, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:27:42.618426: step 1989, loss 0.000156628, acc 1\n",
      "2018-08-10T17:27:42.757799: step 1990, loss 0.000902184, acc 1\n",
      "2018-08-10T17:27:42.898439: step 1991, loss 0.000121217, acc 1\n",
      "2018-08-10T17:27:43.036057: step 1992, loss 0.00170746, acc 1\n",
      "2018-08-10T17:27:43.180239: step 1993, loss 0.000937068, acc 1\n",
      "2018-08-10T17:27:43.318471: step 1994, loss 0.000809605, acc 1\n",
      "2018-08-10T17:27:43.463003: step 1995, loss 0.000327368, acc 1\n",
      "2018-08-10T17:27:43.599426: step 1996, loss 0.000103281, acc 1\n",
      "2018-08-10T17:27:43.736032: step 1997, loss 0.000207937, acc 1\n",
      "2018-08-10T17:27:43.872793: step 1998, loss 0.0105728, acc 1\n",
      "2018-08-10T17:27:44.011228: step 1999, loss 4.73496e-05, acc 1\n",
      "2018-08-10T17:27:44.151112: step 2000, loss 0.000491394, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:27:44.292089: step 2000, loss 0.930001, acc 0.851675\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533921767/checkpoints/model-2000\n",
      "\n",
      "2018-08-10T17:27:44.481510: step 2001, loss 0.00032745, acc 1\n",
      "2018-08-10T17:27:44.616133: step 2002, loss 4.28248e-05, acc 1\n",
      "2018-08-10T17:27:44.754604: step 2003, loss 0.000434333, acc 1\n",
      "2018-08-10T17:27:44.891478: step 2004, loss 0.000804131, acc 1\n",
      "2018-08-10T17:27:45.028475: step 2005, loss 6.07379e-05, acc 1\n",
      "2018-08-10T17:27:45.167070: step 2006, loss 0.000135804, acc 1\n",
      "2018-08-10T17:27:45.304119: step 2007, loss 0.00130896, acc 1\n",
      "2018-08-10T17:27:45.447475: step 2008, loss 0.000987767, acc 1\n",
      "2018-08-10T17:27:45.587555: step 2009, loss 0.000554921, acc 1\n",
      "2018-08-10T17:27:45.732000: step 2010, loss 0.000459747, acc 1\n",
      "2018-08-10T17:27:45.875299: step 2011, loss 0.00745308, acc 1\n",
      "2018-08-10T17:27:46.020143: step 2012, loss 0.00125832, acc 1\n",
      "2018-08-10T17:27:46.166273: step 2013, loss 0.0305437, acc 0.984375\n",
      "2018-08-10T17:27:46.309669: step 2014, loss 0.000644751, acc 1\n",
      "2018-08-10T17:27:46.451895: step 2015, loss 0.000130949, acc 1\n",
      "2018-08-10T17:27:46.596637: step 2016, loss 5.83021e-05, acc 1\n",
      "2018-08-10T17:27:46.740590: step 2017, loss 0.000281435, acc 1\n",
      "2018-08-10T17:27:46.884390: step 2018, loss 0.000867209, acc 1\n",
      "2018-08-10T17:27:47.029924: step 2019, loss 0.000201147, acc 1\n",
      "2018-08-10T17:27:47.175156: step 2020, loss 0.000409213, acc 1\n",
      "2018-08-10T17:27:47.320173: step 2021, loss 0.000117353, acc 1\n",
      "2018-08-10T17:27:47.469655: step 2022, loss 0.000834084, acc 1\n",
      "2018-08-10T17:27:47.614973: step 2023, loss 0.00106491, acc 1\n",
      "2018-08-10T17:27:47.760098: step 2024, loss 0.000357752, acc 1\n",
      "2018-08-10T17:27:47.907109: step 2025, loss 0.00506387, acc 1\n",
      "2018-08-10T17:27:48.053693: step 2026, loss 0.000219021, acc 1\n",
      "2018-08-10T17:27:48.199572: step 2027, loss 0.000352315, acc 1\n",
      "2018-08-10T17:27:48.341412: step 2028, loss 9.09656e-05, acc 1\n",
      "2018-08-10T17:27:48.486821: step 2029, loss 0.00332181, acc 1\n",
      "2018-08-10T17:27:48.635985: step 2030, loss 0.000273046, acc 1\n",
      "2018-08-10T17:27:48.781736: step 2031, loss 6.32579e-05, acc 1\n",
      "2018-08-10T17:27:48.928259: step 2032, loss 0.000164593, acc 1\n",
      "2018-08-10T17:27:49.076606: step 2033, loss 4.25348e-05, acc 1\n",
      "2018-08-10T17:27:49.224172: step 2034, loss 0.000213808, acc 1\n",
      "2018-08-10T17:27:49.369469: step 2035, loss 0.00135498, acc 1\n",
      "2018-08-10T17:27:49.520634: step 2036, loss 0.0604405, acc 0.984375\n",
      "2018-08-10T17:27:49.669068: step 2037, loss 0.000199454, acc 1\n",
      "2018-08-10T17:27:49.814888: step 2038, loss 0.000902842, acc 1\n",
      "2018-08-10T17:27:49.963585: step 2039, loss 0.000235938, acc 1\n",
      "2018-08-10T17:27:50.110412: step 2040, loss 0.000219709, acc 1\n",
      "2018-08-10T17:27:50.254352: step 2041, loss 0.000210229, acc 1\n",
      "2018-08-10T17:27:50.402661: step 2042, loss 0.000820389, acc 1\n",
      "2018-08-10T17:27:50.549711: step 2043, loss 0.000115617, acc 1\n",
      "2018-08-10T17:27:50.698620: step 2044, loss 0.000128632, acc 1\n",
      "2018-08-10T17:27:50.842746: step 2045, loss 0.000513647, acc 1\n",
      "2018-08-10T17:27:50.988450: step 2046, loss 6.85817e-05, acc 1\n",
      "2018-08-10T17:27:51.136324: step 2047, loss 0.000493103, acc 1\n",
      "2018-08-10T17:27:51.280455: step 2048, loss 0.0440188, acc 0.984375\n",
      "2018-08-10T17:27:51.428706: step 2049, loss 0.00346369, acc 1\n",
      "2018-08-10T17:27:51.578753: step 2050, loss 5.20221e-05, acc 1\n",
      "2018-08-10T17:27:51.726581: step 2051, loss 0.000212471, acc 1\n",
      "2018-08-10T17:27:51.874672: step 2052, loss 0.0157871, acc 0.984375\n",
      "2018-08-10T17:27:52.022644: step 2053, loss 0.000290473, acc 1\n",
      "2018-08-10T17:27:52.168909: step 2054, loss 0.000338522, acc 1\n",
      "2018-08-10T17:27:52.322619: step 2055, loss 0.000151089, acc 1\n",
      "2018-08-10T17:27:52.474645: step 2056, loss 0.000241617, acc 1\n",
      "2018-08-10T17:27:52.621435: step 2057, loss 7.40211e-05, acc 1\n",
      "2018-08-10T17:27:52.767728: step 2058, loss 0.000138486, acc 1\n",
      "2018-08-10T17:27:52.913880: step 2059, loss 0.0004977, acc 1\n",
      "2018-08-10T17:27:53.062884: step 2060, loss 0.00147952, acc 1\n",
      "2018-08-10T17:27:53.208620: step 2061, loss 0.000639653, acc 1\n",
      "2018-08-10T17:27:53.355639: step 2062, loss 0.000168063, acc 1\n",
      "2018-08-10T17:27:53.506831: step 2063, loss 0.000622759, acc 1\n",
      "2018-08-10T17:27:53.656732: step 2064, loss 1.9217e-05, acc 1\n",
      "2018-08-10T17:27:53.804670: step 2065, loss 0.000203345, acc 1\n",
      "2018-08-10T17:27:53.953943: step 2066, loss 0.00229323, acc 1\n",
      "2018-08-10T17:27:54.100400: step 2067, loss 0.000467688, acc 1\n",
      "2018-08-10T17:27:54.252339: step 2068, loss 0.00158283, acc 1\n",
      "2018-08-10T17:27:54.400306: step 2069, loss 0.000523409, acc 1\n",
      "2018-08-10T17:27:54.546497: step 2070, loss 0.00104061, acc 1\n",
      "2018-08-10T17:27:54.693447: step 2071, loss 0.000887742, acc 1\n",
      "2018-08-10T17:27:54.842141: step 2072, loss 0.00106454, acc 1\n",
      "2018-08-10T17:27:54.989369: step 2073, loss 0.000534089, acc 1\n",
      "2018-08-10T17:27:55.136641: step 2074, loss 0.000318169, acc 1\n",
      "2018-08-10T17:27:55.285638: step 2075, loss 0.000313053, acc 1\n",
      "2018-08-10T17:27:55.436629: step 2076, loss 0.000240008, acc 1\n",
      "2018-08-10T17:27:55.580421: step 2077, loss 0.000110965, acc 1\n",
      "2018-08-10T17:27:55.727709: step 2078, loss 0.000410648, acc 1\n",
      "2018-08-10T17:27:55.875744: step 2079, loss 6.54713e-05, acc 1\n",
      "2018-08-10T17:27:56.018737: step 2080, loss 0.000164887, acc 1\n",
      "2018-08-10T17:27:56.159534: step 2081, loss 0.000177282, acc 1\n",
      "2018-08-10T17:27:56.298649: step 2082, loss 0.000154882, acc 1\n",
      "2018-08-10T17:27:56.437453: step 2083, loss 9.00866e-05, acc 1\n",
      "2018-08-10T17:27:56.580456: step 2084, loss 0.00471914, acc 1\n",
      "2018-08-10T17:27:56.723907: step 2085, loss 0.00014763, acc 1\n",
      "2018-08-10T17:27:56.870760: step 2086, loss 0.00180182, acc 1\n",
      "2018-08-10T17:27:57.017773: step 2087, loss 0.00090058, acc 1\n",
      "2018-08-10T17:27:57.166035: step 2088, loss 0.000213389, acc 1\n",
      "2018-08-10T17:27:57.313655: step 2089, loss 0.000507094, acc 1\n",
      "2018-08-10T17:27:57.461610: step 2090, loss 0.00403016, acc 1\n",
      "2018-08-10T17:27:57.608862: step 2091, loss 8.89248e-05, acc 1\n",
      "2018-08-10T17:27:57.756473: step 2092, loss 7.56433e-05, acc 1\n",
      "2018-08-10T17:27:57.901417: step 2093, loss 0.000363186, acc 1\n",
      "2018-08-10T17:27:58.047022: step 2094, loss 0.00046063, acc 1\n",
      "2018-08-10T17:27:58.196980: step 2095, loss 0.000439499, acc 1\n",
      "2018-08-10T17:27:58.347047: step 2096, loss 1.7921e-05, acc 1\n",
      "2018-08-10T17:27:58.494583: step 2097, loss 0.000269888, acc 1\n",
      "2018-08-10T17:27:58.642552: step 2098, loss 0.000247428, acc 1\n",
      "2018-08-10T17:27:58.791851: step 2099, loss 5.5694e-05, acc 1\n",
      "2018-08-10T17:27:58.939658: step 2100, loss 0.00935873, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:27:59.082857: step 2100, loss 0.938165, acc 0.851675\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533921767/checkpoints/model-2100\n",
      "\n",
      "2018-08-10T17:27:59.291397: step 2101, loss 0.00045443, acc 1\n",
      "2018-08-10T17:27:59.442996: step 2102, loss 0.000130044, acc 1\n",
      "2018-08-10T17:27:59.588837: step 2103, loss 0.00141, acc 1\n",
      "2018-08-10T17:27:59.739011: step 2104, loss 0.00101874, acc 1\n",
      "2018-08-10T17:27:59.887939: step 2105, loss 7.677e-05, acc 1\n",
      "2018-08-10T17:28:00.032472: step 2106, loss 0.000407576, acc 1\n",
      "2018-08-10T17:28:00.179296: step 2107, loss 0.00016264, acc 1\n",
      "2018-08-10T17:28:00.327233: step 2108, loss 0.0017713, acc 1\n",
      "2018-08-10T17:28:00.476329: step 2109, loss 0.000357632, acc 1\n",
      "2018-08-10T17:28:00.622293: step 2110, loss 0.000451326, acc 1\n",
      "2018-08-10T17:28:00.770100: step 2111, loss 6.89827e-05, acc 1\n",
      "2018-08-10T17:28:00.917708: step 2112, loss 0.000653865, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:28:01.067398: step 2113, loss 0.000135452, acc 1\n",
      "2018-08-10T17:28:01.213872: step 2114, loss 0.000630985, acc 1\n",
      "2018-08-10T17:28:01.360322: step 2115, loss 0.000104279, acc 1\n",
      "2018-08-10T17:28:01.512619: step 2116, loss 0.00149036, acc 1\n",
      "2018-08-10T17:28:01.665763: step 2117, loss 0.000114499, acc 1\n",
      "2018-08-10T17:28:01.810741: step 2118, loss 9.78439e-05, acc 1\n",
      "2018-08-10T17:28:01.957424: step 2119, loss 0.00198683, acc 1\n",
      "2018-08-10T17:28:02.112030: step 2120, loss 0.0220019, acc 0.984375\n",
      "2018-08-10T17:28:02.256969: step 2121, loss 7.90759e-05, acc 1\n",
      "2018-08-10T17:28:02.398420: step 2122, loss 0.000346047, acc 1\n",
      "2018-08-10T17:28:02.538225: step 2123, loss 0.000691245, acc 1\n",
      "2018-08-10T17:28:02.679309: step 2124, loss 0.000441069, acc 1\n",
      "2018-08-10T17:28:02.819809: step 2125, loss 0.000229023, acc 1\n",
      "2018-08-10T17:28:02.958917: step 2126, loss 0.00201864, acc 1\n",
      "2018-08-10T17:28:03.098301: step 2127, loss 5.57895e-05, acc 1\n",
      "2018-08-10T17:28:03.237068: step 2128, loss 0.000164496, acc 1\n",
      "2018-08-10T17:28:03.376108: step 2129, loss 0.00173435, acc 1\n",
      "2018-08-10T17:28:03.518469: step 2130, loss 0.0371104, acc 0.984375\n",
      "2018-08-10T17:28:03.659832: step 2131, loss 5.12723e-05, acc 1\n",
      "2018-08-10T17:28:03.793954: step 2132, loss 0.080153, acc 0.983871\n",
      "2018-08-10T17:28:03.933831: step 2133, loss 3.37861e-05, acc 1\n",
      "2018-08-10T17:28:04.072244: step 2134, loss 0.000593556, acc 1\n",
      "2018-08-10T17:28:04.211345: step 2135, loss 3.79042e-05, acc 1\n",
      "2018-08-10T17:28:04.349725: step 2136, loss 0.000481285, acc 1\n",
      "2018-08-10T17:28:04.486188: step 2137, loss 0.000109158, acc 1\n",
      "2018-08-10T17:28:04.627641: step 2138, loss 0.000119081, acc 1\n",
      "2018-08-10T17:28:04.770218: step 2139, loss 0.00162569, acc 1\n",
      "2018-08-10T17:28:04.907595: step 2140, loss 0.000777978, acc 1\n",
      "2018-08-10T17:28:05.046141: step 2141, loss 4.13677e-05, acc 1\n",
      "2018-08-10T17:28:05.185793: step 2142, loss 0.0462942, acc 0.984375\n",
      "2018-08-10T17:28:05.323070: step 2143, loss 1.40325e-05, acc 1\n",
      "2018-08-10T17:28:05.467775: step 2144, loss 0.00092834, acc 1\n",
      "2018-08-10T17:28:05.610759: step 2145, loss 0.000115507, acc 1\n",
      "2018-08-10T17:28:05.752227: step 2146, loss 0.00119591, acc 1\n",
      "2018-08-10T17:28:05.890921: step 2147, loss 0.000220546, acc 1\n",
      "2018-08-10T17:28:06.029203: step 2148, loss 0.00186978, acc 1\n",
      "2018-08-10T17:28:06.170172: step 2149, loss 0.000273455, acc 1\n",
      "2018-08-10T17:28:06.308565: step 2150, loss 0.00057206, acc 1\n",
      "2018-08-10T17:28:06.450196: step 2151, loss 0.000549097, acc 1\n",
      "2018-08-10T17:28:06.591958: step 2152, loss 0.000582676, acc 1\n",
      "2018-08-10T17:28:06.735015: step 2153, loss 6.42607e-05, acc 1\n",
      "2018-08-10T17:28:06.878825: step 2154, loss 5.72876e-05, acc 1\n",
      "2018-08-10T17:28:07.026252: step 2155, loss 0.000329902, acc 1\n",
      "2018-08-10T17:28:07.172732: step 2156, loss 7.92301e-05, acc 1\n",
      "2018-08-10T17:28:07.319233: step 2157, loss 0.000344091, acc 1\n",
      "2018-08-10T17:28:07.466177: step 2158, loss 0.000207423, acc 1\n",
      "2018-08-10T17:28:07.620942: step 2159, loss 3.81892e-05, acc 1\n",
      "2018-08-10T17:28:07.766145: step 2160, loss 0.0016149, acc 1\n",
      "2018-08-10T17:28:07.911377: step 2161, loss 7.23151e-05, acc 1\n",
      "2018-08-10T17:28:08.059635: step 2162, loss 0.000320275, acc 1\n",
      "2018-08-10T17:28:08.206826: step 2163, loss 0.000228925, acc 1\n",
      "2018-08-10T17:28:08.352938: step 2164, loss 0.000119476, acc 1\n",
      "2018-08-10T17:28:08.500027: step 2165, loss 0.000465967, acc 1\n",
      "2018-08-10T17:28:08.647896: step 2166, loss 0.00615895, acc 1\n",
      "2018-08-10T17:28:08.796275: step 2167, loss 0.00036246, acc 1\n",
      "2018-08-10T17:28:08.942361: step 2168, loss 0.000583383, acc 1\n",
      "2018-08-10T17:28:09.088796: step 2169, loss 0.000526508, acc 1\n",
      "2018-08-10T17:28:09.235726: step 2170, loss 0.000123138, acc 1\n",
      "2018-08-10T17:28:09.379135: step 2171, loss 7.4175e-05, acc 1\n",
      "2018-08-10T17:28:09.527742: step 2172, loss 0.0013858, acc 1\n",
      "2018-08-10T17:28:09.675248: step 2173, loss 0.00155085, acc 1\n",
      "2018-08-10T17:28:09.820257: step 2174, loss 0.00032428, acc 1\n",
      "2018-08-10T17:28:09.968791: step 2175, loss 0.000483535, acc 1\n",
      "2018-08-10T17:28:10.114117: step 2176, loss 0.000203147, acc 1\n",
      "2018-08-10T17:28:10.267693: step 2177, loss 0.000119756, acc 1\n",
      "2018-08-10T17:28:10.412356: step 2178, loss 6.85853e-05, acc 1\n",
      "2018-08-10T17:28:10.558679: step 2179, loss 2.83826e-05, acc 1\n",
      "2018-08-10T17:28:10.706735: step 2180, loss 0.000257673, acc 1\n",
      "2018-08-10T17:28:10.856547: step 2181, loss 0.00071006, acc 1\n",
      "2018-08-10T17:28:10.999915: step 2182, loss 0.00088751, acc 1\n",
      "2018-08-10T17:28:11.146191: step 2183, loss 0.000379523, acc 1\n",
      "2018-08-10T17:28:11.288141: step 2184, loss 0.00146624, acc 1\n",
      "2018-08-10T17:28:11.438628: step 2185, loss 0.000263019, acc 1\n",
      "2018-08-10T17:28:11.587745: step 2186, loss 0.000267511, acc 1\n",
      "2018-08-10T17:28:11.737376: step 2187, loss 0.000325311, acc 1\n",
      "2018-08-10T17:28:11.886299: step 2188, loss 0.00019118, acc 1\n",
      "2018-08-10T17:28:12.036845: step 2189, loss 0.000343573, acc 1\n",
      "2018-08-10T17:28:12.187209: step 2190, loss 0.000174103, acc 1\n",
      "2018-08-10T17:28:12.338076: step 2191, loss 0.000276185, acc 1\n",
      "2018-08-10T17:28:12.487470: step 2192, loss 0.00200996, acc 1\n",
      "2018-08-10T17:28:12.635956: step 2193, loss 0.000367608, acc 1\n",
      "2018-08-10T17:28:12.784268: step 2194, loss 0.000974064, acc 1\n",
      "2018-08-10T17:28:12.935086: step 2195, loss 7.76527e-05, acc 1\n",
      "2018-08-10T17:28:13.080944: step 2196, loss 0.000118805, acc 1\n",
      "2018-08-10T17:28:13.226728: step 2197, loss 0.000420114, acc 1\n",
      "2018-08-10T17:28:13.373229: step 2198, loss 8.34227e-05, acc 1\n",
      "2018-08-10T17:28:13.522968: step 2199, loss 3.55659e-05, acc 1\n",
      "2018-08-10T17:28:13.668789: step 2200, loss 0.000134116, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:28:13.809059: step 2200, loss 0.921731, acc 0.84689\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533921767/checkpoints/model-2200\n",
      "\n",
      "2018-08-10T17:28:14.015631: step 2201, loss 0.00165639, acc 1\n",
      "2018-08-10T17:28:14.162096: step 2202, loss 6.38835e-05, acc 1\n",
      "2018-08-10T17:28:14.311892: step 2203, loss 0.000151961, acc 1\n",
      "2018-08-10T17:28:14.458750: step 2204, loss 0.000188958, acc 1\n",
      "2018-08-10T17:28:14.607498: step 2205, loss 7.15038e-05, acc 1\n",
      "2018-08-10T17:28:14.755428: step 2206, loss 0.000133583, acc 1\n",
      "2018-08-10T17:28:14.903674: step 2207, loss 0.000187043, acc 1\n",
      "2018-08-10T17:28:15.048554: step 2208, loss 0.0013206, acc 1\n",
      "2018-08-10T17:28:15.198319: step 2209, loss 9.0715e-05, acc 1\n",
      "2018-08-10T17:28:15.344593: step 2210, loss 9.0246e-05, acc 1\n",
      "2018-08-10T17:28:15.498727: step 2211, loss 0.0178351, acc 0.984375\n",
      "2018-08-10T17:28:15.649413: step 2212, loss 0.000122081, acc 1\n",
      "2018-08-10T17:28:15.798348: step 2213, loss 0.000264796, acc 1\n",
      "2018-08-10T17:28:15.945979: step 2214, loss 5.84548e-05, acc 1\n",
      "2018-08-10T17:28:16.092641: step 2215, loss 0.000956733, acc 1\n",
      "2018-08-10T17:28:16.244652: step 2216, loss 0.000214656, acc 1\n",
      "2018-08-10T17:28:16.396869: step 2217, loss 0.000171676, acc 1\n",
      "2018-08-10T17:28:16.545625: step 2218, loss 6.16078e-05, acc 1\n",
      "2018-08-10T17:28:16.692100: step 2219, loss 0.00164474, acc 1\n",
      "2018-08-10T17:28:16.836997: step 2220, loss 3.59633e-05, acc 1\n",
      "2018-08-10T17:28:16.981075: step 2221, loss 3.23482e-05, acc 1\n",
      "2018-08-10T17:28:17.121112: step 2222, loss 0.00807513, acc 1\n",
      "2018-08-10T17:28:17.257886: step 2223, loss 0.000172524, acc 1\n",
      "2018-08-10T17:28:17.404110: step 2224, loss 0.00091859, acc 1\n",
      "2018-08-10T17:28:17.553316: step 2225, loss 0.000426591, acc 1\n",
      "2018-08-10T17:28:17.704894: step 2226, loss 0.000674001, acc 1\n",
      "2018-08-10T17:28:17.853886: step 2227, loss 0.00048432, acc 1\n",
      "2018-08-10T17:28:18.002047: step 2228, loss 0.0185892, acc 0.984375\n",
      "2018-08-10T17:28:18.153653: step 2229, loss 0.000127049, acc 1\n",
      "2018-08-10T17:28:18.300971: step 2230, loss 5.89212e-05, acc 1\n",
      "2018-08-10T17:28:18.448944: step 2231, loss 0.000972539, acc 1\n",
      "2018-08-10T17:28:18.596144: step 2232, loss 1.9526e-05, acc 1\n",
      "2018-08-10T17:28:18.749734: step 2233, loss 0.000257267, acc 1\n",
      "2018-08-10T17:28:18.897398: step 2234, loss 8.90455e-05, acc 1\n",
      "2018-08-10T17:28:19.047369: step 2235, loss 0.00140453, acc 1\n",
      "2018-08-10T17:28:19.192408: step 2236, loss 0.000405159, acc 1\n",
      "2018-08-10T17:28:19.342877: step 2237, loss 0.000105045, acc 1\n",
      "2018-08-10T17:28:19.493094: step 2238, loss 7.07954e-05, acc 1\n",
      "2018-08-10T17:28:19.645710: step 2239, loss 0.000192377, acc 1\n",
      "2018-08-10T17:28:19.793274: step 2240, loss 0.000764497, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:28:19.940596: step 2241, loss 1.55085e-05, acc 1\n",
      "2018-08-10T17:28:20.087993: step 2242, loss 0.00101146, acc 1\n",
      "2018-08-10T17:28:20.240287: step 2243, loss 0.000347452, acc 1\n",
      "2018-08-10T17:28:20.389532: step 2244, loss 7.09535e-05, acc 1\n",
      "2018-08-10T17:28:20.538615: step 2245, loss 0.00211001, acc 1\n",
      "2018-08-10T17:28:20.687590: step 2246, loss 7.70882e-05, acc 1\n",
      "2018-08-10T17:28:20.836781: step 2247, loss 0.00135503, acc 1\n",
      "2018-08-10T17:28:20.984766: step 2248, loss 0.000254837, acc 1\n",
      "2018-08-10T17:28:21.132587: step 2249, loss 6.18753e-05, acc 1\n",
      "2018-08-10T17:28:21.286673: step 2250, loss 5.18689e-05, acc 1\n",
      "2018-08-10T17:28:21.446598: step 2251, loss 0.000206844, acc 1\n",
      "2018-08-10T17:28:21.595676: step 2252, loss 0.000248282, acc 1\n",
      "2018-08-10T17:28:21.746465: step 2253, loss 0.000296266, acc 1\n",
      "2018-08-10T17:28:21.893880: step 2254, loss 0.000112172, acc 1\n",
      "2018-08-10T17:28:22.044348: step 2255, loss 2.13968e-05, acc 1\n",
      "2018-08-10T17:28:22.196078: step 2256, loss 0.0067306, acc 1\n",
      "2018-08-10T17:28:22.345333: step 2257, loss 4.17853e-05, acc 1\n",
      "2018-08-10T17:28:22.495232: step 2258, loss 0.000453726, acc 1\n",
      "2018-08-10T17:28:22.644581: step 2259, loss 0.000707748, acc 1\n",
      "2018-08-10T17:28:22.793243: step 2260, loss 0.000284831, acc 1\n",
      "2018-08-10T17:28:22.944240: step 2261, loss 0.000570494, acc 1\n",
      "2018-08-10T17:28:23.091608: step 2262, loss 0.000523934, acc 1\n",
      "2018-08-10T17:28:23.234417: step 2263, loss 0.00382252, acc 1\n",
      "2018-08-10T17:28:23.375025: step 2264, loss 0.000593271, acc 1\n",
      "2018-08-10T17:28:23.518994: step 2265, loss 0.00609664, acc 1\n",
      "2018-08-10T17:28:23.660319: step 2266, loss 0.00105607, acc 1\n",
      "2018-08-10T17:28:23.801329: step 2267, loss 7.06698e-05, acc 1\n",
      "2018-08-10T17:28:23.938455: step 2268, loss 0.000319692, acc 1\n",
      "2018-08-10T17:28:24.078474: step 2269, loss 0.000164198, acc 1\n",
      "2018-08-10T17:28:24.216906: step 2270, loss 5.38857e-05, acc 1\n",
      "2018-08-10T17:28:24.356901: step 2271, loss 8.79381e-05, acc 1\n",
      "2018-08-10T17:28:24.495597: step 2272, loss 0.00186764, acc 1\n",
      "2018-08-10T17:28:24.637838: step 2273, loss 0.000175147, acc 1\n",
      "2018-08-10T17:28:24.778109: step 2274, loss 4.68451e-05, acc 1\n",
      "2018-08-10T17:28:24.915061: step 2275, loss 0.000392601, acc 1\n",
      "2018-08-10T17:28:25.054767: step 2276, loss 0.00055773, acc 1\n",
      "2018-08-10T17:28:25.196212: step 2277, loss 0.00142625, acc 1\n",
      "2018-08-10T17:28:25.335661: step 2278, loss 0.00178477, acc 1\n",
      "2018-08-10T17:28:25.479073: step 2279, loss 0.00192245, acc 1\n",
      "2018-08-10T17:28:25.619064: step 2280, loss 0.000672567, acc 1\n",
      "2018-08-10T17:28:25.756399: step 2281, loss 0.00269004, acc 1\n",
      "2018-08-10T17:28:25.893163: step 2282, loss 5.46922e-05, acc 1\n",
      "2018-08-10T17:28:26.031385: step 2283, loss 0.0011607, acc 1\n",
      "2018-08-10T17:28:26.170655: step 2284, loss 0.000139126, acc 1\n",
      "2018-08-10T17:28:26.308862: step 2285, loss 0.000246007, acc 1\n",
      "2018-08-10T17:28:26.444918: step 2286, loss 4.78471e-05, acc 1\n",
      "2018-08-10T17:28:26.583819: step 2287, loss 0.00163179, acc 1\n",
      "2018-08-10T17:28:26.717517: step 2288, loss 0.00059354, acc 1\n",
      "2018-08-10T17:28:26.855676: step 2289, loss 8.9851e-05, acc 1\n",
      "2018-08-10T17:28:26.997489: step 2290, loss 0.000243187, acc 1\n",
      "2018-08-10T17:28:27.137949: step 2291, loss 3.37655e-05, acc 1\n",
      "2018-08-10T17:28:27.280067: step 2292, loss 0.00176234, acc 1\n",
      "2018-08-10T17:28:27.423208: step 2293, loss 0.00093642, acc 1\n",
      "2018-08-10T17:28:27.573342: step 2294, loss 0.0271722, acc 0.984375\n",
      "2018-08-10T17:28:27.721266: step 2295, loss 6.57097e-05, acc 1\n",
      "2018-08-10T17:28:27.866870: step 2296, loss 9.51345e-05, acc 1\n",
      "2018-08-10T17:28:28.013718: step 2297, loss 0.00133167, acc 1\n",
      "2018-08-10T17:28:28.157280: step 2298, loss 0.000122368, acc 1\n",
      "2018-08-10T17:28:28.301710: step 2299, loss 0.00109658, acc 1\n",
      "2018-08-10T17:28:28.447195: step 2300, loss 0.000377512, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:28:28.586266: step 2300, loss 0.812912, acc 0.856459\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533921767/checkpoints/model-2300\n",
      "\n",
      "2018-08-10T17:28:28.783679: step 2301, loss 0.00225047, acc 1\n",
      "2018-08-10T17:28:28.928460: step 2302, loss 0.000133701, acc 1\n",
      "2018-08-10T17:28:29.071662: step 2303, loss 0.000148889, acc 1\n",
      "2018-08-10T17:28:29.219690: step 2304, loss 0.000447523, acc 1\n",
      "2018-08-10T17:28:29.365654: step 2305, loss 0.00187467, acc 1\n",
      "2018-08-10T17:28:29.512830: step 2306, loss 0.000359184, acc 1\n",
      "2018-08-10T17:28:29.662490: step 2307, loss 0.000438073, acc 1\n",
      "2018-08-10T17:28:29.805712: step 2308, loss 0.00011034, acc 1\n",
      "2018-08-10T17:28:29.948696: step 2309, loss 0.00973852, acc 1\n",
      "2018-08-10T17:28:30.104369: step 2310, loss 0.000350098, acc 1\n",
      "2018-08-10T17:28:30.254216: step 2311, loss 9.428e-05, acc 1\n",
      "2018-08-10T17:28:30.400068: step 2312, loss 2.77896e-05, acc 1\n",
      "2018-08-10T17:28:30.544221: step 2313, loss 0.000365839, acc 1\n",
      "2018-08-10T17:28:30.684692: step 2314, loss 0.00109289, acc 1\n",
      "2018-08-10T17:28:30.831752: step 2315, loss 0.0179702, acc 0.984375\n",
      "2018-08-10T17:28:30.978269: step 2316, loss 3.93037e-05, acc 1\n",
      "2018-08-10T17:28:31.129482: step 2317, loss 9.96528e-05, acc 1\n",
      "2018-08-10T17:28:31.275756: step 2318, loss 2.17087e-05, acc 1\n",
      "2018-08-10T17:28:31.419932: step 2319, loss 3.866e-05, acc 1\n",
      "2018-08-10T17:28:31.569710: step 2320, loss 5.2129e-05, acc 1\n",
      "2018-08-10T17:28:31.717940: step 2321, loss 8.54909e-05, acc 1\n",
      "2018-08-10T17:28:31.864782: step 2322, loss 0.00013002, acc 1\n",
      "2018-08-10T17:28:32.010909: step 2323, loss 9.69178e-05, acc 1\n",
      "2018-08-10T17:28:32.157211: step 2324, loss 0.00016416, acc 1\n",
      "2018-08-10T17:28:32.303894: step 2325, loss 0.00018712, acc 1\n",
      "2018-08-10T17:28:32.452294: step 2326, loss 8.07743e-05, acc 1\n",
      "2018-08-10T17:28:32.594699: step 2327, loss 0.000141188, acc 1\n",
      "2018-08-10T17:28:32.744475: step 2328, loss 7.60002e-05, acc 1\n",
      "2018-08-10T17:28:32.890027: step 2329, loss 0.00165861, acc 1\n",
      "2018-08-10T17:28:33.035315: step 2330, loss 0.000476522, acc 1\n",
      "2018-08-10T17:28:33.181112: step 2331, loss 0.000651431, acc 1\n",
      "2018-08-10T17:28:33.329073: step 2332, loss 0.00231643, acc 1\n",
      "2018-08-10T17:28:33.475785: step 2333, loss 5.8898e-05, acc 1\n",
      "2018-08-10T17:28:33.623431: step 2334, loss 0.00012728, acc 1\n",
      "2018-08-10T17:28:33.767298: step 2335, loss 0.000303432, acc 1\n",
      "2018-08-10T17:28:33.914143: step 2336, loss 0.00015243, acc 1\n",
      "2018-08-10T17:28:34.059217: step 2337, loss 0.000123061, acc 1\n",
      "2018-08-10T17:28:34.206281: step 2338, loss 0.000175744, acc 1\n",
      "2018-08-10T17:28:34.352670: step 2339, loss 0.00034256, acc 1\n",
      "2018-08-10T17:28:34.496371: step 2340, loss 2.90726e-05, acc 1\n",
      "2018-08-10T17:28:34.647417: step 2341, loss 0.0176888, acc 0.984375\n",
      "2018-08-10T17:28:34.796172: step 2342, loss 3.2396e-05, acc 1\n",
      "2018-08-10T17:28:34.943893: step 2343, loss 4.20932e-05, acc 1\n",
      "2018-08-10T17:28:35.091259: step 2344, loss 0.000283059, acc 1\n",
      "2018-08-10T17:28:35.242163: step 2345, loss 0.0019587, acc 1\n",
      "2018-08-10T17:28:35.390266: step 2346, loss 0.000151319, acc 1\n",
      "2018-08-10T17:28:35.539399: step 2347, loss 0.00018754, acc 1\n",
      "2018-08-10T17:28:35.687977: step 2348, loss 3.37881e-05, acc 1\n",
      "2018-08-10T17:28:35.834089: step 2349, loss 3.87824e-05, acc 1\n",
      "2018-08-10T17:28:35.982920: step 2350, loss 0.000260483, acc 1\n",
      "2018-08-10T17:28:36.129074: step 2351, loss 3.14251e-05, acc 1\n",
      "2018-08-10T17:28:36.277129: step 2352, loss 4.50503e-05, acc 1\n",
      "2018-08-10T17:28:36.421624: step 2353, loss 0.00010908, acc 1\n",
      "2018-08-10T17:28:36.570868: step 2354, loss 0.000919355, acc 1\n",
      "2018-08-10T17:28:36.716570: step 2355, loss 0.000233009, acc 1\n",
      "2018-08-10T17:28:36.860730: step 2356, loss 0.00156148, acc 1\n",
      "2018-08-10T17:28:37.010489: step 2357, loss 0.00105602, acc 1\n",
      "2018-08-10T17:28:37.154294: step 2358, loss 8.97045e-05, acc 1\n",
      "2018-08-10T17:28:37.299744: step 2359, loss 0.000161594, acc 1\n",
      "2018-08-10T17:28:37.449981: step 2360, loss 8.4667e-05, acc 1\n",
      "2018-08-10T17:28:37.592840: step 2361, loss 7.54332e-05, acc 1\n",
      "2018-08-10T17:28:37.737694: step 2362, loss 0.00156353, acc 1\n",
      "2018-08-10T17:28:37.878766: step 2363, loss 0.000217778, acc 1\n",
      "2018-08-10T17:28:38.018943: step 2364, loss 4.98081e-05, acc 1\n",
      "2018-08-10T17:28:38.158134: step 2365, loss 0.000226401, acc 1\n",
      "2018-08-10T17:28:38.294967: step 2366, loss 7.01221e-05, acc 1\n",
      "2018-08-10T17:28:38.437502: step 2367, loss 0.00106502, acc 1\n",
      "2018-08-10T17:28:38.584347: step 2368, loss 0.000135092, acc 1\n",
      "2018-08-10T17:28:38.732400: step 2369, loss 0.00743076, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:28:38.881113: step 2370, loss 0.000560287, acc 1\n",
      "2018-08-10T17:28:39.026767: step 2371, loss 2.38795e-05, acc 1\n",
      "2018-08-10T17:28:39.173355: step 2372, loss 4.7792e-05, acc 1\n",
      "2018-08-10T17:28:39.318383: step 2373, loss 1.28101e-05, acc 1\n",
      "2018-08-10T17:28:39.468621: step 2374, loss 0.00023922, acc 1\n",
      "2018-08-10T17:28:39.616196: step 2375, loss 0.000259394, acc 1\n",
      "2018-08-10T17:28:39.760949: step 2376, loss 7.42448e-05, acc 1\n",
      "2018-08-10T17:28:39.906572: step 2377, loss 0.000112914, acc 1\n",
      "2018-08-10T17:28:40.054656: step 2378, loss 0.000120646, acc 1\n",
      "2018-08-10T17:28:40.198634: step 2379, loss 0.00282183, acc 1\n",
      "2018-08-10T17:28:40.346853: step 2380, loss 5.01917e-05, acc 1\n",
      "2018-08-10T17:28:40.497101: step 2381, loss 3.59057e-05, acc 1\n",
      "2018-08-10T17:28:40.651481: step 2382, loss 0.000389609, acc 1\n",
      "2018-08-10T17:28:40.800794: step 2383, loss 0.00114749, acc 1\n",
      "2018-08-10T17:28:40.948970: step 2384, loss 0.00012051, acc 1\n",
      "2018-08-10T17:28:41.101069: step 2385, loss 7.81602e-05, acc 1\n",
      "2018-08-10T17:28:41.249024: step 2386, loss 0.000173358, acc 1\n",
      "2018-08-10T17:28:41.399965: step 2387, loss 0.000920643, acc 1\n",
      "2018-08-10T17:28:41.551037: step 2388, loss 0.000162016, acc 1\n",
      "2018-08-10T17:28:41.703003: step 2389, loss 5.10502e-05, acc 1\n",
      "2018-08-10T17:28:41.855367: step 2390, loss 0.000556429, acc 1\n",
      "2018-08-10T17:28:42.004758: step 2391, loss 0.00804402, acc 1\n",
      "2018-08-10T17:28:42.150976: step 2392, loss 0.0416424, acc 0.983871\n",
      "2018-08-10T17:28:42.297752: step 2393, loss 5.64417e-05, acc 1\n",
      "2018-08-10T17:28:42.448230: step 2394, loss 0.000602067, acc 1\n",
      "2018-08-10T17:28:42.598294: step 2395, loss 0.00127432, acc 1\n",
      "2018-08-10T17:28:42.747902: step 2396, loss 0.000237059, acc 1\n",
      "2018-08-10T17:28:42.896490: step 2397, loss 0.0139493, acc 0.984375\n",
      "2018-08-10T17:28:43.049447: step 2398, loss 0.00277206, acc 1\n",
      "2018-08-10T17:28:43.201734: step 2399, loss 0.000154729, acc 1\n",
      "2018-08-10T17:28:43.352112: step 2400, loss 0.00014211, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:28:43.498712: step 2400, loss 1.09363, acc 0.842105\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533921767/checkpoints/model-2400\n",
      "\n",
      "2018-08-10T17:28:43.714759: step 2401, loss 0.00020602, acc 1\n",
      "2018-08-10T17:28:43.862339: step 2402, loss 0.0174016, acc 0.984375\n",
      "2018-08-10T17:28:44.006963: step 2403, loss 0.00122175, acc 1\n",
      "2018-08-10T17:28:44.146309: step 2404, loss 0.000175623, acc 1\n",
      "2018-08-10T17:28:44.283727: step 2405, loss 6.53311e-05, acc 1\n",
      "2018-08-10T17:28:44.421499: step 2406, loss 0.000103406, acc 1\n",
      "2018-08-10T17:28:44.561920: step 2407, loss 0.00176333, acc 1\n",
      "2018-08-10T17:28:44.703124: step 2408, loss 0.000121081, acc 1\n",
      "2018-08-10T17:28:44.840270: step 2409, loss 0.0127872, acc 0.984375\n",
      "2018-08-10T17:28:44.984247: step 2410, loss 0.000279208, acc 1\n",
      "2018-08-10T17:28:45.121774: step 2411, loss 7.80468e-05, acc 1\n",
      "2018-08-10T17:28:45.261598: step 2412, loss 0.000497934, acc 1\n",
      "2018-08-10T17:28:45.404629: step 2413, loss 0.000233114, acc 1\n",
      "2018-08-10T17:28:45.544537: step 2414, loss 7.65562e-05, acc 1\n",
      "2018-08-10T17:28:45.686282: step 2415, loss 0.000456058, acc 1\n",
      "2018-08-10T17:28:45.824633: step 2416, loss 0.000260431, acc 1\n",
      "2018-08-10T17:28:45.966809: step 2417, loss 0.000475601, acc 1\n",
      "2018-08-10T17:28:46.102562: step 2418, loss 6.34837e-05, acc 1\n",
      "2018-08-10T17:28:46.240838: step 2419, loss 4.90515e-05, acc 1\n",
      "2018-08-10T17:28:46.382254: step 2420, loss 0.000276603, acc 1\n",
      "2018-08-10T17:28:46.522109: step 2421, loss 0.000831393, acc 1\n",
      "2018-08-10T17:28:46.661198: step 2422, loss 0.00105281, acc 1\n",
      "2018-08-10T17:28:46.800758: step 2423, loss 0.00220496, acc 1\n",
      "2018-08-10T17:28:46.940195: step 2424, loss 0.000132746, acc 1\n",
      "2018-08-10T17:28:47.078503: step 2425, loss 0.00248719, acc 1\n",
      "2018-08-10T17:28:47.216097: step 2426, loss 0.00109014, acc 1\n",
      "2018-08-10T17:28:47.355895: step 2427, loss 3.85574e-05, acc 1\n",
      "2018-08-10T17:28:47.496789: step 2428, loss 0.00353487, acc 1\n",
      "2018-08-10T17:28:47.639568: step 2429, loss 0.00140236, acc 1\n",
      "2018-08-10T17:28:47.779426: step 2430, loss 0.000300398, acc 1\n",
      "2018-08-10T17:28:47.915611: step 2431, loss 0.000181164, acc 1\n",
      "2018-08-10T17:28:48.054955: step 2432, loss 0.000354664, acc 1\n",
      "2018-08-10T17:28:48.196765: step 2433, loss 0.0032073, acc 1\n",
      "2018-08-10T17:28:48.338490: step 2434, loss 0.00026363, acc 1\n",
      "2018-08-10T17:28:48.482134: step 2435, loss 0.000941079, acc 1\n",
      "2018-08-10T17:28:48.630040: step 2436, loss 3.81888e-05, acc 1\n",
      "2018-08-10T17:28:48.777450: step 2437, loss 0.000568116, acc 1\n",
      "2018-08-10T17:28:48.923042: step 2438, loss 0.00043551, acc 1\n",
      "2018-08-10T17:28:49.069632: step 2439, loss 0.000109746, acc 1\n",
      "2018-08-10T17:28:49.216849: step 2440, loss 0.000191269, acc 1\n",
      "2018-08-10T17:28:49.363701: step 2441, loss 7.39911e-05, acc 1\n",
      "2018-08-10T17:28:49.514438: step 2442, loss 0.000396226, acc 1\n",
      "2018-08-10T17:28:49.662476: step 2443, loss 0.000143853, acc 1\n",
      "2018-08-10T17:28:49.802344: step 2444, loss 1.39962e-05, acc 1\n",
      "2018-08-10T17:28:49.949267: step 2445, loss 7.49789e-05, acc 1\n",
      "2018-08-10T17:28:50.096885: step 2446, loss 0.000151049, acc 1\n",
      "2018-08-10T17:28:50.252178: step 2447, loss 0.00348182, acc 1\n",
      "2018-08-10T17:28:50.399368: step 2448, loss 0.000966619, acc 1\n",
      "2018-08-10T17:28:50.550275: step 2449, loss 0.000233426, acc 1\n",
      "2018-08-10T17:28:50.702069: step 2450, loss 0.000729545, acc 1\n",
      "2018-08-10T17:28:50.847112: step 2451, loss 5.60941e-05, acc 1\n",
      "2018-08-10T17:28:50.994632: step 2452, loss 0.000166975, acc 1\n",
      "2018-08-10T17:28:51.138856: step 2453, loss 7.54117e-05, acc 1\n",
      "2018-08-10T17:28:51.285879: step 2454, loss 0.000200823, acc 1\n",
      "2018-08-10T17:28:51.437898: step 2455, loss 0.000344817, acc 1\n",
      "2018-08-10T17:28:51.584158: step 2456, loss 2.98182e-05, acc 1\n",
      "2018-08-10T17:28:51.728632: step 2457, loss 0.00036101, acc 1\n",
      "2018-08-10T17:28:51.875409: step 2458, loss 0.000488805, acc 1\n",
      "2018-08-10T17:28:52.023135: step 2459, loss 0.000384269, acc 1\n",
      "2018-08-10T17:28:52.171350: step 2460, loss 0.000150068, acc 1\n",
      "2018-08-10T17:28:52.319595: step 2461, loss 3.4997e-05, acc 1\n",
      "2018-08-10T17:28:52.469033: step 2462, loss 9.05966e-05, acc 1\n",
      "2018-08-10T17:28:52.616641: step 2463, loss 0.000152632, acc 1\n",
      "2018-08-10T17:28:52.760990: step 2464, loss 3.55566e-05, acc 1\n",
      "2018-08-10T17:28:52.909728: step 2465, loss 0.00047684, acc 1\n",
      "2018-08-10T17:28:53.055983: step 2466, loss 9.85156e-05, acc 1\n",
      "2018-08-10T17:28:53.206577: step 2467, loss 0.000780609, acc 1\n",
      "2018-08-10T17:28:53.353761: step 2468, loss 7.75005e-05, acc 1\n",
      "2018-08-10T17:28:53.506793: step 2469, loss 0.00390151, acc 1\n",
      "2018-08-10T17:28:53.652635: step 2470, loss 0.000121681, acc 1\n",
      "2018-08-10T17:28:53.803253: step 2471, loss 0.000512562, acc 1\n",
      "2018-08-10T17:28:53.950409: step 2472, loss 3.73552e-05, acc 1\n",
      "2018-08-10T17:28:54.098613: step 2473, loss 0.000321709, acc 1\n",
      "2018-08-10T17:28:54.245619: step 2474, loss 0.00327352, acc 1\n",
      "2018-08-10T17:28:54.390368: step 2475, loss 2.15622e-05, acc 1\n",
      "2018-08-10T17:28:54.536415: step 2476, loss 0.000343635, acc 1\n",
      "2018-08-10T17:28:54.685641: step 2477, loss 0.00128201, acc 1\n",
      "2018-08-10T17:28:54.833975: step 2478, loss 0.000255467, acc 1\n",
      "2018-08-10T17:28:54.980284: step 2479, loss 0.000480677, acc 1\n",
      "2018-08-10T17:28:55.124326: step 2480, loss 2.3395e-05, acc 1\n",
      "2018-08-10T17:28:55.275761: step 2481, loss 5.60253e-05, acc 1\n",
      "2018-08-10T17:28:55.425757: step 2482, loss 0.00089438, acc 1\n",
      "2018-08-10T17:28:55.572263: step 2483, loss 0.000181548, acc 1\n",
      "2018-08-10T17:28:55.719664: step 2484, loss 3.56649e-05, acc 1\n",
      "2018-08-10T17:28:55.866296: step 2485, loss 0.000228569, acc 1\n",
      "2018-08-10T17:28:56.016360: step 2486, loss 0.000114526, acc 1\n",
      "2018-08-10T17:28:56.164251: step 2487, loss 4.58343e-05, acc 1\n",
      "2018-08-10T17:28:56.308453: step 2488, loss 0.000198382, acc 1\n",
      "2018-08-10T17:28:56.454136: step 2489, loss 0.000295192, acc 1\n",
      "2018-08-10T17:28:56.603480: step 2490, loss 0.000117461, acc 1\n",
      "2018-08-10T17:28:56.755815: step 2491, loss 0.000359233, acc 1\n",
      "2018-08-10T17:28:56.900915: step 2492, loss 0.000237018, acc 1\n",
      "2018-08-10T17:28:57.047643: step 2493, loss 0.00014222, acc 1\n",
      "2018-08-10T17:28:57.192306: step 2494, loss 0.000509067, acc 1\n",
      "2018-08-10T17:28:57.338959: step 2495, loss 0.00066453, acc 1\n",
      "2018-08-10T17:28:57.483746: step 2496, loss 7.84748e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:28:57.635726: step 2497, loss 0.00431111, acc 1\n",
      "2018-08-10T17:28:57.783131: step 2498, loss 7.47595e-05, acc 1\n",
      "2018-08-10T17:28:57.929861: step 2499, loss 7.04662e-05, acc 1\n",
      "2018-08-10T17:28:58.074015: step 2500, loss 0.006382, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:28:58.214230: step 2500, loss 0.978796, acc 0.842105\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533921767/checkpoints/model-2500\n",
      "\n",
      "2018-08-10T17:28:58.417247: step 2501, loss 0.00114533, acc 1\n",
      "2018-08-10T17:28:58.565601: step 2502, loss 0.000156147, acc 1\n",
      "2018-08-10T17:28:58.714256: step 2503, loss 0.00111408, acc 1\n",
      "2018-08-10T17:28:58.854348: step 2504, loss 0.000137951, acc 1\n",
      "2018-08-10T17:28:58.994436: step 2505, loss 0.000620643, acc 1\n",
      "2018-08-10T17:28:59.136146: step 2506, loss 0.0049684, acc 1\n",
      "2018-08-10T17:28:59.281683: step 2507, loss 8.86211e-05, acc 1\n",
      "2018-08-10T17:28:59.430902: step 2508, loss 0.000367905, acc 1\n",
      "2018-08-10T17:28:59.578208: step 2509, loss 0.000120096, acc 1\n",
      "2018-08-10T17:28:59.725259: step 2510, loss 2.9504e-05, acc 1\n",
      "2018-08-10T17:28:59.871238: step 2511, loss 8.77459e-05, acc 1\n",
      "2018-08-10T17:29:00.018541: step 2512, loss 0.00100603, acc 1\n",
      "2018-08-10T17:29:00.166864: step 2513, loss 5.34982e-05, acc 1\n",
      "2018-08-10T17:29:00.315222: step 2514, loss 3.59686e-05, acc 1\n",
      "2018-08-10T17:29:00.465911: step 2515, loss 0.00329737, acc 1\n",
      "2018-08-10T17:29:00.615364: step 2516, loss 9.02485e-05, acc 1\n",
      "2018-08-10T17:29:00.761391: step 2517, loss 0.00318722, acc 1\n",
      "2018-08-10T17:29:00.912362: step 2518, loss 0.000301529, acc 1\n",
      "2018-08-10T17:29:01.061966: step 2519, loss 0.000239985, acc 1\n",
      "2018-08-10T17:29:01.210976: step 2520, loss 9.58736e-05, acc 1\n",
      "2018-08-10T17:29:01.358953: step 2521, loss 0.000389881, acc 1\n",
      "2018-08-10T17:29:01.502980: step 2522, loss 2.50506e-05, acc 1\n",
      "2018-08-10T17:29:01.652102: step 2523, loss 0.00100474, acc 1\n",
      "2018-08-10T17:29:01.799060: step 2524, loss 0.000171144, acc 1\n",
      "2018-08-10T17:29:01.947276: step 2525, loss 0.000178805, acc 1\n",
      "2018-08-10T17:29:02.095438: step 2526, loss 0.000160432, acc 1\n",
      "2018-08-10T17:29:02.243647: step 2527, loss 8.50919e-05, acc 1\n",
      "2018-08-10T17:29:02.391958: step 2528, loss 0.00109527, acc 1\n",
      "2018-08-10T17:29:02.539334: step 2529, loss 5.10223e-05, acc 1\n",
      "2018-08-10T17:29:02.686346: step 2530, loss 3.63024e-05, acc 1\n",
      "2018-08-10T17:29:02.834220: step 2531, loss 0.00105891, acc 1\n",
      "2018-08-10T17:29:02.983694: step 2532, loss 4.80186e-05, acc 1\n",
      "2018-08-10T17:29:03.133030: step 2533, loss 0.00091194, acc 1\n",
      "2018-08-10T17:29:03.279403: step 2534, loss 0.00160596, acc 1\n",
      "2018-08-10T17:29:03.420686: step 2535, loss 0.000195006, acc 1\n",
      "2018-08-10T17:29:03.568112: step 2536, loss 0.00555686, acc 1\n",
      "2018-08-10T17:29:03.719024: step 2537, loss 0.000225642, acc 1\n",
      "2018-08-10T17:29:03.866302: step 2538, loss 8.81728e-06, acc 1\n",
      "2018-08-10T17:29:04.012237: step 2539, loss 0.000239599, acc 1\n",
      "2018-08-10T17:29:04.160787: step 2540, loss 0.00132194, acc 1\n",
      "2018-08-10T17:29:04.309717: step 2541, loss 3.79258e-05, acc 1\n",
      "2018-08-10T17:29:04.457203: step 2542, loss 5.15197e-05, acc 1\n",
      "2018-08-10T17:29:04.604385: step 2543, loss 0.000439358, acc 1\n",
      "2018-08-10T17:29:04.750359: step 2544, loss 1.89005e-05, acc 1\n",
      "2018-08-10T17:29:04.894244: step 2545, loss 6.22241e-05, acc 1\n",
      "2018-08-10T17:29:05.030885: step 2546, loss 0.0011768, acc 1\n",
      "2018-08-10T17:29:05.169960: step 2547, loss 0.000120196, acc 1\n",
      "2018-08-10T17:29:05.303718: step 2548, loss 0.000105781, acc 1\n",
      "2018-08-10T17:29:05.442852: step 2549, loss 3.76578e-05, acc 1\n",
      "2018-08-10T17:29:05.579460: step 2550, loss 0.000185804, acc 1\n",
      "2018-08-10T17:29:05.719991: step 2551, loss 0.000162198, acc 1\n",
      "2018-08-10T17:29:05.857026: step 2552, loss 0.00118311, acc 1\n",
      "2018-08-10T17:29:05.994320: step 2553, loss 3.43929e-05, acc 1\n",
      "2018-08-10T17:29:06.130299: step 2554, loss 4.32214e-05, acc 1\n",
      "2018-08-10T17:29:06.268400: step 2555, loss 6.73231e-05, acc 1\n",
      "2018-08-10T17:29:06.407867: step 2556, loss 2.7695e-05, acc 1\n",
      "2018-08-10T17:29:06.544233: step 2557, loss 0.000122311, acc 1\n",
      "2018-08-10T17:29:06.681996: step 2558, loss 1.81625e-05, acc 1\n",
      "2018-08-10T17:29:06.821360: step 2559, loss 0.000108259, acc 1\n",
      "2018-08-10T17:29:06.959778: step 2560, loss 0.000137761, acc 1\n",
      "2018-08-10T17:29:07.093612: step 2561, loss 7.90187e-05, acc 1\n",
      "2018-08-10T17:29:07.230943: step 2562, loss 0.000342015, acc 1\n",
      "2018-08-10T17:29:07.369358: step 2563, loss 5.09636e-05, acc 1\n",
      "2018-08-10T17:29:07.510368: step 2564, loss 6.66904e-05, acc 1\n",
      "2018-08-10T17:29:07.650940: step 2565, loss 0.000184037, acc 1\n",
      "2018-08-10T17:29:07.790440: step 2566, loss 0.000191665, acc 1\n",
      "2018-08-10T17:29:07.929142: step 2567, loss 7.80545e-05, acc 1\n",
      "2018-08-10T17:29:08.066031: step 2568, loss 0.000135312, acc 1\n",
      "2018-08-10T17:29:08.204519: step 2569, loss 3.50632e-05, acc 1\n",
      "2018-08-10T17:29:08.342399: step 2570, loss 9.5963e-05, acc 1\n",
      "2018-08-10T17:29:08.481173: step 2571, loss 0.000210076, acc 1\n",
      "2018-08-10T17:29:08.623226: step 2572, loss 0.000258252, acc 1\n",
      "2018-08-10T17:29:08.760885: step 2573, loss 0.000116386, acc 1\n",
      "2018-08-10T17:29:08.895699: step 2574, loss 0.000242291, acc 1\n",
      "2018-08-10T17:29:09.037600: step 2575, loss 0.000210382, acc 1\n",
      "2018-08-10T17:29:09.177585: step 2576, loss 7.94211e-05, acc 1\n",
      "2018-08-10T17:29:09.320893: step 2577, loss 3.97101e-05, acc 1\n",
      "2018-08-10T17:29:09.474837: step 2578, loss 0.000161376, acc 1\n",
      "2018-08-10T17:29:09.623246: step 2579, loss 0.034427, acc 0.984375\n",
      "2018-08-10T17:29:09.769442: step 2580, loss 0.000166352, acc 1\n",
      "2018-08-10T17:29:09.918110: step 2581, loss 0.00143736, acc 1\n",
      "2018-08-10T17:29:10.062747: step 2582, loss 1.65302e-05, acc 1\n",
      "2018-08-10T17:29:10.208127: step 2583, loss 0.000951923, acc 1\n",
      "2018-08-10T17:29:10.354975: step 2584, loss 3.15971e-05, acc 1\n",
      "2018-08-10T17:29:10.502797: step 2585, loss 0.00049522, acc 1\n",
      "2018-08-10T17:29:10.652737: step 2586, loss 0.000554155, acc 1\n",
      "2018-08-10T17:29:10.793900: step 2587, loss 0.000174551, acc 1\n",
      "2018-08-10T17:29:10.941195: step 2588, loss 0.00119524, acc 1\n",
      "2018-08-10T17:29:11.089729: step 2589, loss 0.00113696, acc 1\n",
      "2018-08-10T17:29:11.234732: step 2590, loss 0.0020219, acc 1\n",
      "2018-08-10T17:29:11.379151: step 2591, loss 0.000118299, acc 1\n",
      "2018-08-10T17:29:11.529068: step 2592, loss 0.000669712, acc 1\n",
      "2018-08-10T17:29:11.672919: step 2593, loss 0.000232814, acc 1\n",
      "2018-08-10T17:29:11.818349: step 2594, loss 5.69913e-05, acc 1\n",
      "2018-08-10T17:29:11.964090: step 2595, loss 0.000191381, acc 1\n",
      "2018-08-10T17:29:12.111743: step 2596, loss 0.000322838, acc 1\n",
      "2018-08-10T17:29:12.259642: step 2597, loss 0.000161245, acc 1\n",
      "2018-08-10T17:29:12.407551: step 2598, loss 0.0002273, acc 1\n",
      "2018-08-10T17:29:12.557841: step 2599, loss 0.000202479, acc 1\n",
      "2018-08-10T17:29:12.702770: step 2600, loss 0.000806061, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:29:12.844967: step 2600, loss 1.12272, acc 0.837321\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533921767/checkpoints/model-2600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from Text_CNN import TextCNN\n",
    "\n",
    "# CNN Model\n",
    "# Leveraged Denny Britz template for CNN:\n",
    "# http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\n",
    "\n",
    "# run this to clear tf flags without re-starting kernel\n",
    "\n",
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()\n",
    "    keys_list = [keys for keys in flags_dict]\n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "        \n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS(sys.argv)\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "x_train = np.array(train_x_neural)\n",
    "y_train = np.array(train_y_neural)\n",
    "x_dev = np.array(test_x_neural)\n",
    "y_dev = np.array(test_y_neural)\n",
    "\n",
    "print(\"x_train\", np.shape(x_train))\n",
    "print(\"y_train\", np.shape(y_train))\n",
    "print(\"x_dev\", np.shape(x_dev))\n",
    "print(\"x_dev\", np.shape(y_dev))\n",
    "\n",
    "print(\"Vocabulary Size: {:d}\".format(vocab.size))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "\n",
    "\n",
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]\n",
    "            \n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=2,\n",
    "            vocab_size=vocab.size,\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
