{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os, sys, re, json, time, datetime, shutil, csv, glob\n",
    "import itertools, collections\n",
    "from importlib import reload\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.8\"))\n",
    "\n",
    "from common import utils, vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS_SENTIMENT\n",
      "0    213\n",
      "1    826\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load labeled data set\n",
    "review_sentences = pd.read_csv('./data/Feature_sentences_Prod_Final_2018-07-09_Labeled_1K.csv', encoding='utf-8')\n",
    "# Distribution by Sentiment Class\n",
    "print(review_sentences.groupby('POS_SENTIMENT').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(369888, 8)\n"
     ]
    }
   ],
   "source": [
    "# Load inital extract for join/matching to obtain ratings\n",
    "path =r'..' # use your path\n",
    "allFiles = glob.glob(path + \"/data/RR_Pared_All_Active_2018-07-09_*.csv\")\n",
    "all_reviews = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_, index_col=None, header=0)\n",
    "    list_.append(df)\n",
    "all_reviews = pd.concat(list_)\n",
    "\n",
    "print(all_reviews.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment sentences (word features) with ratings data:\n",
    "\n",
    "#print(review_sentences.head())\n",
    "#print(all_reviews.head())\n",
    "#rating = []\n",
    "for index, row in review_sentences.iterrows():\n",
    "    review_sentences.at[index, 'SENTENCE'] += ' '+str(all_reviews['OVERALL'][all_reviews['TESTIMONIALID'] == row['TESTIMONIAL_ID']].values[0])\n",
    "    review_sentences.at[index, 'SENTENCE'] += ' '+str(all_reviews['ORGANIZATION'][all_reviews['TESTIMONIALID'] == row['TESTIMONIAL_ID']].values[0])\n",
    "    review_sentences.at[index, 'SENTENCE'] += ' '+str(all_reviews['APPEARANCE'][all_reviews['TESTIMONIALID'] == row['TESTIMONIAL_ID']].values[0])\n",
    "    review_sentences.at[index, 'SENTENCE'] += ' '+str(all_reviews['PRICE_VALUE'][all_reviews['TESTIMONIALID'] == row['TESTIMONIAL_ID']].values[0])\n",
    "    review_sentences.at[index, 'SENTENCE'] += ' '+str(all_reviews['DURABILITY'][all_reviews['TESTIMONIALID'] == row['TESTIMONIAL_ID']].values[0])\n",
    "\n",
    "    #rating.append(all_reviews['OVERALL'][all_reviews['TESTIMONIALID'] == row['TESTIMONIAL_ID']])\n",
    "    #rating.append(all_reviews['ORGANIZATION'][all_reviews['TESTIMONIALID'] == row['TESTIMONIAL_ID']])\n",
    "    #rating.append(all_reviews['APPEARANCE'][all_reviews['TESTIMONIALID'] == row['TESTIMONIAL_ID']])\n",
    "    #rating.append(all_reviews['PRICE_VALUE'][all_reviews['TESTIMONIALID'] == row['TESTIMONIAL_ID']])\n",
    "    #rating.append(all_reviews['DURABILITY'][all_reviews['TESTIMONIALID'] == row['TESTIMONIAL_ID']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (830, 4)\n",
      "test shape: (209, 4)\n"
     ]
    }
   ],
   "source": [
    "# Create training and test sets, keeping 80:20 ratio of sentiment class\n",
    "dataset_class_0 = review_sentences[review_sentences['POS_SENTIMENT'] == 0]\n",
    "dataset_class_1 = review_sentences[review_sentences['POS_SENTIMENT'] == 1]\n",
    "\n",
    "#train = 170:660 = 830\n",
    "#test = 43:166 = 209\n",
    "#               == 1,039\n",
    "\n",
    "train = dataset_class_0.iloc[0:170,]\n",
    "train = train.append(dataset_class_1.iloc[0:660,])\n",
    "print('train shape:', train.shape)\n",
    "\n",
    "test = dataset_class_0.iloc[170:213,]\n",
    "test = test.append(dataset_class_1.iloc[660:826,])\n",
    "print('test shape:', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train['SENTENCE']\n",
    "train_y = train['POS_SENTIMENT']\n",
    "test_x = test['SENTENCE']\n",
    "test_y = test['POS_SENTIMENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences\n",
    "\n",
    "train_x_tokens = []\n",
    "test_x_tokens = []\n",
    "\n",
    "for x in train_x:\n",
    "    train_x_tokens.append(word_tokenize(x))\n",
    "    \n",
    "for x in test_x:\n",
    "    test_x_tokens.append(word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1,710\n"
     ]
    }
   ],
   "source": [
    "# define vocabulary using w266 common vocab\n",
    "\n",
    "train_text_all = [item for sublist in train_x_tokens for item in sublist]\n",
    "\n",
    "vocab = vocabulary.Vocabulary(train_text_all, size=20000)\n",
    "print(\"Vocabulary size: {:,}\".format(vocab.size))\n",
    "#print(\"Vocabulary dict: \", vocab.word_to_id)\n",
    "\n",
    "train_x_ids = []\n",
    "test_x_ids = []\n",
    "\n",
    "for x in train_x_tokens:\n",
    "    train_x_ids.append(vocab.words_to_ids(x))\n",
    "    \n",
    "for x in test_x_tokens:\n",
    "    test_x_ids.append(vocab.words_to_ids(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating id lists...\n",
      "starting id lists to sparse bow conversion...\n",
      "training Multinomial Naive Bayes for simple baseline model...\n",
      "Accuracy on test set: 84.21%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.40      0.51        43\n",
      "          1       0.86      0.96      0.91       166\n",
      "\n",
      "avg / total       0.83      0.84      0.82       209\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NB - BoW Baseline\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_x_fdict = []\n",
    "test_x_fdict = []\n",
    "\n",
    "for x in train_x_ids:\n",
    "    train_x_fdict.append(collections.Counter(x))\n",
    "    \n",
    "for x in test_x_ids:\n",
    "    test_x_fdict.append(collections.Counter(x))\n",
    "\n",
    "train_x_vector = []\n",
    "test_x_vector = []\n",
    "\n",
    "num_features = vocab.size\n",
    "print('creating id lists...')\n",
    "for x in train_x_fdict:\n",
    "    train_x_vector.append([x.get(i, 0) for i in range(num_features)])\n",
    "    \n",
    "for x in test_x_fdict:\n",
    "    test_x_vector.append([x.get(i, 0) for i in range(num_features)])\n",
    "\n",
    "# use w266 common utils to convert id lists to sparse bow matrix\n",
    "print('starting id lists to sparse bow conversion...')\n",
    "train_x_sparse_bow = utils.id_lists_to_sparse_bow(train_x_fdict, vocab.size)\n",
    "test_x_sparse_bow = utils.id_lists_to_sparse_bow(test_x_fdict, vocab.size)\n",
    "\n",
    "# training Multinomial Naive Bayes for simple baseline model\n",
    "print('training Multinomial Naive Bayes for simple baseline model...')\n",
    "nb = MultinomialNB()\n",
    "nb.fit(train_x_sparse_bow, train_y)\n",
    "y_pred = nb.predict(test_x_sparse_bow)\n",
    "\n",
    "acc = accuracy_score(test_y, y_pred)\n",
    "print(\"Accuracy on test set: {:.02%}\".format(acc))\n",
    "classrep = classification_report(test_y, y_pred)\n",
    "print(classrep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for CNN model\n",
    "\n",
    "from sklearn import preprocessing\n",
    "# pad all id vectors to max length for Neural model\n",
    "\n",
    "max_comment_length = max([len(x) for x in train_x_ids])\n",
    "\n",
    "train_x_neural = train_x_ids\n",
    "test_x_neural = test_x_ids\n",
    "\n",
    "for x in train_x_neural:\n",
    "    if len(x) < max_comment_length:\n",
    "        for n in range(max_comment_length - len(x)):\n",
    "            x.append(1)\n",
    "    elif len(x) > max_comment_length:\n",
    "        x = x[:max_comment_length]\n",
    "        \n",
    "for x in test_x_neural:\n",
    "    if len(x) < max_comment_length:\n",
    "        for n in range(max_comment_length - len(x)):\n",
    "            x.append(1)\n",
    "    elif len(x) > max_comment_length:\n",
    "        x = x[:max_comment_length]\n",
    "\n",
    "# convert labels to one-hot vectors for Neural model --update your class lables below!!\n",
    "\n",
    "terms_list = [[0],[1]]\n",
    "lb = preprocessing.MultiLabelBinarizer()\n",
    "lb.fit(terms_list)\n",
    "\n",
    "train_labels = np.array(train_y.tolist())\n",
    "test_labels = np.array(test_y.tolist())\n",
    "\n",
    "train_lables_format = [train_labels[i:i+1] for i in range(0, len(train_labels), 1)]    \n",
    "test_lables_format = [test_labels[i:i+1] for i in range(0, len(test_labels), 1)]\n",
    "\n",
    "train_y_neural = lb.transform(train_lables_format)\n",
    "test_y_neural = lb.transform(test_lables_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=<absl.flags._flag.BooleanFlag object at 0x7f8b6d7bebe0>\n",
      "BATCH_SIZE=<absl.flags._flag.Flag object at 0x7f8b6d7bef60>\n",
      "CHECKPOINT_EVERY=<absl.flags._flag.Flag object at 0x7f8b6d7bee80>\n",
      "DROPOUT_KEEP_PROB=<absl.flags._flag.Flag object at 0x7f8b6d538908>\n",
      "EMBEDDING_DIM=<absl.flags._flag.Flag object at 0x7f8b6d80ff28>\n",
      "EVALUATE_EVERY=<absl.flags._flag.Flag object at 0x7f8b6d7bef28>\n",
      "F=<absl.flags._flag.Flag object at 0x7f8b6d7bea20>\n",
      "FILTER_SIZES=<absl.flags._flag.Flag object at 0x7f8b6d778550>\n",
      "L2_REG_LAMBDA=<absl.flags._flag.Flag object at 0x7f8b6d7befd0>\n",
      "LOG_DEVICE_PLACEMENT=<absl.flags._flag.BooleanFlag object at 0x7f8b6d7bed30>\n",
      "NUM_CHECKPOINTS=<absl.flags._flag.Flag object at 0x7f8b6d7bec50>\n",
      "NUM_EPOCHS=<absl.flags._flag.Flag object at 0x7f8b6d7bee48>\n",
      "NUM_FILTERS=<absl.flags._flag.Flag object at 0x7f8b6d7c3160>\n",
      "\n",
      "x_train (830, 76)\n",
      "y_train (830, 2)\n",
      "x_dev (209, 76)\n",
      "x_dev (209, 2)\n",
      "Vocabulary Size: 1710\n",
      "Train/Dev split: 830/209\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533923672\n",
      "\n",
      "2018-08-10T17:54:33.398659: step 1, loss 0.829301, acc 0.734375\n",
      "2018-08-10T17:54:33.563248: step 2, loss 1.31612, acc 0.734375\n",
      "2018-08-10T17:54:33.714202: step 3, loss 2.13099, acc 0.65625\n",
      "2018-08-10T17:54:33.870254: step 4, loss 1.0773, acc 0.796875\n",
      "2018-08-10T17:54:34.003013: step 5, loss 0.813631, acc 0.734375\n",
      "2018-08-10T17:54:34.157582: step 6, loss 1.73478, acc 0.625\n",
      "2018-08-10T17:54:34.296097: step 7, loss 1.13858, acc 0.75\n",
      "2018-08-10T17:54:34.428094: step 8, loss 1.33344, acc 0.671875\n",
      "2018-08-10T17:54:34.582966: step 9, loss 0.983922, acc 0.6875\n",
      "2018-08-10T17:54:34.744249: step 10, loss 1.50048, acc 0.65625\n",
      "2018-08-10T17:54:34.896647: step 11, loss 1.91343, acc 0.640625\n",
      "2018-08-10T17:54:35.054034: step 12, loss 1.21811, acc 0.6875\n",
      "2018-08-10T17:54:35.208095: step 13, loss 1.05384, acc 0.741935\n",
      "2018-08-10T17:54:35.368059: step 14, loss 1.17396, acc 0.6875\n",
      "2018-08-10T17:54:35.531475: step 15, loss 0.942716, acc 0.703125\n",
      "2018-08-10T17:54:35.688042: step 16, loss 0.775068, acc 0.78125\n",
      "2018-08-10T17:54:35.848747: step 17, loss 1.39867, acc 0.609375\n",
      "2018-08-10T17:54:36.007086: step 18, loss 0.615294, acc 0.78125\n",
      "2018-08-10T17:54:36.164704: step 19, loss 0.979126, acc 0.75\n",
      "2018-08-10T17:54:36.322625: step 20, loss 0.451831, acc 0.890625\n",
      "2018-08-10T17:54:36.485474: step 21, loss 0.95369, acc 0.8125\n",
      "2018-08-10T17:54:36.644717: step 22, loss 0.740443, acc 0.8125\n",
      "2018-08-10T17:54:36.806586: step 23, loss 0.785668, acc 0.796875\n",
      "2018-08-10T17:54:36.974955: step 24, loss 0.555885, acc 0.859375\n",
      "2018-08-10T17:54:37.132622: step 25, loss 1.18789, acc 0.65625\n",
      "2018-08-10T17:54:37.286367: step 26, loss 1.21256, acc 0.709677\n",
      "2018-08-10T17:54:37.448333: step 27, loss 0.722097, acc 0.796875\n",
      "2018-08-10T17:54:37.608735: step 28, loss 1.08849, acc 0.796875\n",
      "2018-08-10T17:54:37.770914: step 29, loss 0.943424, acc 0.75\n",
      "2018-08-10T17:54:37.930658: step 30, loss 0.54621, acc 0.734375\n",
      "2018-08-10T17:54:38.090670: step 31, loss 0.528289, acc 0.84375\n",
      "2018-08-10T17:54:38.245826: step 32, loss 0.653197, acc 0.8125\n",
      "2018-08-10T17:54:38.405162: step 33, loss 0.884807, acc 0.796875\n",
      "2018-08-10T17:54:38.564710: step 34, loss 0.554357, acc 0.828125\n",
      "2018-08-10T17:54:38.724221: step 35, loss 1.08429, acc 0.734375\n",
      "2018-08-10T17:54:38.883925: step 36, loss 1.43507, acc 0.65625\n",
      "2018-08-10T17:54:39.041053: step 37, loss 0.837981, acc 0.734375\n",
      "2018-08-10T17:54:39.197912: step 38, loss 0.856981, acc 0.734375\n",
      "2018-08-10T17:54:39.354079: step 39, loss 0.856627, acc 0.758065\n",
      "2018-08-10T17:54:39.513760: step 40, loss 0.858457, acc 0.78125\n",
      "2018-08-10T17:54:39.669681: step 41, loss 0.324765, acc 0.90625\n",
      "2018-08-10T17:54:39.837072: step 42, loss 0.315723, acc 0.875\n",
      "2018-08-10T17:54:39.996942: step 43, loss 0.48087, acc 0.828125\n",
      "2018-08-10T17:54:40.153421: step 44, loss 0.973717, acc 0.78125\n",
      "2018-08-10T17:54:40.308228: step 45, loss 0.568711, acc 0.796875\n",
      "2018-08-10T17:54:40.458130: step 46, loss 0.677672, acc 0.828125\n",
      "2018-08-10T17:54:40.608061: step 47, loss 0.399132, acc 0.84375\n",
      "2018-08-10T17:54:40.757794: step 48, loss 0.932829, acc 0.765625\n",
      "2018-08-10T17:54:40.908162: step 49, loss 0.503188, acc 0.828125\n",
      "2018-08-10T17:54:41.055484: step 50, loss 0.474339, acc 0.875\n",
      "2018-08-10T17:54:41.204725: step 51, loss 0.391183, acc 0.859375\n",
      "2018-08-10T17:54:41.348865: step 52, loss 0.511675, acc 0.822581\n",
      "2018-08-10T17:54:41.497946: step 53, loss 0.490576, acc 0.890625\n",
      "2018-08-10T17:54:41.645830: step 54, loss 0.481761, acc 0.796875\n",
      "2018-08-10T17:54:41.797449: step 55, loss 0.441103, acc 0.828125\n",
      "2018-08-10T17:54:41.945344: step 56, loss 0.346031, acc 0.84375\n",
      "2018-08-10T17:54:42.092956: step 57, loss 0.324722, acc 0.90625\n",
      "2018-08-10T17:54:42.239783: step 58, loss 0.49799, acc 0.859375\n",
      "2018-08-10T17:54:42.388933: step 59, loss 0.765546, acc 0.765625\n",
      "2018-08-10T17:54:42.540371: step 60, loss 0.397147, acc 0.859375\n",
      "2018-08-10T17:54:42.691925: step 61, loss 0.172087, acc 0.921875\n",
      "2018-08-10T17:54:42.841796: step 62, loss 0.42547, acc 0.875\n",
      "2018-08-10T17:54:42.991749: step 63, loss 0.452692, acc 0.859375\n",
      "2018-08-10T17:54:43.139798: step 64, loss 0.507043, acc 0.875\n",
      "2018-08-10T17:54:43.285012: step 65, loss 0.524698, acc 0.822581\n",
      "2018-08-10T17:54:43.433811: step 66, loss 0.322016, acc 0.875\n",
      "2018-08-10T17:54:43.582748: step 67, loss 0.370808, acc 0.875\n",
      "2018-08-10T17:54:43.730068: step 68, loss 0.438887, acc 0.84375\n",
      "2018-08-10T17:54:43.879899: step 69, loss 0.293774, acc 0.875\n",
      "2018-08-10T17:54:44.030521: step 70, loss 0.162961, acc 0.890625\n",
      "2018-08-10T17:54:44.179964: step 71, loss 0.453902, acc 0.859375\n",
      "2018-08-10T17:54:44.327268: step 72, loss 0.413438, acc 0.8125\n",
      "2018-08-10T17:54:44.476919: step 73, loss 0.386076, acc 0.875\n",
      "2018-08-10T17:54:44.630694: step 74, loss 0.648897, acc 0.828125\n",
      "2018-08-10T17:54:44.785519: step 75, loss 0.468843, acc 0.859375\n",
      "2018-08-10T17:54:44.937887: step 76, loss 0.24849, acc 0.890625\n",
      "2018-08-10T17:54:45.097057: step 77, loss 0.515825, acc 0.796875\n",
      "2018-08-10T17:54:45.252614: step 78, loss 0.576743, acc 0.790323\n",
      "2018-08-10T17:54:45.410939: step 79, loss 0.266233, acc 0.90625\n",
      "2018-08-10T17:54:45.567727: step 80, loss 0.175245, acc 0.96875\n",
      "2018-08-10T17:54:45.724000: step 81, loss 0.342666, acc 0.921875\n",
      "2018-08-10T17:54:45.885041: step 82, loss 0.440235, acc 0.859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:54:46.042680: step 83, loss 0.430868, acc 0.859375\n",
      "2018-08-10T17:54:46.197367: step 84, loss 0.409527, acc 0.875\n",
      "2018-08-10T17:54:46.352900: step 85, loss 0.379597, acc 0.84375\n",
      "2018-08-10T17:54:46.513703: step 86, loss 0.228107, acc 0.890625\n",
      "2018-08-10T17:54:46.670874: step 87, loss 0.371715, acc 0.828125\n",
      "2018-08-10T17:54:46.833380: step 88, loss 0.441709, acc 0.859375\n",
      "2018-08-10T17:54:46.990967: step 89, loss 0.383134, acc 0.859375\n",
      "2018-08-10T17:54:47.147132: step 90, loss 0.191923, acc 0.90625\n",
      "2018-08-10T17:54:47.300448: step 91, loss 0.140194, acc 0.935484\n",
      "2018-08-10T17:54:47.458620: step 92, loss 0.194591, acc 0.9375\n",
      "2018-08-10T17:54:47.615166: step 93, loss 0.32854, acc 0.875\n",
      "2018-08-10T17:54:47.777311: step 94, loss 0.267252, acc 0.9375\n",
      "2018-08-10T17:54:47.932297: step 95, loss 0.409309, acc 0.875\n",
      "2018-08-10T17:54:48.088874: step 96, loss 0.260496, acc 0.875\n",
      "2018-08-10T17:54:48.245838: step 97, loss 0.302888, acc 0.859375\n",
      "2018-08-10T17:54:48.401112: step 98, loss 0.353303, acc 0.859375\n",
      "2018-08-10T17:54:48.558675: step 99, loss 0.387047, acc 0.890625\n",
      "2018-08-10T17:54:48.713846: step 100, loss 0.146865, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:54:48.920362: step 100, loss 0.502863, acc 0.827751\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533923672/checkpoints/model-100\n",
      "\n",
      "2018-08-10T17:54:49.144859: step 101, loss 0.202086, acc 0.90625\n",
      "2018-08-10T17:54:49.301365: step 102, loss 0.127024, acc 0.96875\n",
      "2018-08-10T17:54:49.460072: step 103, loss 0.241587, acc 0.890625\n",
      "2018-08-10T17:54:49.613842: step 104, loss 0.0427174, acc 1\n",
      "2018-08-10T17:54:49.771730: step 105, loss 0.24038, acc 0.875\n",
      "2018-08-10T17:54:49.929660: step 106, loss 0.34744, acc 0.90625\n",
      "2018-08-10T17:54:50.086741: step 107, loss 0.197524, acc 0.875\n",
      "2018-08-10T17:54:50.241977: step 108, loss 0.303064, acc 0.890625\n",
      "2018-08-10T17:54:50.400155: step 109, loss 0.281524, acc 0.890625\n",
      "2018-08-10T17:54:50.557095: step 110, loss 0.23336, acc 0.90625\n",
      "2018-08-10T17:54:50.715562: step 111, loss 0.183873, acc 0.921875\n",
      "2018-08-10T17:54:50.874967: step 112, loss 0.287758, acc 0.9375\n",
      "2018-08-10T17:54:51.033918: step 113, loss 0.220148, acc 0.875\n",
      "2018-08-10T17:54:51.189429: step 114, loss 0.180058, acc 0.921875\n",
      "2018-08-10T17:54:51.345577: step 115, loss 0.151431, acc 0.90625\n",
      "2018-08-10T17:54:51.503230: step 116, loss 0.434906, acc 0.8125\n",
      "2018-08-10T17:54:51.657420: step 117, loss 0.239169, acc 0.935484\n",
      "2018-08-10T17:54:51.815670: step 118, loss 0.0971881, acc 0.96875\n",
      "2018-08-10T17:54:51.974411: step 119, loss 0.247867, acc 0.9375\n",
      "2018-08-10T17:54:52.129555: step 120, loss 0.10358, acc 0.9375\n",
      "2018-08-10T17:54:52.283355: step 121, loss 0.128867, acc 0.953125\n",
      "2018-08-10T17:54:52.441000: step 122, loss 0.204362, acc 0.9375\n",
      "2018-08-10T17:54:52.597780: step 123, loss 0.135226, acc 0.96875\n",
      "2018-08-10T17:54:52.756157: step 124, loss 0.272037, acc 0.921875\n",
      "2018-08-10T17:54:52.912315: step 125, loss 0.411881, acc 0.890625\n",
      "2018-08-10T17:54:53.066866: step 126, loss 0.224798, acc 0.9375\n",
      "2018-08-10T17:54:53.222649: step 127, loss 0.200534, acc 0.890625\n",
      "2018-08-10T17:54:53.379721: step 128, loss 0.254312, acc 0.875\n",
      "2018-08-10T17:54:53.533763: step 129, loss 0.1835, acc 0.921875\n",
      "2018-08-10T17:54:53.686482: step 130, loss 0.128467, acc 0.951613\n",
      "2018-08-10T17:54:53.844015: step 131, loss 0.211793, acc 0.90625\n",
      "2018-08-10T17:54:53.998572: step 132, loss 0.128226, acc 0.953125\n",
      "2018-08-10T17:54:54.155177: step 133, loss 0.193857, acc 0.921875\n",
      "2018-08-10T17:54:54.308320: step 134, loss 0.295707, acc 0.953125\n",
      "2018-08-10T17:54:54.467230: step 135, loss 0.145173, acc 0.921875\n",
      "2018-08-10T17:54:54.622532: step 136, loss 0.187275, acc 0.953125\n",
      "2018-08-10T17:54:54.780341: step 137, loss 0.133787, acc 0.953125\n",
      "2018-08-10T17:54:54.936913: step 138, loss 0.0685171, acc 0.96875\n",
      "2018-08-10T17:54:55.089840: step 139, loss 0.324912, acc 0.921875\n",
      "2018-08-10T17:54:55.237029: step 140, loss 0.147255, acc 0.953125\n",
      "2018-08-10T17:54:55.383351: step 141, loss 0.0637173, acc 0.96875\n",
      "2018-08-10T17:54:55.530416: step 142, loss 0.0808848, acc 0.984375\n",
      "2018-08-10T17:54:55.680397: step 143, loss 0.16739, acc 0.903226\n",
      "2018-08-10T17:54:55.839569: step 144, loss 0.15364, acc 0.9375\n",
      "2018-08-10T17:54:55.996441: step 145, loss 0.168308, acc 0.921875\n",
      "2018-08-10T17:54:56.154185: step 146, loss 0.199071, acc 0.921875\n",
      "2018-08-10T17:54:56.310659: step 147, loss 0.236109, acc 0.9375\n",
      "2018-08-10T17:54:56.471304: step 148, loss 0.221881, acc 0.90625\n",
      "2018-08-10T17:54:56.630840: step 149, loss 0.106887, acc 0.96875\n",
      "2018-08-10T17:54:56.788955: step 150, loss 0.147959, acc 0.921875\n",
      "2018-08-10T17:54:56.946528: step 151, loss 0.10073, acc 0.953125\n",
      "2018-08-10T17:54:57.100640: step 152, loss 0.167539, acc 0.953125\n",
      "2018-08-10T17:54:57.257008: step 153, loss 0.0927303, acc 0.96875\n",
      "2018-08-10T17:54:57.412417: step 154, loss 0.230198, acc 0.890625\n",
      "2018-08-10T17:54:57.568986: step 155, loss 0.0971804, acc 0.9375\n",
      "2018-08-10T17:54:57.722438: step 156, loss 0.110483, acc 0.951613\n",
      "2018-08-10T17:54:57.882757: step 157, loss 0.143371, acc 0.9375\n",
      "2018-08-10T17:54:58.041143: step 158, loss 0.136656, acc 0.953125\n",
      "2018-08-10T17:54:58.198681: step 159, loss 0.129454, acc 0.9375\n",
      "2018-08-10T17:54:58.354877: step 160, loss 0.228041, acc 0.921875\n",
      "2018-08-10T17:54:58.510429: step 161, loss 0.146895, acc 0.9375\n",
      "2018-08-10T17:54:58.666870: step 162, loss 0.173399, acc 0.921875\n",
      "2018-08-10T17:54:58.825410: step 163, loss 0.124352, acc 0.953125\n",
      "2018-08-10T17:54:58.980755: step 164, loss 0.311887, acc 0.890625\n",
      "2018-08-10T17:54:59.137629: step 165, loss 0.0649739, acc 0.96875\n",
      "2018-08-10T17:54:59.292298: step 166, loss 0.0726531, acc 0.96875\n",
      "2018-08-10T17:54:59.448310: step 167, loss 0.161987, acc 0.9375\n",
      "2018-08-10T17:54:59.604428: step 168, loss 0.35706, acc 0.90625\n",
      "2018-08-10T17:54:59.758058: step 169, loss 0.17332, acc 0.967742\n",
      "2018-08-10T17:54:59.917256: step 170, loss 0.0725856, acc 0.96875\n",
      "2018-08-10T17:55:00.071476: step 171, loss 0.121632, acc 0.953125\n",
      "2018-08-10T17:55:00.230836: step 172, loss 0.050179, acc 0.96875\n",
      "2018-08-10T17:55:00.387728: step 173, loss 0.0833009, acc 0.953125\n",
      "2018-08-10T17:55:00.549185: step 174, loss 0.0525079, acc 0.984375\n",
      "2018-08-10T17:55:00.706971: step 175, loss 0.174336, acc 0.9375\n",
      "2018-08-10T17:55:00.865617: step 176, loss 0.0579273, acc 0.96875\n",
      "2018-08-10T17:55:01.020638: step 177, loss 0.136766, acc 0.9375\n",
      "2018-08-10T17:55:01.172971: step 178, loss 0.0813032, acc 0.96875\n",
      "2018-08-10T17:55:01.323051: step 179, loss 0.0753099, acc 0.96875\n",
      "2018-08-10T17:55:01.473101: step 180, loss 0.0737191, acc 0.96875\n",
      "2018-08-10T17:55:01.620587: step 181, loss 0.162276, acc 0.96875\n",
      "2018-08-10T17:55:01.766082: step 182, loss 0.147703, acc 0.951613\n",
      "2018-08-10T17:55:01.916563: step 183, loss 0.0506397, acc 0.96875\n",
      "2018-08-10T17:55:02.062837: step 184, loss 0.111265, acc 0.953125\n",
      "2018-08-10T17:55:02.209161: step 185, loss 0.029075, acc 1\n",
      "2018-08-10T17:55:02.355611: step 186, loss 0.0430757, acc 0.984375\n",
      "2018-08-10T17:55:02.507395: step 187, loss 0.105369, acc 0.96875\n",
      "2018-08-10T17:55:02.657116: step 188, loss 0.162663, acc 0.90625\n",
      "2018-08-10T17:55:02.807462: step 189, loss 0.185045, acc 0.921875\n",
      "2018-08-10T17:55:02.954962: step 190, loss 0.0586736, acc 0.96875\n",
      "2018-08-10T17:55:03.105630: step 191, loss 0.0539692, acc 0.96875\n",
      "2018-08-10T17:55:03.252122: step 192, loss 0.138024, acc 0.921875\n",
      "2018-08-10T17:55:03.402322: step 193, loss 0.0975788, acc 0.96875\n",
      "2018-08-10T17:55:03.552619: step 194, loss 0.0839782, acc 0.953125\n",
      "2018-08-10T17:55:03.698086: step 195, loss 0.0915544, acc 0.951613\n",
      "2018-08-10T17:55:03.847531: step 196, loss 0.0394187, acc 0.96875\n",
      "2018-08-10T17:55:03.996580: step 197, loss 0.153516, acc 0.9375\n",
      "2018-08-10T17:55:04.143955: step 198, loss 0.116936, acc 0.953125\n",
      "2018-08-10T17:55:04.292102: step 199, loss 0.0961017, acc 0.953125\n",
      "2018-08-10T17:55:04.441882: step 200, loss 0.117148, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:55:04.594992: step 200, loss 0.539833, acc 0.856459\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533923672/checkpoints/model-200\n",
      "\n",
      "2018-08-10T17:55:04.794418: step 201, loss 0.105637, acc 0.984375\n",
      "2018-08-10T17:55:04.942886: step 202, loss 0.159418, acc 0.953125\n",
      "2018-08-10T17:55:05.091367: step 203, loss 0.0436028, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:55:05.239758: step 204, loss 0.0741588, acc 0.984375\n",
      "2018-08-10T17:55:05.386124: step 205, loss 0.0621876, acc 0.96875\n",
      "2018-08-10T17:55:05.537977: step 206, loss 0.129883, acc 0.953125\n",
      "2018-08-10T17:55:05.693117: step 207, loss 0.0372406, acc 0.984375\n",
      "2018-08-10T17:55:05.847780: step 208, loss 0.120228, acc 0.951613\n",
      "2018-08-10T17:55:06.004748: step 209, loss 0.112203, acc 0.9375\n",
      "2018-08-10T17:55:06.161991: step 210, loss 0.0570202, acc 0.984375\n",
      "2018-08-10T17:55:06.319946: step 211, loss 0.125462, acc 0.9375\n",
      "2018-08-10T17:55:06.481631: step 212, loss 0.0483198, acc 0.984375\n",
      "2018-08-10T17:55:06.638719: step 213, loss 0.057819, acc 0.984375\n",
      "2018-08-10T17:55:06.798293: step 214, loss 0.0325004, acc 0.984375\n",
      "2018-08-10T17:55:06.955006: step 215, loss 0.104745, acc 0.953125\n",
      "2018-08-10T17:55:07.112197: step 216, loss 0.0248563, acc 1\n",
      "2018-08-10T17:55:07.268763: step 217, loss 0.0223161, acc 1\n",
      "2018-08-10T17:55:07.424354: step 218, loss 0.0677887, acc 0.953125\n",
      "2018-08-10T17:55:07.578843: step 219, loss 0.112593, acc 0.9375\n",
      "2018-08-10T17:55:07.738271: step 220, loss 0.22377, acc 0.96875\n",
      "2018-08-10T17:55:07.894058: step 221, loss 0.0922209, acc 0.951613\n",
      "2018-08-10T17:55:08.050582: step 222, loss 0.0918495, acc 0.953125\n",
      "2018-08-10T17:55:08.210286: step 223, loss 0.029439, acc 0.984375\n",
      "2018-08-10T17:55:08.369274: step 224, loss 0.0400885, acc 0.984375\n",
      "2018-08-10T17:55:08.527481: step 225, loss 0.0262873, acc 1\n",
      "2018-08-10T17:55:08.686603: step 226, loss 0.200921, acc 0.953125\n",
      "2018-08-10T17:55:08.847130: step 227, loss 0.0698837, acc 0.96875\n",
      "2018-08-10T17:55:09.004721: step 228, loss 0.0727511, acc 0.984375\n",
      "2018-08-10T17:55:09.162095: step 229, loss 0.036532, acc 0.984375\n",
      "2018-08-10T17:55:09.319857: step 230, loss 0.0465406, acc 0.96875\n",
      "2018-08-10T17:55:09.477283: step 231, loss 0.0961187, acc 0.96875\n",
      "2018-08-10T17:55:09.637698: step 232, loss 0.079842, acc 0.953125\n",
      "2018-08-10T17:55:09.799561: step 233, loss 0.0334713, acc 0.984375\n",
      "2018-08-10T17:55:09.952883: step 234, loss 0.15727, acc 0.935484\n",
      "2018-08-10T17:55:10.111699: step 235, loss 0.0925163, acc 0.9375\n",
      "2018-08-10T17:55:10.270808: step 236, loss 0.0307656, acc 0.984375\n",
      "2018-08-10T17:55:10.428072: step 237, loss 0.0910879, acc 0.984375\n",
      "2018-08-10T17:55:10.589022: step 238, loss 0.158539, acc 0.921875\n",
      "2018-08-10T17:55:10.749157: step 239, loss 0.0549395, acc 0.984375\n",
      "2018-08-10T17:55:10.909484: step 240, loss 0.0714721, acc 0.96875\n",
      "2018-08-10T17:55:11.067069: step 241, loss 0.0643641, acc 0.96875\n",
      "2018-08-10T17:55:11.225223: step 242, loss 0.0520831, acc 0.96875\n",
      "2018-08-10T17:55:11.386226: step 243, loss 0.130531, acc 0.9375\n",
      "2018-08-10T17:55:11.544131: step 244, loss 0.00919571, acc 1\n",
      "2018-08-10T17:55:11.702756: step 245, loss 0.113414, acc 0.953125\n",
      "2018-08-10T17:55:11.865110: step 246, loss 0.0307899, acc 1\n",
      "2018-08-10T17:55:12.022639: step 247, loss 0.129027, acc 0.951613\n",
      "2018-08-10T17:55:12.180936: step 248, loss 0.0283997, acc 0.984375\n",
      "2018-08-10T17:55:12.337662: step 249, loss 0.0737729, acc 0.984375\n",
      "2018-08-10T17:55:12.499454: step 250, loss 0.149385, acc 0.953125\n",
      "2018-08-10T17:55:12.660794: step 251, loss 0.136313, acc 0.953125\n",
      "2018-08-10T17:55:12.825732: step 252, loss 0.109576, acc 0.953125\n",
      "2018-08-10T17:55:12.982568: step 253, loss 0.0679849, acc 0.953125\n",
      "2018-08-10T17:55:13.144634: step 254, loss 0.0334831, acc 1\n",
      "2018-08-10T17:55:13.304678: step 255, loss 0.0248215, acc 1\n",
      "2018-08-10T17:55:13.463468: step 256, loss 0.0289617, acc 0.984375\n",
      "2018-08-10T17:55:13.621041: step 257, loss 0.05476, acc 0.984375\n",
      "2018-08-10T17:55:13.783145: step 258, loss 0.0473747, acc 0.96875\n",
      "2018-08-10T17:55:13.940987: step 259, loss 0.123355, acc 0.96875\n",
      "2018-08-10T17:55:14.097892: step 260, loss 0.129818, acc 0.951613\n",
      "2018-08-10T17:55:14.255861: step 261, loss 0.0491529, acc 0.96875\n",
      "2018-08-10T17:55:14.413848: step 262, loss 0.0427532, acc 0.984375\n",
      "2018-08-10T17:55:14.575985: step 263, loss 0.0495587, acc 0.984375\n",
      "2018-08-10T17:55:14.738648: step 264, loss 0.0119608, acc 1\n",
      "2018-08-10T17:55:14.901059: step 265, loss 0.115293, acc 0.953125\n",
      "2018-08-10T17:55:15.062567: step 266, loss 0.0569371, acc 0.96875\n",
      "2018-08-10T17:55:15.219716: step 267, loss 0.0615478, acc 0.96875\n",
      "2018-08-10T17:55:15.379819: step 268, loss 0.0524298, acc 0.96875\n",
      "2018-08-10T17:55:15.536545: step 269, loss 0.0232, acc 1\n",
      "2018-08-10T17:55:15.696923: step 270, loss 0.0259828, acc 0.984375\n",
      "2018-08-10T17:55:15.857103: step 271, loss 0.0575107, acc 0.96875\n",
      "2018-08-10T17:55:16.013469: step 272, loss 0.0272416, acc 0.984375\n",
      "2018-08-10T17:55:16.158539: step 273, loss 0.0864113, acc 0.967742\n",
      "2018-08-10T17:55:16.308418: step 274, loss 0.069893, acc 0.96875\n",
      "2018-08-10T17:55:16.462035: step 275, loss 0.154595, acc 0.9375\n",
      "2018-08-10T17:55:16.617227: step 276, loss 0.0263055, acc 0.984375\n",
      "2018-08-10T17:55:16.776931: step 277, loss 0.0920145, acc 0.96875\n",
      "2018-08-10T17:55:16.933417: step 278, loss 0.0387267, acc 0.96875\n",
      "2018-08-10T17:55:17.089788: step 279, loss 0.0156297, acc 1\n",
      "2018-08-10T17:55:17.244931: step 280, loss 0.0815503, acc 0.984375\n",
      "2018-08-10T17:55:17.404074: step 281, loss 0.111723, acc 0.96875\n",
      "2018-08-10T17:55:17.561119: step 282, loss 0.0097062, acc 1\n",
      "2018-08-10T17:55:17.719433: step 283, loss 0.0322859, acc 1\n",
      "2018-08-10T17:55:17.878120: step 284, loss 0.015309, acc 1\n",
      "2018-08-10T17:55:18.036541: step 285, loss 0.0096018, acc 1\n",
      "2018-08-10T17:55:18.190179: step 286, loss 0.11112, acc 0.983871\n",
      "2018-08-10T17:55:18.349486: step 287, loss 0.0218953, acc 1\n",
      "2018-08-10T17:55:18.505963: step 288, loss 0.0576782, acc 0.96875\n",
      "2018-08-10T17:55:18.661315: step 289, loss 0.0481574, acc 0.984375\n",
      "2018-08-10T17:55:18.821004: step 290, loss 0.0436991, acc 0.984375\n",
      "2018-08-10T17:55:18.976730: step 291, loss 0.0645114, acc 0.984375\n",
      "2018-08-10T17:55:19.135613: step 292, loss 0.0639715, acc 0.96875\n",
      "2018-08-10T17:55:19.291971: step 293, loss 0.00544833, acc 1\n",
      "2018-08-10T17:55:19.448863: step 294, loss 0.0581669, acc 0.96875\n",
      "2018-08-10T17:55:19.604892: step 295, loss 0.00939871, acc 1\n",
      "2018-08-10T17:55:19.763651: step 296, loss 0.111287, acc 0.984375\n",
      "2018-08-10T17:55:19.920563: step 297, loss 0.0358032, acc 0.984375\n",
      "2018-08-10T17:55:20.080992: step 298, loss 0.0140537, acc 1\n",
      "2018-08-10T17:55:20.233056: step 299, loss 0.134631, acc 0.983871\n",
      "2018-08-10T17:55:20.389315: step 300, loss 0.0276355, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:55:20.543407: step 300, loss 0.507242, acc 0.870813\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533923672/checkpoints/model-300\n",
      "\n",
      "2018-08-10T17:55:20.761756: step 301, loss 0.0503938, acc 1\n",
      "2018-08-10T17:55:20.919636: step 302, loss 0.11958, acc 0.984375\n",
      "2018-08-10T17:55:21.078773: step 303, loss 0.0574236, acc 0.96875\n",
      "2018-08-10T17:55:21.235839: step 304, loss 0.0131423, acc 1\n",
      "2018-08-10T17:55:21.392573: step 305, loss 0.0701258, acc 0.984375\n",
      "2018-08-10T17:55:21.551864: step 306, loss 0.0341889, acc 0.96875\n",
      "2018-08-10T17:55:21.706674: step 307, loss 0.0774457, acc 0.984375\n",
      "2018-08-10T17:55:21.866740: step 308, loss 0.0426491, acc 0.984375\n",
      "2018-08-10T17:55:22.022705: step 309, loss 0.0107432, acc 1\n",
      "2018-08-10T17:55:22.174151: step 310, loss 0.0218304, acc 1\n",
      "2018-08-10T17:55:22.324363: step 311, loss 0.0424162, acc 1\n",
      "2018-08-10T17:55:22.470356: step 312, loss 0.0412748, acc 0.983871\n",
      "2018-08-10T17:55:22.617420: step 313, loss 0.0483233, acc 0.984375\n",
      "2018-08-10T17:55:22.768116: step 314, loss 0.0452852, acc 0.984375\n",
      "2018-08-10T17:55:22.917740: step 315, loss 0.017144, acc 1\n",
      "2018-08-10T17:55:23.068340: step 316, loss 0.0368512, acc 0.96875\n",
      "2018-08-10T17:55:23.216208: step 317, loss 0.0241067, acc 1\n",
      "2018-08-10T17:55:23.365384: step 318, loss 0.0715614, acc 0.96875\n",
      "2018-08-10T17:55:23.515615: step 319, loss 0.0222703, acc 1\n",
      "2018-08-10T17:55:23.664432: step 320, loss 0.0218245, acc 1\n",
      "2018-08-10T17:55:23.812990: step 321, loss 0.0950642, acc 0.984375\n",
      "2018-08-10T17:55:23.961303: step 322, loss 0.111309, acc 0.9375\n",
      "2018-08-10T17:55:24.110826: step 323, loss 0.0761851, acc 0.984375\n",
      "2018-08-10T17:55:24.260034: step 324, loss 0.0455936, acc 0.96875\n",
      "2018-08-10T17:55:24.403534: step 325, loss 0.078991, acc 0.967742\n",
      "2018-08-10T17:55:24.555742: step 326, loss 0.0269445, acc 1\n",
      "2018-08-10T17:55:24.706067: step 327, loss 0.0177379, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:55:24.855311: step 328, loss 0.00290821, acc 1\n",
      "2018-08-10T17:55:25.003984: step 329, loss 0.0207451, acc 1\n",
      "2018-08-10T17:55:25.153744: step 330, loss 0.0576305, acc 0.96875\n",
      "2018-08-10T17:55:25.304686: step 331, loss 0.0140056, acc 1\n",
      "2018-08-10T17:55:25.452585: step 332, loss 0.00890186, acc 1\n",
      "2018-08-10T17:55:25.601261: step 333, loss 0.136605, acc 0.953125\n",
      "2018-08-10T17:55:25.748183: step 334, loss 0.0100075, acc 1\n",
      "2018-08-10T17:55:25.897696: step 335, loss 0.047136, acc 0.984375\n",
      "2018-08-10T17:55:26.047740: step 336, loss 0.0122091, acc 1\n",
      "2018-08-10T17:55:26.194498: step 337, loss 0.152638, acc 0.96875\n",
      "2018-08-10T17:55:26.337319: step 338, loss 0.0251671, acc 1\n",
      "2018-08-10T17:55:26.489892: step 339, loss 0.00815219, acc 1\n",
      "2018-08-10T17:55:26.646407: step 340, loss 0.0107403, acc 1\n",
      "2018-08-10T17:55:26.803221: step 341, loss 0.0085333, acc 1\n",
      "2018-08-10T17:55:26.961214: step 342, loss 0.0642681, acc 0.953125\n",
      "2018-08-10T17:55:27.116581: step 343, loss 0.0149521, acc 1\n",
      "2018-08-10T17:55:27.274762: step 344, loss 0.00805511, acc 1\n",
      "2018-08-10T17:55:27.432197: step 345, loss 0.049672, acc 0.984375\n",
      "2018-08-10T17:55:27.591642: step 346, loss 0.0235754, acc 0.984375\n",
      "2018-08-10T17:55:27.746487: step 347, loss 0.0361433, acc 0.984375\n",
      "2018-08-10T17:55:27.905791: step 348, loss 0.00850246, acc 1\n",
      "2018-08-10T17:55:28.061234: step 349, loss 0.021742, acc 0.984375\n",
      "2018-08-10T17:55:28.218849: step 350, loss 0.00778849, acc 1\n",
      "2018-08-10T17:55:28.372615: step 351, loss 0.0406548, acc 0.983871\n",
      "2018-08-10T17:55:28.533613: step 352, loss 0.0730376, acc 0.984375\n",
      "2018-08-10T17:55:28.688979: step 353, loss 0.00927486, acc 1\n",
      "2018-08-10T17:55:28.845919: step 354, loss 0.0894163, acc 0.984375\n",
      "2018-08-10T17:55:29.008563: step 355, loss 0.0147464, acc 1\n",
      "2018-08-10T17:55:29.164448: step 356, loss 0.0804427, acc 0.96875\n",
      "2018-08-10T17:55:29.320165: step 357, loss 0.0896706, acc 0.953125\n",
      "2018-08-10T17:55:29.477815: step 358, loss 0.00984194, acc 1\n",
      "2018-08-10T17:55:29.637874: step 359, loss 0.0327074, acc 0.984375\n",
      "2018-08-10T17:55:29.794553: step 360, loss 0.0104641, acc 1\n",
      "2018-08-10T17:55:29.954089: step 361, loss 0.0114097, acc 1\n",
      "2018-08-10T17:55:30.113917: step 362, loss 0.0620484, acc 0.96875\n",
      "2018-08-10T17:55:30.269163: step 363, loss 0.084239, acc 0.96875\n",
      "2018-08-10T17:55:30.422292: step 364, loss 0.0485616, acc 0.983871\n",
      "2018-08-10T17:55:30.581955: step 365, loss 0.0083964, acc 1\n",
      "2018-08-10T17:55:30.738457: step 366, loss 0.031542, acc 0.984375\n",
      "2018-08-10T17:55:30.896808: step 367, loss 0.0334493, acc 0.984375\n",
      "2018-08-10T17:55:31.052713: step 368, loss 0.0142688, acc 0.984375\n",
      "2018-08-10T17:55:31.205534: step 369, loss 0.00997402, acc 1\n",
      "2018-08-10T17:55:31.364221: step 370, loss 0.0020896, acc 1\n",
      "2018-08-10T17:55:31.523593: step 371, loss 0.0429106, acc 0.984375\n",
      "2018-08-10T17:55:31.677046: step 372, loss 0.0511734, acc 0.96875\n",
      "2018-08-10T17:55:31.835024: step 373, loss 0.0194338, acc 1\n",
      "2018-08-10T17:55:31.991881: step 374, loss 0.0179032, acc 1\n",
      "2018-08-10T17:55:32.149182: step 375, loss 0.0401514, acc 0.984375\n",
      "2018-08-10T17:55:32.304589: step 376, loss 0.021075, acc 0.984375\n",
      "2018-08-10T17:55:32.457724: step 377, loss 0.0115255, acc 1\n",
      "2018-08-10T17:55:32.615784: step 378, loss 0.00937865, acc 1\n",
      "2018-08-10T17:55:32.773757: step 379, loss 0.0812596, acc 0.953125\n",
      "2018-08-10T17:55:32.932286: step 380, loss 0.0141965, acc 1\n",
      "2018-08-10T17:55:33.087988: step 381, loss 0.00828172, acc 1\n",
      "2018-08-10T17:55:33.242119: step 382, loss 0.0503251, acc 0.984375\n",
      "2018-08-10T17:55:33.399109: step 383, loss 0.0300109, acc 0.984375\n",
      "2018-08-10T17:55:33.555133: step 384, loss 0.0337097, acc 0.96875\n",
      "2018-08-10T17:55:33.709799: step 385, loss 0.0321875, acc 0.984375\n",
      "2018-08-10T17:55:33.867452: step 386, loss 0.011993, acc 1\n",
      "2018-08-10T17:55:34.020832: step 387, loss 0.0213179, acc 0.984375\n",
      "2018-08-10T17:55:34.176120: step 388, loss 0.0324775, acc 1\n",
      "2018-08-10T17:55:34.330683: step 389, loss 0.0452775, acc 0.984375\n",
      "2018-08-10T17:55:34.482981: step 390, loss 0.00845997, acc 1\n",
      "2018-08-10T17:55:34.640480: step 391, loss 0.0430181, acc 0.984375\n",
      "2018-08-10T17:55:34.799065: step 392, loss 0.00603534, acc 1\n",
      "2018-08-10T17:55:34.955835: step 393, loss 0.0631187, acc 0.984375\n",
      "2018-08-10T17:55:35.110631: step 394, loss 0.0100183, acc 1\n",
      "2018-08-10T17:55:35.264006: step 395, loss 0.0336338, acc 0.984375\n",
      "2018-08-10T17:55:35.419895: step 396, loss 0.0315519, acc 0.984375\n",
      "2018-08-10T17:55:35.575810: step 397, loss 0.0285999, acc 0.984375\n",
      "2018-08-10T17:55:35.732669: step 398, loss 0.00331466, acc 1\n",
      "2018-08-10T17:55:35.894001: step 399, loss 0.0181519, acc 0.984375\n",
      "2018-08-10T17:55:36.052250: step 400, loss 0.0380591, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:55:36.202433: step 400, loss 0.632138, acc 0.856459\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533923672/checkpoints/model-400\n",
      "\n",
      "2018-08-10T17:55:36.414259: step 401, loss 0.00522311, acc 1\n",
      "2018-08-10T17:55:36.571980: step 402, loss 0.00467803, acc 1\n",
      "2018-08-10T17:55:36.726219: step 403, loss 0.00933623, acc 1\n",
      "2018-08-10T17:55:36.884444: step 404, loss 0.00342225, acc 1\n",
      "2018-08-10T17:55:37.035273: step 405, loss 0.0329368, acc 0.984375\n",
      "2018-08-10T17:55:37.182695: step 406, loss 0.0141172, acc 1\n",
      "2018-08-10T17:55:37.331327: step 407, loss 0.106496, acc 0.96875\n",
      "2018-08-10T17:55:37.481229: step 408, loss 0.005066, acc 1\n",
      "2018-08-10T17:55:37.637265: step 409, loss 0.00809985, acc 1\n",
      "2018-08-10T17:55:37.793986: step 410, loss 0.00602011, acc 1\n",
      "2018-08-10T17:55:37.949268: step 411, loss 0.0508508, acc 0.96875\n",
      "2018-08-10T17:55:38.103087: step 412, loss 0.0154527, acc 0.984375\n",
      "2018-08-10T17:55:38.260623: step 413, loss 0.0171394, acc 0.984375\n",
      "2018-08-10T17:55:38.417343: step 414, loss 0.0895504, acc 0.96875\n",
      "2018-08-10T17:55:38.576311: step 415, loss 0.0359947, acc 0.984375\n",
      "2018-08-10T17:55:38.740429: step 416, loss 0.0162035, acc 1\n",
      "2018-08-10T17:55:38.899869: step 417, loss 0.025672, acc 0.984375\n",
      "2018-08-10T17:55:39.055303: step 418, loss 0.0296613, acc 0.984375\n",
      "2018-08-10T17:55:39.210903: step 419, loss 0.00964848, acc 1\n",
      "2018-08-10T17:55:39.369033: step 420, loss 0.00491463, acc 1\n",
      "2018-08-10T17:55:39.526661: step 421, loss 0.0437465, acc 0.96875\n",
      "2018-08-10T17:55:39.682651: step 422, loss 0.0205787, acc 1\n",
      "2018-08-10T17:55:39.844200: step 423, loss 0.0495035, acc 0.984375\n",
      "2018-08-10T17:55:39.999342: step 424, loss 0.0159258, acc 0.984375\n",
      "2018-08-10T17:55:40.154182: step 425, loss 0.00474207, acc 1\n",
      "2018-08-10T17:55:40.307306: step 426, loss 0.0191335, acc 1\n",
      "2018-08-10T17:55:40.467635: step 427, loss 0.0379899, acc 0.984375\n",
      "2018-08-10T17:55:40.622078: step 428, loss 0.0301933, acc 0.984375\n",
      "2018-08-10T17:55:40.776613: step 429, loss 0.026834, acc 0.983871\n",
      "2018-08-10T17:55:40.936303: step 430, loss 0.0271867, acc 0.984375\n",
      "2018-08-10T17:55:41.092569: step 431, loss 0.0877853, acc 0.96875\n",
      "2018-08-10T17:55:41.250256: step 432, loss 0.0144262, acc 1\n",
      "2018-08-10T17:55:41.405277: step 433, loss 0.00988597, acc 1\n",
      "2018-08-10T17:55:41.559308: step 434, loss 0.00625982, acc 1\n",
      "2018-08-10T17:55:41.713394: step 435, loss 0.0846245, acc 0.96875\n",
      "2018-08-10T17:55:41.873782: step 436, loss 0.00654519, acc 1\n",
      "2018-08-10T17:55:42.029932: step 437, loss 0.0537585, acc 0.984375\n",
      "2018-08-10T17:55:42.185717: step 438, loss 0.00289294, acc 1\n",
      "2018-08-10T17:55:42.340403: step 439, loss 0.00640639, acc 1\n",
      "2018-08-10T17:55:42.495488: step 440, loss 0.0238328, acc 1\n",
      "2018-08-10T17:55:42.652437: step 441, loss 0.0337138, acc 0.984375\n",
      "2018-08-10T17:55:42.806252: step 442, loss 0.00477094, acc 1\n",
      "2018-08-10T17:55:42.965158: step 443, loss 0.0646082, acc 0.96875\n",
      "2018-08-10T17:55:43.118038: step 444, loss 0.0287114, acc 0.984375\n",
      "2018-08-10T17:55:43.267173: step 445, loss 0.00954859, acc 1\n",
      "2018-08-10T17:55:43.414867: step 446, loss 0.00676443, acc 1\n",
      "2018-08-10T17:55:43.561018: step 447, loss 0.0627292, acc 0.984375\n",
      "2018-08-10T17:55:43.708960: step 448, loss 0.0198438, acc 0.984375\n",
      "2018-08-10T17:55:43.857394: step 449, loss 0.00766157, acc 1\n",
      "2018-08-10T17:55:44.004340: step 450, loss 0.00345227, acc 1\n",
      "2018-08-10T17:55:44.152300: step 451, loss 0.0268205, acc 0.984375\n",
      "2018-08-10T17:55:44.297081: step 452, loss 0.0046495, acc 1\n",
      "2018-08-10T17:55:44.451048: step 453, loss 0.00878627, acc 1\n",
      "2018-08-10T17:55:44.601086: step 454, loss 0.00433012, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:55:44.744559: step 455, loss 0.0139315, acc 1\n",
      "2018-08-10T17:55:44.895764: step 456, loss 0.00868863, acc 1\n",
      "2018-08-10T17:55:45.045191: step 457, loss 0.00893528, acc 1\n",
      "2018-08-10T17:55:45.192314: step 458, loss 0.0396543, acc 0.984375\n",
      "2018-08-10T17:55:45.342885: step 459, loss 0.00856131, acc 1\n",
      "2018-08-10T17:55:45.491999: step 460, loss 0.0153585, acc 1\n",
      "2018-08-10T17:55:45.639632: step 461, loss 0.012088, acc 1\n",
      "2018-08-10T17:55:45.787273: step 462, loss 0.00716194, acc 1\n",
      "2018-08-10T17:55:45.935571: step 463, loss 0.00774224, acc 1\n",
      "2018-08-10T17:55:46.084521: step 464, loss 0.00247498, acc 1\n",
      "2018-08-10T17:55:46.231151: step 465, loss 0.0163791, acc 1\n",
      "2018-08-10T17:55:46.378046: step 466, loss 0.00220665, acc 1\n",
      "2018-08-10T17:55:46.527725: step 467, loss 0.0127984, acc 1\n",
      "2018-08-10T17:55:46.671629: step 468, loss 0.00602419, acc 1\n",
      "2018-08-10T17:55:46.821739: step 469, loss 0.00541734, acc 1\n",
      "2018-08-10T17:55:46.969071: step 470, loss 0.00952735, acc 1\n",
      "2018-08-10T17:55:47.116331: step 471, loss 0.0131396, acc 0.984375\n",
      "2018-08-10T17:55:47.262476: step 472, loss 0.00764377, acc 1\n",
      "2018-08-10T17:55:47.412455: step 473, loss 0.00303561, acc 1\n",
      "2018-08-10T17:55:47.564689: step 474, loss 0.00799439, acc 1\n",
      "2018-08-10T17:55:47.718965: step 475, loss 0.0263833, acc 0.984375\n",
      "2018-08-10T17:55:47.879167: step 476, loss 0.0213842, acc 0.984375\n",
      "2018-08-10T17:55:48.035812: step 477, loss 0.0386429, acc 0.984375\n",
      "2018-08-10T17:55:48.192604: step 478, loss 0.0171623, acc 1\n",
      "2018-08-10T17:55:48.350023: step 479, loss 0.0224602, acc 1\n",
      "2018-08-10T17:55:48.507735: step 480, loss 0.0264001, acc 0.984375\n",
      "2018-08-10T17:55:48.662829: step 481, loss 0.00954497, acc 1\n",
      "2018-08-10T17:55:48.819656: step 482, loss 0.0133853, acc 1\n",
      "2018-08-10T17:55:48.976345: step 483, loss 0.00287372, acc 1\n",
      "2018-08-10T17:55:49.129975: step 484, loss 0.00872795, acc 1\n",
      "2018-08-10T17:55:49.283575: step 485, loss 0.0106209, acc 1\n",
      "2018-08-10T17:55:49.441438: step 486, loss 0.00188155, acc 1\n",
      "2018-08-10T17:55:49.599870: step 487, loss 0.0142476, acc 1\n",
      "2018-08-10T17:55:49.754973: step 488, loss 0.0255379, acc 0.984375\n",
      "2018-08-10T17:55:49.908890: step 489, loss 0.0284053, acc 0.984375\n",
      "2018-08-10T17:55:50.063979: step 490, loss 0.0474855, acc 0.984375\n",
      "2018-08-10T17:55:50.219848: step 491, loss 0.00287425, acc 1\n",
      "2018-08-10T17:55:50.375169: step 492, loss 0.0224055, acc 0.984375\n",
      "2018-08-10T17:55:50.534189: step 493, loss 0.0131836, acc 1\n",
      "2018-08-10T17:55:50.683736: step 494, loss 0.00496379, acc 1\n",
      "2018-08-10T17:55:50.842866: step 495, loss 0.00772186, acc 1\n",
      "2018-08-10T17:55:51.001169: step 496, loss 0.0230202, acc 0.984375\n",
      "2018-08-10T17:55:51.159986: step 497, loss 0.0145975, acc 1\n",
      "2018-08-10T17:55:51.317583: step 498, loss 0.00801391, acc 1\n",
      "2018-08-10T17:55:51.475766: step 499, loss 0.0162156, acc 1\n",
      "2018-08-10T17:55:51.633018: step 500, loss 0.0552329, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:55:51.789125: step 500, loss 0.493402, acc 0.880383\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533923672/checkpoints/model-500\n",
      "\n",
      "2018-08-10T17:55:51.998276: step 501, loss 0.0042901, acc 1\n",
      "2018-08-10T17:55:52.154732: step 502, loss 0.00699049, acc 1\n",
      "2018-08-10T17:55:52.312035: step 503, loss 0.0142156, acc 1\n",
      "2018-08-10T17:55:52.471574: step 504, loss 0.0152927, acc 0.984375\n",
      "2018-08-10T17:55:52.629676: step 505, loss 0.0959343, acc 0.953125\n",
      "2018-08-10T17:55:52.788547: step 506, loss 0.030082, acc 0.984375\n",
      "2018-08-10T17:55:52.939834: step 507, loss 0.0359664, acc 0.983871\n",
      "2018-08-10T17:55:53.096127: step 508, loss 0.0100697, acc 1\n",
      "2018-08-10T17:55:53.250967: step 509, loss 0.0028047, acc 1\n",
      "2018-08-10T17:55:53.405189: step 510, loss 0.0118035, acc 1\n",
      "2018-08-10T17:55:53.561345: step 511, loss 0.0123624, acc 1\n",
      "2018-08-10T17:55:53.716171: step 512, loss 0.0704724, acc 0.984375\n",
      "2018-08-10T17:55:53.875288: step 513, loss 0.01587, acc 1\n",
      "2018-08-10T17:55:54.031953: step 514, loss 0.00946195, acc 1\n",
      "2018-08-10T17:55:54.188755: step 515, loss 0.0778965, acc 0.96875\n",
      "2018-08-10T17:55:54.345459: step 516, loss 0.0642266, acc 0.96875\n",
      "2018-08-10T17:55:54.503331: step 517, loss 0.0054179, acc 1\n",
      "2018-08-10T17:55:54.658730: step 518, loss 0.00624126, acc 1\n",
      "2018-08-10T17:55:54.818976: step 519, loss 0.0248409, acc 0.984375\n",
      "2018-08-10T17:55:54.972906: step 520, loss 0.00636799, acc 1\n",
      "2018-08-10T17:55:55.133703: step 521, loss 0.00575354, acc 1\n",
      "2018-08-10T17:55:55.290967: step 522, loss 0.0355984, acc 0.984375\n",
      "2018-08-10T17:55:55.448169: step 523, loss 0.0240394, acc 0.984375\n",
      "2018-08-10T17:55:55.606043: step 524, loss 0.00389999, acc 1\n",
      "2018-08-10T17:55:55.764389: step 525, loss 0.0160855, acc 1\n",
      "2018-08-10T17:55:55.924146: step 526, loss 0.0333017, acc 0.984375\n",
      "2018-08-10T17:55:56.080823: step 527, loss 0.0168223, acc 1\n",
      "2018-08-10T17:55:56.237807: step 528, loss 0.0265788, acc 0.984375\n",
      "2018-08-10T17:55:56.395378: step 529, loss 0.0105283, acc 1\n",
      "2018-08-10T17:55:56.555378: step 530, loss 0.014182, acc 0.984375\n",
      "2018-08-10T17:55:56.714151: step 531, loss 0.00736189, acc 1\n",
      "2018-08-10T17:55:56.872914: step 532, loss 0.0908248, acc 0.984375\n",
      "2018-08-10T17:55:57.027709: step 533, loss 0.00152383, acc 1\n",
      "2018-08-10T17:55:57.185073: step 534, loss 0.0214024, acc 1\n",
      "2018-08-10T17:55:57.342129: step 535, loss 0.00910423, acc 1\n",
      "2018-08-10T17:55:57.497723: step 536, loss 0.00134271, acc 1\n",
      "2018-08-10T17:55:57.657697: step 537, loss 0.0107838, acc 1\n",
      "2018-08-10T17:55:57.817475: step 538, loss 0.0364779, acc 0.984375\n",
      "2018-08-10T17:55:57.977042: step 539, loss 0.024586, acc 0.984375\n",
      "2018-08-10T17:55:58.128163: step 540, loss 0.00361621, acc 1\n",
      "2018-08-10T17:55:58.280215: step 541, loss 0.00502861, acc 1\n",
      "2018-08-10T17:55:58.440975: step 542, loss 0.0117246, acc 1\n",
      "2018-08-10T17:55:58.596644: step 543, loss 0.0549075, acc 0.96875\n",
      "2018-08-10T17:55:58.756162: step 544, loss 0.00753568, acc 1\n",
      "2018-08-10T17:55:58.913591: step 545, loss 0.0082531, acc 1\n",
      "2018-08-10T17:55:59.067071: step 546, loss 0.00246597, acc 1\n",
      "2018-08-10T17:55:59.225027: step 547, loss 0.00353214, acc 1\n",
      "2018-08-10T17:55:59.383738: step 548, loss 0.0121479, acc 1\n",
      "2018-08-10T17:55:59.542520: step 549, loss 0.0185502, acc 0.984375\n",
      "2018-08-10T17:55:59.701435: step 550, loss 0.0232014, acc 0.984375\n",
      "2018-08-10T17:55:59.860857: step 551, loss 0.00884753, acc 1\n",
      "2018-08-10T17:56:00.017550: step 552, loss 0.0551822, acc 0.96875\n",
      "2018-08-10T17:56:00.176149: step 553, loss 0.0742217, acc 0.96875\n",
      "2018-08-10T17:56:00.332713: step 554, loss 0.00356037, acc 1\n",
      "2018-08-10T17:56:00.494128: step 555, loss 0.00460962, acc 1\n",
      "2018-08-10T17:56:00.654375: step 556, loss 0.00503924, acc 1\n",
      "2018-08-10T17:56:00.815591: step 557, loss 0.0146722, acc 0.984375\n",
      "2018-08-10T17:56:00.981405: step 558, loss 0.00474434, acc 1\n",
      "2018-08-10T17:56:01.137555: step 559, loss 0.00941316, acc 1\n",
      "2018-08-10T17:56:01.297994: step 560, loss 0.00898925, acc 1\n",
      "2018-08-10T17:56:01.457889: step 561, loss 0.00527833, acc 1\n",
      "2018-08-10T17:56:01.615176: step 562, loss 0.004199, acc 1\n",
      "2018-08-10T17:56:01.775871: step 563, loss 0.0038036, acc 1\n",
      "2018-08-10T17:56:01.935343: step 564, loss 0.00259623, acc 1\n",
      "2018-08-10T17:56:02.091747: step 565, loss 0.00473208, acc 1\n",
      "2018-08-10T17:56:02.246739: step 566, loss 0.040561, acc 0.984375\n",
      "2018-08-10T17:56:02.403769: step 567, loss 0.064009, acc 0.96875\n",
      "2018-08-10T17:56:02.560053: step 568, loss 0.00572922, acc 1\n",
      "2018-08-10T17:56:02.720328: step 569, loss 0.00199801, acc 1\n",
      "2018-08-10T17:56:02.881953: step 570, loss 0.0665135, acc 0.984375\n",
      "2018-08-10T17:56:03.041646: step 571, loss 0.0114867, acc 1\n",
      "2018-08-10T17:56:03.193828: step 572, loss 0.0177895, acc 0.983871\n",
      "2018-08-10T17:56:03.349836: step 573, loss 0.00396087, acc 1\n",
      "2018-08-10T17:56:03.505952: step 574, loss 0.0043517, acc 1\n",
      "2018-08-10T17:56:03.663329: step 575, loss 0.0667858, acc 0.96875\n",
      "2018-08-10T17:56:03.822241: step 576, loss 0.0127405, acc 0.984375\n",
      "2018-08-10T17:56:03.982226: step 577, loss 0.000841523, acc 1\n",
      "2018-08-10T17:56:04.134627: step 578, loss 0.055821, acc 0.984375\n",
      "2018-08-10T17:56:04.284059: step 579, loss 0.00624962, acc 1\n",
      "2018-08-10T17:56:04.433117: step 580, loss 0.000821155, acc 1\n",
      "2018-08-10T17:56:04.581784: step 581, loss 0.0200886, acc 0.984375\n",
      "2018-08-10T17:56:04.733563: step 582, loss 0.00795459, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:56:04.884908: step 583, loss 0.00296964, acc 1\n",
      "2018-08-10T17:56:05.035083: step 584, loss 0.0130966, acc 1\n",
      "2018-08-10T17:56:05.179836: step 585, loss 0.00408945, acc 1\n",
      "2018-08-10T17:56:05.326149: step 586, loss 0.00294464, acc 1\n",
      "2018-08-10T17:56:05.474281: step 587, loss 0.00996801, acc 1\n",
      "2018-08-10T17:56:05.623319: step 588, loss 0.0658709, acc 0.96875\n",
      "2018-08-10T17:56:05.773109: step 589, loss 0.0074356, acc 1\n",
      "2018-08-10T17:56:05.920084: step 590, loss 0.00554395, acc 1\n",
      "2018-08-10T17:56:06.068693: step 591, loss 0.0527918, acc 0.984375\n",
      "2018-08-10T17:56:06.215072: step 592, loss 0.019254, acc 0.984375\n",
      "2018-08-10T17:56:06.364290: step 593, loss 0.00419362, acc 1\n",
      "2018-08-10T17:56:06.512318: step 594, loss 0.00229754, acc 1\n",
      "2018-08-10T17:56:06.662273: step 595, loss 0.0372981, acc 0.96875\n",
      "2018-08-10T17:56:06.810322: step 596, loss 0.037835, acc 0.984375\n",
      "2018-08-10T17:56:06.958653: step 597, loss 0.00690028, acc 1\n",
      "2018-08-10T17:56:07.103501: step 598, loss 0.00256814, acc 1\n",
      "2018-08-10T17:56:07.258630: step 599, loss 0.00763037, acc 1\n",
      "2018-08-10T17:56:07.408346: step 600, loss 0.0216553, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:56:07.564739: step 600, loss 0.550441, acc 0.866029\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533923672/checkpoints/model-600\n",
      "\n",
      "2018-08-10T17:56:07.768401: step 601, loss 0.003367, acc 1\n",
      "2018-08-10T17:56:07.917778: step 602, loss 0.00290337, acc 1\n",
      "2018-08-10T17:56:08.067440: step 603, loss 0.00302537, acc 1\n",
      "2018-08-10T17:56:08.214958: step 604, loss 0.00761292, acc 1\n",
      "2018-08-10T17:56:08.365541: step 605, loss 0.00482017, acc 1\n",
      "2018-08-10T17:56:08.521210: step 606, loss 0.00937939, acc 1\n",
      "2018-08-10T17:56:08.676528: step 607, loss 0.0021409, acc 1\n",
      "2018-08-10T17:56:08.837090: step 608, loss 0.0452091, acc 0.96875\n",
      "2018-08-10T17:56:08.997788: step 609, loss 0.00411569, acc 1\n",
      "2018-08-10T17:56:09.158310: step 610, loss 0.00199448, acc 1\n",
      "2018-08-10T17:56:09.314807: step 611, loss 0.0083379, acc 1\n",
      "2018-08-10T17:56:09.475394: step 612, loss 0.0177868, acc 1\n",
      "2018-08-10T17:56:09.632468: step 613, loss 0.00876451, acc 1\n",
      "2018-08-10T17:56:09.792997: step 614, loss 0.00654297, acc 1\n",
      "2018-08-10T17:56:09.956611: step 615, loss 0.0159923, acc 1\n",
      "2018-08-10T17:56:10.116612: step 616, loss 0.0232329, acc 0.984375\n",
      "2018-08-10T17:56:10.275096: step 617, loss 0.00583397, acc 1\n",
      "2018-08-10T17:56:10.434251: step 618, loss 0.00307461, acc 1\n",
      "2018-08-10T17:56:10.595703: step 619, loss 0.0022087, acc 1\n",
      "2018-08-10T17:56:10.757448: step 620, loss 0.00160181, acc 1\n",
      "2018-08-10T17:56:10.916137: step 621, loss 0.0129883, acc 1\n",
      "2018-08-10T17:56:11.079160: step 622, loss 0.0227765, acc 0.984375\n",
      "2018-08-10T17:56:11.235738: step 623, loss 0.0236848, acc 0.984375\n",
      "2018-08-10T17:56:11.389998: step 624, loss 0.00373827, acc 1\n",
      "2018-08-10T17:56:11.547652: step 625, loss 0.0343779, acc 0.984375\n",
      "2018-08-10T17:56:11.706373: step 626, loss 0.00157966, acc 1\n",
      "2018-08-10T17:56:11.871795: step 627, loss 0.00953174, acc 1\n",
      "2018-08-10T17:56:12.034235: step 628, loss 0.0024375, acc 1\n",
      "2018-08-10T17:56:12.194116: step 629, loss 0.0044082, acc 1\n",
      "2018-08-10T17:56:12.353989: step 630, loss 0.000831759, acc 1\n",
      "2018-08-10T17:56:12.516282: step 631, loss 0.00451258, acc 1\n",
      "2018-08-10T17:56:12.675386: step 632, loss 0.000562392, acc 1\n",
      "2018-08-10T17:56:12.838109: step 633, loss 0.0097515, acc 1\n",
      "2018-08-10T17:56:12.995699: step 634, loss 0.00307015, acc 1\n",
      "2018-08-10T17:56:13.153372: step 635, loss 0.0124438, acc 1\n",
      "2018-08-10T17:56:13.313326: step 636, loss 0.0616021, acc 0.984375\n",
      "2018-08-10T17:56:13.468674: step 637, loss 0.0184009, acc 0.983871\n",
      "2018-08-10T17:56:13.629450: step 638, loss 0.0165787, acc 0.984375\n",
      "2018-08-10T17:56:13.789135: step 639, loss 0.000988507, acc 1\n",
      "2018-08-10T17:56:13.949349: step 640, loss 0.00160991, acc 1\n",
      "2018-08-10T17:56:14.106857: step 641, loss 0.00247117, acc 1\n",
      "2018-08-10T17:56:14.264266: step 642, loss 0.00432008, acc 1\n",
      "2018-08-10T17:56:14.422837: step 643, loss 0.00220211, acc 1\n",
      "2018-08-10T17:56:14.587383: step 644, loss 0.00325225, acc 1\n",
      "2018-08-10T17:56:14.745871: step 645, loss 0.00350891, acc 1\n",
      "2018-08-10T17:56:14.903878: step 646, loss 0.00551949, acc 1\n",
      "2018-08-10T17:56:15.063821: step 647, loss 0.00780176, acc 1\n",
      "2018-08-10T17:56:15.220646: step 648, loss 0.00832271, acc 1\n",
      "2018-08-10T17:56:15.379119: step 649, loss 0.00264257, acc 1\n",
      "2018-08-10T17:56:15.531805: step 650, loss 0.0183371, acc 1\n",
      "2018-08-10T17:56:15.689283: step 651, loss 0.00508354, acc 1\n",
      "2018-08-10T17:56:15.848724: step 652, loss 0.00450195, acc 1\n",
      "2018-08-10T17:56:16.005652: step 653, loss 0.0308232, acc 0.984375\n",
      "2018-08-10T17:56:16.162963: step 654, loss 0.003179, acc 1\n",
      "2018-08-10T17:56:16.319301: step 655, loss 0.00393565, acc 1\n",
      "2018-08-10T17:56:16.478306: step 656, loss 0.00469308, acc 1\n",
      "2018-08-10T17:56:16.638760: step 657, loss 0.00984772, acc 1\n",
      "2018-08-10T17:56:16.796151: step 658, loss 0.00534905, acc 1\n",
      "2018-08-10T17:56:16.955619: step 659, loss 0.00398141, acc 1\n",
      "2018-08-10T17:56:17.111744: step 660, loss 0.0256845, acc 0.984375\n",
      "2018-08-10T17:56:17.267222: step 661, loss 0.036111, acc 0.984375\n",
      "2018-08-10T17:56:17.420815: step 662, loss 0.0080635, acc 1\n",
      "2018-08-10T17:56:17.573706: step 663, loss 0.00137816, acc 1\n",
      "2018-08-10T17:56:17.731781: step 664, loss 0.00898174, acc 1\n",
      "2018-08-10T17:56:17.889613: step 665, loss 0.0108333, acc 1\n",
      "2018-08-10T17:56:18.044625: step 666, loss 0.00276031, acc 1\n",
      "2018-08-10T17:56:18.201183: step 667, loss 0.0144789, acc 0.984375\n",
      "2018-08-10T17:56:18.357618: step 668, loss 0.00332695, acc 1\n",
      "2018-08-10T17:56:18.516435: step 669, loss 0.00527823, acc 1\n",
      "2018-08-10T17:56:18.671205: step 670, loss 0.00884658, acc 1\n",
      "2018-08-10T17:56:18.831779: step 671, loss 0.0228635, acc 0.984375\n",
      "2018-08-10T17:56:18.981736: step 672, loss 0.00830719, acc 1\n",
      "2018-08-10T17:56:19.129585: step 673, loss 0.00189498, acc 1\n",
      "2018-08-10T17:56:19.277548: step 674, loss 0.00373428, acc 1\n",
      "2018-08-10T17:56:19.429433: step 675, loss 0.0320189, acc 0.984375\n",
      "2018-08-10T17:56:19.579449: step 676, loss 0.00473606, acc 1\n",
      "2018-08-10T17:56:19.735562: step 677, loss 0.00121563, acc 1\n",
      "2018-08-10T17:56:19.892401: step 678, loss 0.0137412, acc 1\n",
      "2018-08-10T17:56:20.046298: step 679, loss 0.00278839, acc 1\n",
      "2018-08-10T17:56:20.200078: step 680, loss 0.00193391, acc 1\n",
      "2018-08-10T17:56:20.356432: step 681, loss 0.00263591, acc 1\n",
      "2018-08-10T17:56:20.513139: step 682, loss 0.0027068, acc 1\n",
      "2018-08-10T17:56:20.670311: step 683, loss 0.0159928, acc 1\n",
      "2018-08-10T17:56:20.824971: step 684, loss 0.00305087, acc 1\n",
      "2018-08-10T17:56:20.980778: step 685, loss 0.0152089, acc 1\n",
      "2018-08-10T17:56:21.135727: step 686, loss 0.00157589, acc 1\n",
      "2018-08-10T17:56:21.292052: step 687, loss 0.0100675, acc 1\n",
      "2018-08-10T17:56:21.447319: step 688, loss 0.00128068, acc 1\n",
      "2018-08-10T17:56:21.601227: step 689, loss 0.0249185, acc 0.983871\n",
      "2018-08-10T17:56:21.756468: step 690, loss 0.000826049, acc 1\n",
      "2018-08-10T17:56:21.912853: step 691, loss 0.0124224, acc 1\n",
      "2018-08-10T17:56:22.067839: step 692, loss 0.0384975, acc 0.984375\n",
      "2018-08-10T17:56:22.222215: step 693, loss 0.00739804, acc 1\n",
      "2018-08-10T17:56:22.374942: step 694, loss 0.00413464, acc 1\n",
      "2018-08-10T17:56:22.533044: step 695, loss 0.0099939, acc 1\n",
      "2018-08-10T17:56:22.688945: step 696, loss 0.0013961, acc 1\n",
      "2018-08-10T17:56:22.849709: step 697, loss 0.00580071, acc 1\n",
      "2018-08-10T17:56:23.005101: step 698, loss 0.0015719, acc 1\n",
      "2018-08-10T17:56:23.162414: step 699, loss 0.00783188, acc 1\n",
      "2018-08-10T17:56:23.317024: step 700, loss 0.000659577, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:56:23.466439: step 700, loss 0.626707, acc 0.861244\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533923672/checkpoints/model-700\n",
      "\n",
      "2018-08-10T17:56:23.678197: step 701, loss 0.00421548, acc 1\n",
      "2018-08-10T17:56:23.832259: step 702, loss 0.00225736, acc 1\n",
      "2018-08-10T17:56:23.988224: step 703, loss 0.0025095, acc 1\n",
      "2018-08-10T17:56:24.145634: step 704, loss 0.00108007, acc 1\n",
      "2018-08-10T17:56:24.301216: step 705, loss 0.02131, acc 0.984375\n",
      "2018-08-10T17:56:24.458312: step 706, loss 0.00306773, acc 1\n",
      "2018-08-10T17:56:24.617563: step 707, loss 0.00510947, acc 1\n",
      "2018-08-10T17:56:24.775864: step 708, loss 0.00117737, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:56:24.932131: step 709, loss 0.0139701, acc 1\n",
      "2018-08-10T17:56:25.084459: step 710, loss 0.00148139, acc 1\n",
      "2018-08-10T17:56:25.236002: step 711, loss 0.0357071, acc 0.984375\n",
      "2018-08-10T17:56:25.381378: step 712, loss 0.00288769, acc 1\n",
      "2018-08-10T17:56:25.530293: step 713, loss 0.00653244, acc 1\n",
      "2018-08-10T17:56:25.678149: step 714, loss 0.00450609, acc 1\n",
      "2018-08-10T17:56:25.823791: step 715, loss 0.0338729, acc 0.983871\n",
      "2018-08-10T17:56:25.969984: step 716, loss 0.002281, acc 1\n",
      "2018-08-10T17:56:26.117799: step 717, loss 0.0011525, acc 1\n",
      "2018-08-10T17:56:26.265233: step 718, loss 0.0064105, acc 1\n",
      "2018-08-10T17:56:26.413381: step 719, loss 0.00115071, acc 1\n",
      "2018-08-10T17:56:26.562322: step 720, loss 0.0019168, acc 1\n",
      "2018-08-10T17:56:26.711860: step 721, loss 0.00356792, acc 1\n",
      "2018-08-10T17:56:26.860103: step 722, loss 0.00529688, acc 1\n",
      "2018-08-10T17:56:27.008145: step 723, loss 0.00131913, acc 1\n",
      "2018-08-10T17:56:27.155246: step 724, loss 0.00239418, acc 1\n",
      "2018-08-10T17:56:27.303383: step 725, loss 0.00202494, acc 1\n",
      "2018-08-10T17:56:27.451986: step 726, loss 0.000950672, acc 1\n",
      "2018-08-10T17:56:27.599458: step 727, loss 0.00777782, acc 1\n",
      "2018-08-10T17:56:27.744088: step 728, loss 0.00154995, acc 1\n",
      "2018-08-10T17:56:27.896307: step 729, loss 0.0108619, acc 1\n",
      "2018-08-10T17:56:28.043627: step 730, loss 0.00131728, acc 1\n",
      "2018-08-10T17:56:28.192888: step 731, loss 0.00205097, acc 1\n",
      "2018-08-10T17:56:28.340566: step 732, loss 0.013102, acc 1\n",
      "2018-08-10T17:56:28.491868: step 733, loss 0.00488257, acc 1\n",
      "2018-08-10T17:56:28.637838: step 734, loss 0.00287051, acc 1\n",
      "2018-08-10T17:56:28.788867: step 735, loss 0.00265386, acc 1\n",
      "2018-08-10T17:56:28.936106: step 736, loss 0.00141494, acc 1\n",
      "2018-08-10T17:56:29.083339: step 737, loss 0.00912246, acc 1\n",
      "2018-08-10T17:56:29.230113: step 738, loss 0.00663633, acc 1\n",
      "2018-08-10T17:56:29.384252: step 739, loss 0.00177673, acc 1\n",
      "2018-08-10T17:56:29.539727: step 740, loss 0.0151204, acc 1\n",
      "2018-08-10T17:56:29.691984: step 741, loss 0.00142831, acc 1\n",
      "2018-08-10T17:56:29.849462: step 742, loss 0.000614178, acc 1\n",
      "2018-08-10T17:56:30.006419: step 743, loss 0.00073755, acc 1\n",
      "2018-08-10T17:56:30.163788: step 744, loss 0.00365969, acc 1\n",
      "2018-08-10T17:56:30.324932: step 745, loss 0.0131672, acc 0.984375\n",
      "2018-08-10T17:56:30.486649: step 746, loss 0.000678208, acc 1\n",
      "2018-08-10T17:56:30.649401: step 747, loss 0.0296427, acc 0.984375\n",
      "2018-08-10T17:56:30.809697: step 748, loss 0.00653401, acc 1\n",
      "2018-08-10T17:56:30.967781: step 749, loss 0.00866161, acc 1\n",
      "2018-08-10T17:56:31.128265: step 750, loss 0.00475129, acc 1\n",
      "2018-08-10T17:56:31.287927: step 751, loss 0.00163552, acc 1\n",
      "2018-08-10T17:56:31.449024: step 752, loss 0.00428282, acc 1\n",
      "2018-08-10T17:56:31.607774: step 753, loss 0.00186914, acc 1\n",
      "2018-08-10T17:56:31.763255: step 754, loss 0.000591498, acc 1\n",
      "2018-08-10T17:56:31.925995: step 755, loss 0.00651908, acc 1\n",
      "2018-08-10T17:56:32.084164: step 756, loss 0.0182493, acc 0.984375\n",
      "2018-08-10T17:56:32.241161: step 757, loss 0.00271936, acc 1\n",
      "2018-08-10T17:56:32.398206: step 758, loss 0.0175606, acc 0.984375\n",
      "2018-08-10T17:56:32.556761: step 759, loss 0.0108646, acc 1\n",
      "2018-08-10T17:56:32.717671: step 760, loss 0.00987482, acc 1\n",
      "2018-08-10T17:56:32.877688: step 761, loss 0.00174855, acc 1\n",
      "2018-08-10T17:56:33.037763: step 762, loss 0.00260651, acc 1\n",
      "2018-08-10T17:56:33.197284: step 763, loss 0.0203024, acc 0.984375\n",
      "2018-08-10T17:56:33.355276: step 764, loss 0.0013015, acc 1\n",
      "2018-08-10T17:56:33.517573: step 765, loss 0.0249317, acc 0.984375\n",
      "2018-08-10T17:56:33.678459: step 766, loss 0.118467, acc 0.96875\n",
      "2018-08-10T17:56:33.832870: step 767, loss 0.00522083, acc 1\n",
      "2018-08-10T17:56:33.990836: step 768, loss 0.00225339, acc 1\n",
      "2018-08-10T17:56:34.149865: step 769, loss 0.0014571, acc 1\n",
      "2018-08-10T17:56:34.306176: step 770, loss 0.00164201, acc 1\n",
      "2018-08-10T17:56:34.466931: step 771, loss 0.0020571, acc 1\n",
      "2018-08-10T17:56:34.626884: step 772, loss 0.00159421, acc 1\n",
      "2018-08-10T17:56:34.788420: step 773, loss 0.00452408, acc 1\n",
      "2018-08-10T17:56:34.946828: step 774, loss 0.00292193, acc 1\n",
      "2018-08-10T17:56:35.104589: step 775, loss 0.0179529, acc 1\n",
      "2018-08-10T17:56:35.261139: step 776, loss 0.00511588, acc 1\n",
      "2018-08-10T17:56:35.419475: step 777, loss 0.00285918, acc 1\n",
      "2018-08-10T17:56:35.576151: step 778, loss 0.0264232, acc 0.984375\n",
      "2018-08-10T17:56:35.733581: step 779, loss 0.007085, acc 1\n",
      "2018-08-10T17:56:35.886668: step 780, loss 0.0171222, acc 1\n",
      "2018-08-10T17:56:36.045577: step 781, loss 0.0297946, acc 0.984375\n",
      "2018-08-10T17:56:36.202835: step 782, loss 0.0031799, acc 1\n",
      "2018-08-10T17:56:36.361328: step 783, loss 0.0108961, acc 1\n",
      "2018-08-10T17:56:36.521066: step 784, loss 0.00312999, acc 1\n",
      "2018-08-10T17:56:36.681459: step 785, loss 0.00538606, acc 1\n",
      "2018-08-10T17:56:36.840218: step 786, loss 0.00348269, acc 1\n",
      "2018-08-10T17:56:36.997974: step 787, loss 0.00150633, acc 1\n",
      "2018-08-10T17:56:37.155398: step 788, loss 0.00174759, acc 1\n",
      "2018-08-10T17:56:37.315874: step 789, loss 0.000775519, acc 1\n",
      "2018-08-10T17:56:37.475291: step 790, loss 0.00521993, acc 1\n",
      "2018-08-10T17:56:37.634741: step 791, loss 0.00764282, acc 1\n",
      "2018-08-10T17:56:37.806737: step 792, loss 0.000904124, acc 1\n",
      "2018-08-10T17:56:37.960556: step 793, loss 0.00754572, acc 1\n",
      "2018-08-10T17:56:38.120857: step 794, loss 0.00258355, acc 1\n",
      "2018-08-10T17:56:38.281411: step 795, loss 0.00720795, acc 1\n",
      "2018-08-10T17:56:38.445671: step 796, loss 0.00745486, acc 1\n",
      "2018-08-10T17:56:38.601037: step 797, loss 0.00721264, acc 1\n",
      "2018-08-10T17:56:38.758981: step 798, loss 0.0106235, acc 1\n",
      "2018-08-10T17:56:38.921797: step 799, loss 0.02189, acc 0.984375\n",
      "2018-08-10T17:56:39.079382: step 800, loss 0.00205115, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:56:39.234944: step 800, loss 0.662141, acc 0.851675\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533923672/checkpoints/model-800\n",
      "\n",
      "2018-08-10T17:56:39.455215: step 801, loss 0.000950953, acc 1\n",
      "2018-08-10T17:56:39.613953: step 802, loss 0.00511357, acc 1\n",
      "2018-08-10T17:56:39.775483: step 803, loss 0.0314545, acc 0.984375\n",
      "2018-08-10T17:56:39.937346: step 804, loss 0.00514087, acc 1\n",
      "2018-08-10T17:56:40.087573: step 805, loss 0.0171578, acc 0.984375\n",
      "2018-08-10T17:56:40.233863: step 806, loss 0.00610794, acc 1\n",
      "2018-08-10T17:56:40.385398: step 807, loss 0.0213057, acc 0.984375\n",
      "2018-08-10T17:56:40.542865: step 808, loss 0.00106801, acc 1\n",
      "2018-08-10T17:56:40.700822: step 809, loss 0.0040631, acc 1\n",
      "2018-08-10T17:56:40.858630: step 810, loss 0.000595744, acc 1\n",
      "2018-08-10T17:56:41.015375: step 811, loss 0.0119422, acc 1\n",
      "2018-08-10T17:56:41.174610: step 812, loss 0.0132145, acc 0.984375\n",
      "2018-08-10T17:56:41.332756: step 813, loss 0.00593849, acc 1\n",
      "2018-08-10T17:56:41.491358: step 814, loss 0.022816, acc 0.984375\n",
      "2018-08-10T17:56:41.651110: step 815, loss 0.00364862, acc 1\n",
      "2018-08-10T17:56:41.810610: step 816, loss 0.00465276, acc 1\n",
      "2018-08-10T17:56:41.967751: step 817, loss 0.00650338, acc 1\n",
      "2018-08-10T17:56:42.126177: step 818, loss 0.0014002, acc 1\n",
      "2018-08-10T17:56:42.283006: step 819, loss 0.000983814, acc 1\n",
      "2018-08-10T17:56:42.444790: step 820, loss 0.00577254, acc 1\n",
      "2018-08-10T17:56:42.601973: step 821, loss 0.000859643, acc 1\n",
      "2018-08-10T17:56:42.761692: step 822, loss 0.00475496, acc 1\n",
      "2018-08-10T17:56:42.921228: step 823, loss 0.00825659, acc 1\n",
      "2018-08-10T17:56:43.080885: step 824, loss 0.0199988, acc 0.984375\n",
      "2018-08-10T17:56:43.239608: step 825, loss 0.0188481, acc 1\n",
      "2018-08-10T17:56:43.394962: step 826, loss 0.000772864, acc 1\n",
      "2018-08-10T17:56:43.562218: step 827, loss 0.000696213, acc 1\n",
      "2018-08-10T17:56:43.720702: step 828, loss 0.0012334, acc 1\n",
      "2018-08-10T17:56:43.879768: step 829, loss 0.00259478, acc 1\n",
      "2018-08-10T17:56:44.038117: step 830, loss 0.00246307, acc 1\n",
      "2018-08-10T17:56:44.194404: step 831, loss 0.011068, acc 1\n",
      "2018-08-10T17:56:44.348469: step 832, loss 0.00751966, acc 1\n",
      "2018-08-10T17:56:44.510595: step 833, loss 0.00356718, acc 1\n",
      "2018-08-10T17:56:44.671372: step 834, loss 0.00263661, acc 1\n",
      "2018-08-10T17:56:44.833835: step 835, loss 0.00664665, acc 1\n",
      "2018-08-10T17:56:44.993864: step 836, loss 0.00242204, acc 1\n",
      "2018-08-10T17:56:45.151715: step 837, loss 0.00140578, acc 1\n",
      "2018-08-10T17:56:45.308924: step 838, loss 0.00231801, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:56:45.468954: step 839, loss 0.00260555, acc 1\n",
      "2018-08-10T17:56:45.626823: step 840, loss 0.00451148, acc 1\n",
      "2018-08-10T17:56:45.788071: step 841, loss 0.0120887, acc 0.984375\n",
      "2018-08-10T17:56:45.952461: step 842, loss 0.00158953, acc 1\n",
      "2018-08-10T17:56:46.111114: step 843, loss 0.00178036, acc 1\n",
      "2018-08-10T17:56:46.260096: step 844, loss 0.000811517, acc 1\n",
      "2018-08-10T17:56:46.406262: step 845, loss 0.00382229, acc 1\n",
      "2018-08-10T17:56:46.558310: step 846, loss 0.00130416, acc 1\n",
      "2018-08-10T17:56:46.716481: step 847, loss 0.00232515, acc 1\n",
      "2018-08-10T17:56:46.868224: step 848, loss 0.00930243, acc 1\n",
      "2018-08-10T17:56:47.014703: step 849, loss 0.0039676, acc 1\n",
      "2018-08-10T17:56:47.161913: step 850, loss 0.00324623, acc 1\n",
      "2018-08-10T17:56:47.310746: step 851, loss 0.00150675, acc 1\n",
      "2018-08-10T17:56:47.458090: step 852, loss 0.00243456, acc 1\n",
      "2018-08-10T17:56:47.606986: step 853, loss 0.00351425, acc 1\n",
      "2018-08-10T17:56:47.754256: step 854, loss 0.0189141, acc 0.984375\n",
      "2018-08-10T17:56:47.904411: step 855, loss 0.00414028, acc 1\n",
      "2018-08-10T17:56:48.053938: step 856, loss 0.000415503, acc 1\n",
      "2018-08-10T17:56:48.202442: step 857, loss 0.00125014, acc 1\n",
      "2018-08-10T17:56:48.347055: step 858, loss 0.0225234, acc 0.983871\n",
      "2018-08-10T17:56:48.496917: step 859, loss 0.0134864, acc 0.984375\n",
      "2018-08-10T17:56:48.647536: step 860, loss 0.000936691, acc 1\n",
      "2018-08-10T17:56:48.798321: step 861, loss 0.000754675, acc 1\n",
      "2018-08-10T17:56:48.946498: step 862, loss 0.0210951, acc 0.984375\n",
      "2018-08-10T17:56:49.095210: step 863, loss 0.000910026, acc 1\n",
      "2018-08-10T17:56:49.242097: step 864, loss 0.00159569, acc 1\n",
      "2018-08-10T17:56:49.390651: step 865, loss 0.00148321, acc 1\n",
      "2018-08-10T17:56:49.540145: step 866, loss 0.000362851, acc 1\n",
      "2018-08-10T17:56:49.688387: step 867, loss 0.0109413, acc 1\n",
      "2018-08-10T17:56:49.836795: step 868, loss 0.00231708, acc 1\n",
      "2018-08-10T17:56:49.985392: step 869, loss 0.0295026, acc 0.984375\n",
      "2018-08-10T17:56:50.134374: step 870, loss 0.000557279, acc 1\n",
      "2018-08-10T17:56:50.279320: step 871, loss 0.00225726, acc 1\n",
      "2018-08-10T17:56:50.429037: step 872, loss 0.0161283, acc 0.984375\n",
      "2018-08-10T17:56:50.582606: step 873, loss 0.000975093, acc 1\n",
      "2018-08-10T17:56:50.738859: step 874, loss 0.000639477, acc 1\n",
      "2018-08-10T17:56:50.897445: step 875, loss 0.0403263, acc 0.96875\n",
      "2018-08-10T17:56:51.053908: step 876, loss 0.00279455, acc 1\n",
      "2018-08-10T17:56:51.211401: step 877, loss 0.00137183, acc 1\n",
      "2018-08-10T17:56:51.368907: step 878, loss 0.00596124, acc 1\n",
      "2018-08-10T17:56:51.528994: step 879, loss 0.0264363, acc 0.984375\n",
      "2018-08-10T17:56:51.684646: step 880, loss 0.00444653, acc 1\n",
      "2018-08-10T17:56:51.843342: step 881, loss 0.00828716, acc 1\n",
      "2018-08-10T17:56:52.002126: step 882, loss 0.0138919, acc 0.984375\n",
      "2018-08-10T17:56:52.159822: step 883, loss 0.00375777, acc 1\n",
      "2018-08-10T17:56:52.310888: step 884, loss 0.00866563, acc 1\n",
      "2018-08-10T17:56:52.467854: step 885, loss 0.00198135, acc 1\n",
      "2018-08-10T17:56:52.625159: step 886, loss 0.00793992, acc 1\n",
      "2018-08-10T17:56:52.783117: step 887, loss 0.00262934, acc 1\n",
      "2018-08-10T17:56:52.942354: step 888, loss 0.000798979, acc 1\n",
      "2018-08-10T17:56:53.099987: step 889, loss 0.000941529, acc 1\n",
      "2018-08-10T17:56:53.258675: step 890, loss 0.00117133, acc 1\n",
      "2018-08-10T17:56:53.415892: step 891, loss 0.00759765, acc 1\n",
      "2018-08-10T17:56:53.577939: step 892, loss 0.000435131, acc 1\n",
      "2018-08-10T17:56:53.733940: step 893, loss 0.00558305, acc 1\n",
      "2018-08-10T17:56:53.892553: step 894, loss 0.00135197, acc 1\n",
      "2018-08-10T17:56:54.049673: step 895, loss 0.00130689, acc 1\n",
      "2018-08-10T17:56:54.208469: step 896, loss 0.00181867, acc 1\n",
      "2018-08-10T17:56:54.359965: step 897, loss 0.00462798, acc 1\n",
      "2018-08-10T17:56:54.519257: step 898, loss 0.000573583, acc 1\n",
      "2018-08-10T17:56:54.677788: step 899, loss 0.00182433, acc 1\n",
      "2018-08-10T17:56:54.836950: step 900, loss 0.00764903, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:56:54.989130: step 900, loss 0.642241, acc 0.856459\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533923672/checkpoints/model-900\n",
      "\n",
      "2018-08-10T17:56:55.205013: step 901, loss 0.00315736, acc 1\n",
      "2018-08-10T17:56:55.363028: step 902, loss 0.00427996, acc 1\n",
      "2018-08-10T17:56:55.521990: step 903, loss 0.00715113, acc 1\n",
      "2018-08-10T17:56:55.683207: step 904, loss 0.00116406, acc 1\n",
      "2018-08-10T17:56:55.844789: step 905, loss 0.00508218, acc 1\n",
      "2018-08-10T17:56:56.000946: step 906, loss 0.000542096, acc 1\n",
      "2018-08-10T17:56:56.159669: step 907, loss 0.000774924, acc 1\n",
      "2018-08-10T17:56:56.318040: step 908, loss 0.00108926, acc 1\n",
      "2018-08-10T17:56:56.475835: step 909, loss 0.000539341, acc 1\n",
      "2018-08-10T17:56:56.628847: step 910, loss 0.00400327, acc 1\n",
      "2018-08-10T17:56:56.789696: step 911, loss 0.00180287, acc 1\n",
      "2018-08-10T17:56:56.951663: step 912, loss 0.00192691, acc 1\n",
      "2018-08-10T17:56:57.107437: step 913, loss 0.00177412, acc 1\n",
      "2018-08-10T17:56:57.265926: step 914, loss 0.000769491, acc 1\n",
      "2018-08-10T17:56:57.423436: step 915, loss 0.000942397, acc 1\n",
      "2018-08-10T17:56:57.580726: step 916, loss 0.000967801, acc 1\n",
      "2018-08-10T17:56:57.748804: step 917, loss 0.00841692, acc 1\n",
      "2018-08-10T17:56:57.907364: step 918, loss 0.00156679, acc 1\n",
      "2018-08-10T17:56:58.065616: step 919, loss 0.000322094, acc 1\n",
      "2018-08-10T17:56:58.224099: step 920, loss 0.00739091, acc 1\n",
      "2018-08-10T17:56:58.381828: step 921, loss 0.000678158, acc 1\n",
      "2018-08-10T17:56:58.541120: step 922, loss 0.00385416, acc 1\n",
      "2018-08-10T17:56:58.700908: step 923, loss 0.000640047, acc 1\n",
      "2018-08-10T17:56:58.862592: step 924, loss 0.0103787, acc 1\n",
      "2018-08-10T17:56:59.019383: step 925, loss 0.00558175, acc 1\n",
      "2018-08-10T17:56:59.178352: step 926, loss 0.00180585, acc 1\n",
      "2018-08-10T17:56:59.336057: step 927, loss 0.00311005, acc 1\n",
      "2018-08-10T17:56:59.493707: step 928, loss 0.00107945, acc 1\n",
      "2018-08-10T17:56:59.651535: step 929, loss 0.0158951, acc 0.984375\n",
      "2018-08-10T17:56:59.812415: step 930, loss 0.000913363, acc 1\n",
      "2018-08-10T17:56:59.971983: step 931, loss 0.0403515, acc 0.984375\n",
      "2018-08-10T17:57:00.129644: step 932, loss 0.00204788, acc 1\n",
      "2018-08-10T17:57:00.288671: step 933, loss 0.00102047, acc 1\n",
      "2018-08-10T17:57:00.448626: step 934, loss 0.000470847, acc 1\n",
      "2018-08-10T17:57:00.608401: step 935, loss 0.00186919, acc 1\n",
      "2018-08-10T17:57:00.759323: step 936, loss 0.000548406, acc 1\n",
      "2018-08-10T17:57:00.913743: step 937, loss 0.00229751, acc 1\n",
      "2018-08-10T17:57:01.062563: step 938, loss 0.00476104, acc 1\n",
      "2018-08-10T17:57:01.214055: step 939, loss 0.00355873, acc 1\n",
      "2018-08-10T17:57:01.366560: step 940, loss 0.000692346, acc 1\n",
      "2018-08-10T17:57:01.522583: step 941, loss 0.00185858, acc 1\n",
      "2018-08-10T17:57:01.676052: step 942, loss 0.0047442, acc 1\n",
      "2018-08-10T17:57:01.834070: step 943, loss 0.00812206, acc 1\n",
      "2018-08-10T17:57:01.988245: step 944, loss 0.000379123, acc 1\n",
      "2018-08-10T17:57:02.144646: step 945, loss 0.00377323, acc 1\n",
      "2018-08-10T17:57:02.300965: step 946, loss 0.000506079, acc 1\n",
      "2018-08-10T17:57:02.459304: step 947, loss 0.000194349, acc 1\n",
      "2018-08-10T17:57:02.619245: step 948, loss 0.0019938, acc 1\n",
      "2018-08-10T17:57:02.773871: step 949, loss 0.00389325, acc 1\n",
      "2018-08-10T17:57:02.931959: step 950, loss 0.00140655, acc 1\n",
      "2018-08-10T17:57:03.089574: step 951, loss 0.00396001, acc 1\n",
      "2018-08-10T17:57:03.246917: step 952, loss 0.00103165, acc 1\n",
      "2018-08-10T17:57:03.402681: step 953, loss 0.000599864, acc 1\n",
      "2018-08-10T17:57:03.558293: step 954, loss 0.000580344, acc 1\n",
      "2018-08-10T17:57:03.717457: step 955, loss 0.00488331, acc 1\n",
      "2018-08-10T17:57:03.876052: step 956, loss 0.000202414, acc 1\n",
      "2018-08-10T17:57:04.033875: step 957, loss 0.000476128, acc 1\n",
      "2018-08-10T17:57:04.189387: step 958, loss 0.00395118, acc 1\n",
      "2018-08-10T17:57:04.346150: step 959, loss 0.00556541, acc 1\n",
      "2018-08-10T17:57:04.502401: step 960, loss 0.000230524, acc 1\n",
      "2018-08-10T17:57:04.659235: step 961, loss 0.000560016, acc 1\n",
      "2018-08-10T17:57:04.814196: step 962, loss 0.00821996, acc 1\n",
      "2018-08-10T17:57:04.971192: step 963, loss 0.00220427, acc 1\n",
      "2018-08-10T17:57:05.128965: step 964, loss 0.00339798, acc 1\n",
      "2018-08-10T17:57:05.285836: step 965, loss 0.00396738, acc 1\n",
      "2018-08-10T17:57:05.440942: step 966, loss 0.000354431, acc 1\n",
      "2018-08-10T17:57:05.595768: step 967, loss 0.00136214, acc 1\n",
      "2018-08-10T17:57:05.751904: step 968, loss 0.000749327, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:57:05.911954: step 969, loss 0.000407149, acc 1\n",
      "2018-08-10T17:57:06.068862: step 970, loss 0.00110227, acc 1\n",
      "2018-08-10T17:57:06.225365: step 971, loss 0.000265912, acc 1\n",
      "2018-08-10T17:57:06.380939: step 972, loss 0.00061391, acc 1\n",
      "2018-08-10T17:57:06.539280: step 973, loss 0.000691283, acc 1\n",
      "2018-08-10T17:57:06.710249: step 974, loss 0.00222519, acc 1\n",
      "2018-08-10T17:57:06.866017: step 975, loss 0.00712444, acc 1\n",
      "2018-08-10T17:57:07.021775: step 976, loss 0.00184729, acc 1\n",
      "2018-08-10T17:57:07.170635: step 977, loss 0.00296694, acc 1\n",
      "2018-08-10T17:57:07.318096: step 978, loss 0.00162319, acc 1\n",
      "2018-08-10T17:57:07.467439: step 979, loss 0.00260196, acc 1\n",
      "2018-08-10T17:57:07.615170: step 980, loss 0.000440138, acc 1\n",
      "2018-08-10T17:57:07.761679: step 981, loss 0.00130932, acc 1\n",
      "2018-08-10T17:57:07.910307: step 982, loss 0.000544848, acc 1\n",
      "2018-08-10T17:57:08.058964: step 983, loss 0.00064657, acc 1\n",
      "2018-08-10T17:57:08.204940: step 984, loss 0.0011791, acc 1\n",
      "2018-08-10T17:57:08.353414: step 985, loss 0.00395756, acc 1\n",
      "2018-08-10T17:57:08.502773: step 986, loss 0.000276811, acc 1\n",
      "2018-08-10T17:57:08.652871: step 987, loss 0.00485148, acc 1\n",
      "2018-08-10T17:57:08.800251: step 988, loss 0.00320599, acc 1\n",
      "2018-08-10T17:57:08.949460: step 989, loss 0.00269417, acc 1\n",
      "2018-08-10T17:57:09.097199: step 990, loss 0.00114602, acc 1\n",
      "2018-08-10T17:57:09.245936: step 991, loss 0.00737193, acc 1\n",
      "2018-08-10T17:57:09.394691: step 992, loss 0.00117646, acc 1\n",
      "2018-08-10T17:57:09.543170: step 993, loss 0.000365608, acc 1\n",
      "2018-08-10T17:57:09.690038: step 994, loss 0.00401738, acc 1\n",
      "2018-08-10T17:57:09.840471: step 995, loss 0.00270143, acc 1\n",
      "2018-08-10T17:57:09.990212: step 996, loss 0.000849928, acc 1\n",
      "2018-08-10T17:57:10.138670: step 997, loss 0.000868719, acc 1\n",
      "2018-08-10T17:57:10.285010: step 998, loss 0.000558453, acc 1\n",
      "2018-08-10T17:57:10.432569: step 999, loss 0.00155106, acc 1\n",
      "2018-08-10T17:57:10.582824: step 1000, loss 0.00209218, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:57:10.734922: step 1000, loss 0.698232, acc 0.856459\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533923672/checkpoints/model-1000\n",
      "\n",
      "2018-08-10T17:57:10.931862: step 1001, loss 0.000707326, acc 1\n",
      "2018-08-10T17:57:11.079750: step 1002, loss 0.0010751, acc 1\n",
      "2018-08-10T17:57:11.228623: step 1003, loss 0.00554082, acc 1\n",
      "2018-08-10T17:57:11.381371: step 1004, loss 0.00150899, acc 1\n",
      "2018-08-10T17:57:11.535093: step 1005, loss 0.000677387, acc 1\n",
      "2018-08-10T17:57:11.691934: step 1006, loss 0.000469094, acc 1\n",
      "2018-08-10T17:57:11.851958: step 1007, loss 0.00497895, acc 1\n",
      "2018-08-10T17:57:12.007557: step 1008, loss 0.000436815, acc 1\n",
      "2018-08-10T17:57:12.164872: step 1009, loss 0.00856311, acc 1\n",
      "2018-08-10T17:57:12.321657: step 1010, loss 0.00508779, acc 1\n",
      "2018-08-10T17:57:12.478177: step 1011, loss 0.00148076, acc 1\n",
      "2018-08-10T17:57:12.636198: step 1012, loss 0.00431101, acc 1\n",
      "2018-08-10T17:57:12.796765: step 1013, loss 0.00136245, acc 1\n",
      "2018-08-10T17:57:12.955772: step 1014, loss 0.00076022, acc 1\n",
      "2018-08-10T17:57:13.117007: step 1015, loss 0.00177971, acc 1\n",
      "2018-08-10T17:57:13.276242: step 1016, loss 0.00225917, acc 1\n",
      "2018-08-10T17:57:13.437194: step 1017, loss 0.00126707, acc 1\n",
      "2018-08-10T17:57:13.597849: step 1018, loss 0.000174913, acc 1\n",
      "2018-08-10T17:57:13.754724: step 1019, loss 0.00404785, acc 1\n",
      "2018-08-10T17:57:13.916050: step 1020, loss 0.000912344, acc 1\n",
      "2018-08-10T17:57:14.073109: step 1021, loss 0.000840334, acc 1\n",
      "2018-08-10T17:57:14.231922: step 1022, loss 0.000223152, acc 1\n",
      "2018-08-10T17:57:14.389474: step 1023, loss 0.0131111, acc 0.984375\n",
      "2018-08-10T17:57:14.550061: step 1024, loss 0.00491014, acc 1\n",
      "2018-08-10T17:57:14.707602: step 1025, loss 0.0303629, acc 0.984375\n",
      "2018-08-10T17:57:14.868691: step 1026, loss 0.00169164, acc 1\n",
      "2018-08-10T17:57:15.018079: step 1027, loss 0.000109809, acc 1\n",
      "2018-08-10T17:57:15.175415: step 1028, loss 0.000824542, acc 1\n",
      "2018-08-10T17:57:15.331602: step 1029, loss 0.00272409, acc 1\n",
      "2018-08-10T17:57:15.491370: step 1030, loss 0.000915709, acc 1\n",
      "2018-08-10T17:57:15.649481: step 1031, loss 0.00318715, acc 1\n",
      "2018-08-10T17:57:15.809324: step 1032, loss 0.00489543, acc 1\n",
      "2018-08-10T17:57:15.967719: step 1033, loss 0.00112335, acc 1\n",
      "2018-08-10T17:57:16.124233: step 1034, loss 0.000636636, acc 1\n",
      "2018-08-10T17:57:16.280263: step 1035, loss 0.00297335, acc 1\n",
      "2018-08-10T17:57:16.443193: step 1036, loss 0.00146035, acc 1\n",
      "2018-08-10T17:57:16.601631: step 1037, loss 0.0269637, acc 0.984375\n",
      "2018-08-10T17:57:16.758975: step 1038, loss 0.00305917, acc 1\n",
      "2018-08-10T17:57:16.919960: step 1039, loss 0.0135404, acc 0.984375\n",
      "2018-08-10T17:57:17.073613: step 1040, loss 0.000335207, acc 1\n",
      "2018-08-10T17:57:17.232414: step 1041, loss 0.00114044, acc 1\n",
      "2018-08-10T17:57:17.393441: step 1042, loss 0.00211487, acc 1\n",
      "2018-08-10T17:57:17.552659: step 1043, loss 0.0052448, acc 1\n",
      "2018-08-10T17:57:17.713375: step 1044, loss 0.00130385, acc 1\n",
      "2018-08-10T17:57:17.870624: step 1045, loss 0.000264831, acc 1\n",
      "2018-08-10T17:57:18.026328: step 1046, loss 0.00124759, acc 1\n",
      "2018-08-10T17:57:18.183400: step 1047, loss 0.0110839, acc 0.984375\n",
      "2018-08-10T17:57:18.342767: step 1048, loss 0.00596823, acc 1\n",
      "2018-08-10T17:57:18.501549: step 1049, loss 0.000980523, acc 1\n",
      "2018-08-10T17:57:18.658553: step 1050, loss 0.000676748, acc 1\n",
      "2018-08-10T17:57:18.817901: step 1051, loss 0.00557364, acc 1\n",
      "2018-08-10T17:57:18.975236: step 1052, loss 0.000554499, acc 1\n",
      "2018-08-10T17:57:19.127372: step 1053, loss 0.00361676, acc 1\n",
      "2018-08-10T17:57:19.294540: step 1054, loss 0.0016128, acc 1\n",
      "2018-08-10T17:57:19.452227: step 1055, loss 0.00219351, acc 1\n",
      "2018-08-10T17:57:19.612944: step 1056, loss 0.000859932, acc 1\n",
      "2018-08-10T17:57:19.769148: step 1057, loss 0.00965707, acc 1\n",
      "2018-08-10T17:57:19.928407: step 1058, loss 0.00273269, acc 1\n",
      "2018-08-10T17:57:20.084563: step 1059, loss 0.000307033, acc 1\n",
      "2018-08-10T17:57:20.240336: step 1060, loss 0.00190537, acc 1\n",
      "2018-08-10T17:57:20.397114: step 1061, loss 0.00203566, acc 1\n",
      "2018-08-10T17:57:20.556622: step 1062, loss 0.00569641, acc 1\n",
      "2018-08-10T17:57:20.716739: step 1063, loss 0.0024227, acc 1\n",
      "2018-08-10T17:57:20.875640: step 1064, loss 0.00231053, acc 1\n",
      "2018-08-10T17:57:21.037090: step 1065, loss 0.000685391, acc 1\n",
      "2018-08-10T17:57:21.190412: step 1066, loss 0.000370114, acc 1\n",
      "2018-08-10T17:57:21.346477: step 1067, loss 0.00130164, acc 1\n",
      "2018-08-10T17:57:21.506202: step 1068, loss 0.00542653, acc 1\n",
      "2018-08-10T17:57:21.664449: step 1069, loss 0.000467416, acc 1\n",
      "2018-08-10T17:57:21.819655: step 1070, loss 0.000207639, acc 1\n",
      "2018-08-10T17:57:21.969469: step 1071, loss 0.000447646, acc 1\n",
      "2018-08-10T17:57:22.118716: step 1072, loss 0.00963782, acc 1\n",
      "2018-08-10T17:57:22.270761: step 1073, loss 0.00187329, acc 1\n",
      "2018-08-10T17:57:22.424390: step 1074, loss 0.00609752, acc 1\n",
      "2018-08-10T17:57:22.581683: step 1075, loss 0.0109085, acc 1\n",
      "2018-08-10T17:57:22.742388: step 1076, loss 0.000820748, acc 1\n",
      "2018-08-10T17:57:22.901292: step 1077, loss 0.00152367, acc 1\n",
      "2018-08-10T17:57:23.058233: step 1078, loss 0.000439679, acc 1\n",
      "2018-08-10T17:57:23.210434: step 1079, loss 0.00307444, acc 1\n",
      "2018-08-10T17:57:23.371068: step 1080, loss 0.00569512, acc 1\n",
      "2018-08-10T17:57:23.528146: step 1081, loss 0.00403024, acc 1\n",
      "2018-08-10T17:57:23.683675: step 1082, loss 0.0112561, acc 1\n",
      "2018-08-10T17:57:23.844620: step 1083, loss 0.000955538, acc 1\n",
      "2018-08-10T17:57:24.001735: step 1084, loss 0.00532407, acc 1\n",
      "2018-08-10T17:57:24.158989: step 1085, loss 0.00920396, acc 1\n",
      "2018-08-10T17:57:24.317849: step 1086, loss 0.00189491, acc 1\n",
      "2018-08-10T17:57:24.478322: step 1087, loss 0.0757281, acc 0.96875\n",
      "2018-08-10T17:57:24.638686: step 1088, loss 0.00268964, acc 1\n",
      "2018-08-10T17:57:24.796793: step 1089, loss 0.000483914, acc 1\n",
      "2018-08-10T17:57:24.952472: step 1090, loss 0.00298243, acc 1\n",
      "2018-08-10T17:57:25.107728: step 1091, loss 0.00123155, acc 1\n",
      "2018-08-10T17:57:25.262198: step 1092, loss 0.00389113, acc 1\n",
      "2018-08-10T17:57:25.417291: step 1093, loss 0.00861434, acc 1\n",
      "2018-08-10T17:57:25.573244: step 1094, loss 0.00219715, acc 1\n",
      "2018-08-10T17:57:25.728734: step 1095, loss 0.000612617, acc 1\n",
      "2018-08-10T17:57:25.887246: step 1096, loss 0.023395, acc 1\n",
      "2018-08-10T17:57:26.043040: step 1097, loss 0.0394987, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:57:26.200335: step 1098, loss 0.000738233, acc 1\n",
      "2018-08-10T17:57:26.357588: step 1099, loss 0.00547736, acc 1\n",
      "2018-08-10T17:57:26.520860: step 1100, loss 0.00217281, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:57:26.678066: step 1100, loss 0.558474, acc 0.866029\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533923672/checkpoints/model-1100\n",
      "\n",
      "2018-08-10T17:57:26.898473: step 1101, loss 0.00199706, acc 1\n",
      "2018-08-10T17:57:27.053726: step 1102, loss 0.0147058, acc 0.984375\n",
      "2018-08-10T17:57:27.211670: step 1103, loss 0.000753284, acc 1\n",
      "2018-08-10T17:57:27.370596: step 1104, loss 0.00287121, acc 1\n",
      "2018-08-10T17:57:27.523482: step 1105, loss 0.0016366, acc 1\n",
      "2018-08-10T17:57:27.690412: step 1106, loss 0.000241881, acc 1\n",
      "2018-08-10T17:57:27.855441: step 1107, loss 0.000861747, acc 1\n",
      "2018-08-10T17:57:28.007182: step 1108, loss 0.000567752, acc 1\n",
      "2018-08-10T17:57:28.156168: step 1109, loss 0.000357626, acc 1\n",
      "2018-08-10T17:57:28.304856: step 1110, loss 0.00189541, acc 1\n",
      "2018-08-10T17:57:28.454729: step 1111, loss 0.00803744, acc 1\n",
      "2018-08-10T17:57:28.607222: step 1112, loss 0.0218713, acc 0.984375\n",
      "2018-08-10T17:57:28.754226: step 1113, loss 0.00216377, acc 1\n",
      "2018-08-10T17:57:28.903048: step 1114, loss 0.00466456, acc 1\n",
      "2018-08-10T17:57:29.056892: step 1115, loss 0.0210147, acc 1\n",
      "2018-08-10T17:57:29.204342: step 1116, loss 0.00201521, acc 1\n",
      "2018-08-10T17:57:29.351496: step 1117, loss 0.0044866, acc 1\n",
      "2018-08-10T17:57:29.495923: step 1118, loss 0.000399254, acc 1\n",
      "2018-08-10T17:57:29.644330: step 1119, loss 0.000943626, acc 1\n",
      "2018-08-10T17:57:29.792861: step 1120, loss 0.00146976, acc 1\n",
      "2018-08-10T17:57:29.942402: step 1121, loss 0.000228671, acc 1\n",
      "2018-08-10T17:57:30.089558: step 1122, loss 0.000124164, acc 1\n",
      "2018-08-10T17:57:30.236485: step 1123, loss 0.000642528, acc 1\n",
      "2018-08-10T17:57:30.385753: step 1124, loss 0.00102973, acc 1\n",
      "2018-08-10T17:57:30.538488: step 1125, loss 0.00200028, acc 1\n",
      "2018-08-10T17:57:30.689543: step 1126, loss 0.0152685, acc 0.984375\n",
      "2018-08-10T17:57:30.838078: step 1127, loss 0.000675131, acc 1\n",
      "2018-08-10T17:57:30.985861: step 1128, loss 0.00344593, acc 1\n",
      "2018-08-10T17:57:31.134917: step 1129, loss 0.00157656, acc 1\n",
      "2018-08-10T17:57:31.282620: step 1130, loss 0.00348279, acc 1\n",
      "2018-08-10T17:57:31.428880: step 1131, loss 0.00540018, acc 1\n",
      "2018-08-10T17:57:31.579344: step 1132, loss 0.00293267, acc 1\n",
      "2018-08-10T17:57:31.725930: step 1133, loss 0.000347259, acc 1\n",
      "2018-08-10T17:57:31.875697: step 1134, loss 0.0013599, acc 1\n",
      "2018-08-10T17:57:32.023410: step 1135, loss 0.000386172, acc 1\n",
      "2018-08-10T17:57:32.171779: step 1136, loss 0.00334601, acc 1\n",
      "2018-08-10T17:57:32.321001: step 1137, loss 0.00108741, acc 1\n",
      "2018-08-10T17:57:32.475338: step 1138, loss 0.00128363, acc 1\n",
      "2018-08-10T17:57:32.630272: step 1139, loss 0.000284229, acc 1\n",
      "2018-08-10T17:57:32.786593: step 1140, loss 0.00321055, acc 1\n",
      "2018-08-10T17:57:32.944954: step 1141, loss 0.000661695, acc 1\n",
      "2018-08-10T17:57:33.103414: step 1142, loss 0.0139795, acc 0.984375\n",
      "2018-08-10T17:57:33.263712: step 1143, loss 0.00154184, acc 1\n",
      "2018-08-10T17:57:33.417621: step 1144, loss 0.000933145, acc 1\n",
      "2018-08-10T17:57:33.574156: step 1145, loss 0.00106911, acc 1\n",
      "2018-08-10T17:57:33.728772: step 1146, loss 0.00062206, acc 1\n",
      "2018-08-10T17:57:33.888526: step 1147, loss 0.000400474, acc 1\n",
      "2018-08-10T17:57:34.045285: step 1148, loss 0.000546101, acc 1\n",
      "2018-08-10T17:57:34.201690: step 1149, loss 0.000752212, acc 1\n",
      "2018-08-10T17:57:34.358854: step 1150, loss 0.000168944, acc 1\n",
      "2018-08-10T17:57:34.516314: step 1151, loss 0.00589617, acc 1\n",
      "2018-08-10T17:57:34.672858: step 1152, loss 0.00420873, acc 1\n",
      "2018-08-10T17:57:34.829993: step 1153, loss 0.000455856, acc 1\n",
      "2018-08-10T17:57:34.988242: step 1154, loss 0.00207409, acc 1\n",
      "2018-08-10T17:57:35.148399: step 1155, loss 0.00248661, acc 1\n",
      "2018-08-10T17:57:35.306619: step 1156, loss 0.000705534, acc 1\n",
      "2018-08-10T17:57:35.458047: step 1157, loss 0.00189593, acc 1\n",
      "2018-08-10T17:57:35.614954: step 1158, loss 0.000241392, acc 1\n",
      "2018-08-10T17:57:35.771800: step 1159, loss 0.00119525, acc 1\n",
      "2018-08-10T17:57:35.930697: step 1160, loss 0.000640777, acc 1\n",
      "2018-08-10T17:57:36.089993: step 1161, loss 0.000650559, acc 1\n",
      "2018-08-10T17:57:36.248468: step 1162, loss 0.00157407, acc 1\n",
      "2018-08-10T17:57:36.406230: step 1163, loss 0.00565912, acc 1\n",
      "2018-08-10T17:57:36.564695: step 1164, loss 0.000348185, acc 1\n",
      "2018-08-10T17:57:36.723787: step 1165, loss 0.000597, acc 1\n",
      "2018-08-10T17:57:36.883347: step 1166, loss 0.0272197, acc 0.984375\n",
      "2018-08-10T17:57:37.041427: step 1167, loss 0.000855923, acc 1\n",
      "2018-08-10T17:57:37.200385: step 1168, loss 0.00127446, acc 1\n",
      "2018-08-10T17:57:37.354773: step 1169, loss 0.000251149, acc 1\n",
      "2018-08-10T17:57:37.507323: step 1170, loss 0.000982899, acc 1\n",
      "2018-08-10T17:57:37.664198: step 1171, loss 0.00119247, acc 1\n",
      "2018-08-10T17:57:37.823136: step 1172, loss 0.0273888, acc 0.984375\n",
      "2018-08-10T17:57:37.977258: step 1173, loss 0.000629475, acc 1\n",
      "2018-08-10T17:57:38.133080: step 1174, loss 0.00276046, acc 1\n",
      "2018-08-10T17:57:38.291149: step 1175, loss 0.00307414, acc 1\n",
      "2018-08-10T17:57:38.450046: step 1176, loss 0.00186782, acc 1\n",
      "2018-08-10T17:57:38.604938: step 1177, loss 0.00148957, acc 1\n",
      "2018-08-10T17:57:38.771355: step 1178, loss 0.00185059, acc 1\n",
      "2018-08-10T17:57:38.928668: step 1179, loss 0.000516105, acc 1\n",
      "2018-08-10T17:57:39.084466: step 1180, loss 0.000965375, acc 1\n",
      "2018-08-10T17:57:39.241171: step 1181, loss 0.0105095, acc 1\n",
      "2018-08-10T17:57:39.398697: step 1182, loss 0.0125014, acc 0.984375\n",
      "2018-08-10T17:57:39.551939: step 1183, loss 0.000787268, acc 1\n",
      "2018-08-10T17:57:39.706984: step 1184, loss 0.000438017, acc 1\n",
      "2018-08-10T17:57:39.865551: step 1185, loss 0.000851635, acc 1\n",
      "2018-08-10T17:57:40.023343: step 1186, loss 0.00612308, acc 1\n",
      "2018-08-10T17:57:40.179100: step 1187, loss 0.000608004, acc 1\n",
      "2018-08-10T17:57:40.335142: step 1188, loss 0.000598209, acc 1\n",
      "2018-08-10T17:57:40.490554: step 1189, loss 0.0114206, acc 1\n",
      "2018-08-10T17:57:40.649329: step 1190, loss 0.000473146, acc 1\n",
      "2018-08-10T17:57:40.804363: step 1191, loss 0.000606377, acc 1\n",
      "2018-08-10T17:57:40.962395: step 1192, loss 0.00377387, acc 1\n",
      "2018-08-10T17:57:41.115845: step 1193, loss 0.00754501, acc 1\n",
      "2018-08-10T17:57:41.271816: step 1194, loss 0.0010098, acc 1\n",
      "2018-08-10T17:57:41.427630: step 1195, loss 0.000201008, acc 1\n",
      "2018-08-10T17:57:41.579334: step 1196, loss 0.00164985, acc 1\n",
      "2018-08-10T17:57:41.737221: step 1197, loss 0.000512739, acc 1\n",
      "2018-08-10T17:57:41.896044: step 1198, loss 0.000518164, acc 1\n",
      "2018-08-10T17:57:42.054294: step 1199, loss 0.000703685, acc 1\n",
      "2018-08-10T17:57:42.211829: step 1200, loss 0.0077696, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:57:42.361779: step 1200, loss 0.688564, acc 0.870813\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533923672/checkpoints/model-1200\n",
      "\n",
      "2018-08-10T17:57:42.580795: step 1201, loss 0.0105199, acc 1\n",
      "2018-08-10T17:57:42.733770: step 1202, loss 0.00421749, acc 1\n",
      "2018-08-10T17:57:42.885575: step 1203, loss 0.00118676, acc 1\n",
      "2018-08-10T17:57:43.034846: step 1204, loss 0.000525669, acc 1\n",
      "2018-08-10T17:57:43.184023: step 1205, loss 0.000309763, acc 1\n",
      "2018-08-10T17:57:43.334277: step 1206, loss 0.00258419, acc 1\n",
      "2018-08-10T17:57:43.489099: step 1207, loss 0.000556576, acc 1\n",
      "2018-08-10T17:57:43.643125: step 1208, loss 0.00357459, acc 1\n",
      "2018-08-10T17:57:43.793281: step 1209, loss 0.00133136, acc 1\n",
      "2018-08-10T17:57:43.948287: step 1210, loss 0.00139378, acc 1\n",
      "2018-08-10T17:57:44.106098: step 1211, loss 0.00113356, acc 1\n",
      "2018-08-10T17:57:44.258960: step 1212, loss 0.00335839, acc 1\n",
      "2018-08-10T17:57:44.415725: step 1213, loss 0.0815913, acc 0.96875\n",
      "2018-08-10T17:57:44.572845: step 1214, loss 0.00186128, acc 1\n",
      "2018-08-10T17:57:44.735604: step 1215, loss 0.000430232, acc 1\n",
      "2018-08-10T17:57:44.896819: step 1216, loss 0.00258804, acc 1\n",
      "2018-08-10T17:57:45.054835: step 1217, loss 0.00347106, acc 1\n",
      "2018-08-10T17:57:45.213275: step 1218, loss 0.00098208, acc 1\n",
      "2018-08-10T17:57:45.370686: step 1219, loss 0.000245308, acc 1\n",
      "2018-08-10T17:57:45.526174: step 1220, loss 0.000356475, acc 1\n",
      "2018-08-10T17:57:45.681991: step 1221, loss 0.00143938, acc 1\n",
      "2018-08-10T17:57:45.836279: step 1222, loss 0.01213, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:57:45.994722: step 1223, loss 0.033235, acc 0.984375\n",
      "2018-08-10T17:57:46.151613: step 1224, loss 0.000895393, acc 1\n",
      "2018-08-10T17:57:46.306437: step 1225, loss 0.000427051, acc 1\n",
      "2018-08-10T17:57:46.466644: step 1226, loss 0.0623529, acc 0.984375\n",
      "2018-08-10T17:57:46.622973: step 1227, loss 0.00236227, acc 1\n",
      "2018-08-10T17:57:46.780621: step 1228, loss 0.000790867, acc 1\n",
      "2018-08-10T17:57:46.939192: step 1229, loss 0.00107193, acc 1\n",
      "2018-08-10T17:57:47.094036: step 1230, loss 0.00298498, acc 1\n",
      "2018-08-10T17:57:47.251137: step 1231, loss 0.000193772, acc 1\n",
      "2018-08-10T17:57:47.407879: step 1232, loss 0.000788828, acc 1\n",
      "2018-08-10T17:57:47.565354: step 1233, loss 0.0209502, acc 0.984375\n",
      "2018-08-10T17:57:47.721827: step 1234, loss 0.00205189, acc 1\n",
      "2018-08-10T17:57:47.876216: step 1235, loss 0.000836019, acc 1\n",
      "2018-08-10T17:57:48.032211: step 1236, loss 0.00053605, acc 1\n",
      "2018-08-10T17:57:48.191979: step 1237, loss 0.000234267, acc 1\n",
      "2018-08-10T17:57:48.347295: step 1238, loss 0.0017895, acc 1\n",
      "2018-08-10T17:57:48.502808: step 1239, loss 0.000318048, acc 1\n",
      "2018-08-10T17:57:48.661972: step 1240, loss 0.00108055, acc 1\n",
      "2018-08-10T17:57:48.817429: step 1241, loss 0.00180491, acc 1\n",
      "2018-08-10T17:57:48.965962: step 1242, loss 0.000566689, acc 1\n",
      "2018-08-10T17:57:49.114999: step 1243, loss 0.000477186, acc 1\n",
      "2018-08-10T17:57:49.262745: step 1244, loss 0.00120228, acc 1\n",
      "2018-08-10T17:57:49.412602: step 1245, loss 0.000691684, acc 1\n",
      "2018-08-10T17:57:49.560885: step 1246, loss 0.00514744, acc 1\n",
      "2018-08-10T17:57:49.708462: step 1247, loss 0.000860828, acc 1\n",
      "2018-08-10T17:57:49.854671: step 1248, loss 0.000207004, acc 1\n",
      "2018-08-10T17:57:50.002235: step 1249, loss 0.00117924, acc 1\n",
      "2018-08-10T17:57:50.147706: step 1250, loss 0.000551516, acc 1\n",
      "2018-08-10T17:57:50.296070: step 1251, loss 0.00116581, acc 1\n",
      "2018-08-10T17:57:50.448352: step 1252, loss 0.000628131, acc 1\n",
      "2018-08-10T17:57:50.596272: step 1253, loss 0.0446484, acc 0.984375\n",
      "2018-08-10T17:57:50.743921: step 1254, loss 0.00255318, acc 1\n",
      "2018-08-10T17:57:50.893359: step 1255, loss 0.000810425, acc 1\n",
      "2018-08-10T17:57:51.041583: step 1256, loss 0.00259286, acc 1\n",
      "2018-08-10T17:57:51.188463: step 1257, loss 0.000292488, acc 1\n",
      "2018-08-10T17:57:51.334266: step 1258, loss 0.00117945, acc 1\n",
      "2018-08-10T17:57:51.481633: step 1259, loss 0.000752032, acc 1\n",
      "2018-08-10T17:57:51.629358: step 1260, loss 0.00217452, acc 1\n",
      "2018-08-10T17:57:51.774164: step 1261, loss 0.00165261, acc 1\n",
      "2018-08-10T17:57:51.922886: step 1262, loss 0.00105724, acc 1\n",
      "2018-08-10T17:57:52.071787: step 1263, loss 0.000605289, acc 1\n",
      "2018-08-10T17:57:52.217742: step 1264, loss 0.00282084, acc 1\n",
      "2018-08-10T17:57:52.366993: step 1265, loss 0.0016922, acc 1\n",
      "2018-08-10T17:57:52.519530: step 1266, loss 0.00270995, acc 1\n",
      "2018-08-10T17:57:52.668985: step 1267, loss 0.00339754, acc 1\n",
      "2018-08-10T17:57:52.816957: step 1268, loss 0.00241097, acc 1\n",
      "2018-08-10T17:57:52.967277: step 1269, loss 0.000599337, acc 1\n",
      "2018-08-10T17:57:53.119265: step 1270, loss 0.020508, acc 0.984375\n",
      "2018-08-10T17:57:53.272723: step 1271, loss 0.0025391, acc 1\n",
      "2018-08-10T17:57:53.427230: step 1272, loss 0.00206277, acc 1\n",
      "2018-08-10T17:57:53.581646: step 1273, loss 0.0006614, acc 1\n",
      "2018-08-10T17:57:53.734157: step 1274, loss 0.00336383, acc 1\n",
      "2018-08-10T17:57:53.890996: step 1275, loss 0.000698956, acc 1\n",
      "2018-08-10T17:57:54.049640: step 1276, loss 0.00472947, acc 1\n",
      "2018-08-10T17:57:54.206413: step 1277, loss 0.0538717, acc 0.984375\n",
      "2018-08-10T17:57:54.365732: step 1278, loss 0.00401968, acc 1\n",
      "2018-08-10T17:57:54.523572: step 1279, loss 0.000627636, acc 1\n",
      "2018-08-10T17:57:54.685468: step 1280, loss 0.0147262, acc 0.984375\n",
      "2018-08-10T17:57:54.843657: step 1281, loss 0.00244913, acc 1\n",
      "2018-08-10T17:57:55.001644: step 1282, loss 0.00325797, acc 1\n",
      "2018-08-10T17:57:55.156644: step 1283, loss 0.000188628, acc 1\n",
      "2018-08-10T17:57:55.312825: step 1284, loss 0.000246624, acc 1\n",
      "2018-08-10T17:57:55.471145: step 1285, loss 0.00826338, acc 1\n",
      "2018-08-10T17:57:55.626711: step 1286, loss 0.000610909, acc 1\n",
      "2018-08-10T17:57:55.781962: step 1287, loss 0.000142282, acc 1\n",
      "2018-08-10T17:57:55.943179: step 1288, loss 0.00178607, acc 1\n",
      "2018-08-10T17:57:56.100731: step 1289, loss 0.00134208, acc 1\n",
      "2018-08-10T17:57:56.259183: step 1290, loss 0.000608549, acc 1\n",
      "2018-08-10T17:57:56.416161: step 1291, loss 0.00292662, acc 1\n",
      "2018-08-10T17:57:56.574814: step 1292, loss 0.00137956, acc 1\n",
      "2018-08-10T17:57:56.735104: step 1293, loss 0.000783475, acc 1\n",
      "2018-08-10T17:57:56.895445: step 1294, loss 0.000562547, acc 1\n",
      "2018-08-10T17:57:57.051853: step 1295, loss 0.000663882, acc 1\n",
      "2018-08-10T17:57:57.210909: step 1296, loss 0.0002178, acc 1\n",
      "2018-08-10T17:57:57.368296: step 1297, loss 0.000537988, acc 1\n",
      "2018-08-10T17:57:57.524826: step 1298, loss 0.0006932, acc 1\n",
      "2018-08-10T17:57:57.681173: step 1299, loss 0.00192468, acc 1\n",
      "2018-08-10T17:57:57.836286: step 1300, loss 0.00957531, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:57:57.990876: step 1300, loss 0.685789, acc 0.875598\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533923672/checkpoints/model-1300\n",
      "\n",
      "2018-08-10T17:57:58.206054: step 1301, loss 0.0598016, acc 0.984375\n",
      "2018-08-10T17:57:58.359890: step 1302, loss 0.000562842, acc 1\n",
      "2018-08-10T17:57:58.520250: step 1303, loss 0.00256404, acc 1\n",
      "2018-08-10T17:57:58.681283: step 1304, loss 0.00587186, acc 1\n",
      "2018-08-10T17:57:58.839910: step 1305, loss 0.000828689, acc 1\n",
      "2018-08-10T17:57:58.997432: step 1306, loss 0.0206787, acc 0.984375\n",
      "2018-08-10T17:57:59.156934: step 1307, loss 0.00091944, acc 1\n",
      "2018-08-10T17:57:59.314357: step 1308, loss 0.0018223, acc 1\n",
      "2018-08-10T17:57:59.472924: step 1309, loss 0.000658491, acc 1\n",
      "2018-08-10T17:57:59.628973: step 1310, loss 0.00601598, acc 1\n",
      "2018-08-10T17:57:59.786890: step 1311, loss 0.00161946, acc 1\n",
      "2018-08-10T17:57:59.942684: step 1312, loss 0.000389976, acc 1\n",
      "2018-08-10T17:58:00.095412: step 1313, loss 0.000109901, acc 1\n",
      "2018-08-10T17:58:00.252790: step 1314, loss 0.00129577, acc 1\n",
      "2018-08-10T17:58:00.410027: step 1315, loss 0.00346403, acc 1\n",
      "2018-08-10T17:58:00.570103: step 1316, loss 0.00504313, acc 1\n",
      "2018-08-10T17:58:00.731156: step 1317, loss 0.00132449, acc 1\n",
      "2018-08-10T17:58:00.893657: step 1318, loss 0.000176846, acc 1\n",
      "2018-08-10T17:58:01.051639: step 1319, loss 0.0065478, acc 1\n",
      "2018-08-10T17:58:01.209411: step 1320, loss 0.000194694, acc 1\n",
      "2018-08-10T17:58:01.368998: step 1321, loss 0.000159123, acc 1\n",
      "2018-08-10T17:58:01.526602: step 1322, loss 0.00701215, acc 1\n",
      "2018-08-10T17:58:01.685218: step 1323, loss 0.000793643, acc 1\n",
      "2018-08-10T17:58:01.844303: step 1324, loss 0.000343018, acc 1\n",
      "2018-08-10T17:58:02.005161: step 1325, loss 0.00192206, acc 1\n",
      "2018-08-10T17:58:02.160738: step 1326, loss 0.000185261, acc 1\n",
      "2018-08-10T17:58:02.315139: step 1327, loss 0.00097399, acc 1\n",
      "2018-08-10T17:58:02.474231: step 1328, loss 0.000230246, acc 1\n",
      "2018-08-10T17:58:02.637573: step 1329, loss 0.000690441, acc 1\n",
      "2018-08-10T17:58:02.796053: step 1330, loss 0.00513644, acc 1\n",
      "2018-08-10T17:58:02.951217: step 1331, loss 0.000822144, acc 1\n",
      "2018-08-10T17:58:03.107958: step 1332, loss 0.00108036, acc 1\n",
      "2018-08-10T17:58:03.269139: step 1333, loss 0.0069408, acc 1\n",
      "2018-08-10T17:58:03.425622: step 1334, loss 0.00141918, acc 1\n",
      "2018-08-10T17:58:03.578913: step 1335, loss 0.000218082, acc 1\n",
      "2018-08-10T17:58:03.730586: step 1336, loss 0.000313885, acc 1\n",
      "2018-08-10T17:58:03.880835: step 1337, loss 0.00132174, acc 1\n",
      "2018-08-10T17:58:04.032267: step 1338, loss 0.0104358, acc 1\n",
      "2018-08-10T17:58:04.185150: step 1339, loss 0.00261992, acc 1\n",
      "2018-08-10T17:58:04.341573: step 1340, loss 0.0123511, acc 1\n",
      "2018-08-10T17:58:04.498768: step 1341, loss 0.00247484, acc 1\n",
      "2018-08-10T17:58:04.658948: step 1342, loss 0.00330211, acc 1\n",
      "2018-08-10T17:58:04.818633: step 1343, loss 0.00230639, acc 1\n",
      "2018-08-10T17:58:04.975453: step 1344, loss 0.000788833, acc 1\n",
      "2018-08-10T17:58:05.134807: step 1345, loss 0.00120159, acc 1\n",
      "2018-08-10T17:58:05.293052: step 1346, loss 0.000397855, acc 1\n",
      "2018-08-10T17:58:05.449916: step 1347, loss 0.000247397, acc 1\n",
      "2018-08-10T17:58:05.606776: step 1348, loss 0.000906145, acc 1\n",
      "2018-08-10T17:58:05.765587: step 1349, loss 0.0259434, acc 0.984375\n",
      "2018-08-10T17:58:05.928749: step 1350, loss 0.000758868, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:58:06.085301: step 1351, loss 0.000939093, acc 1\n",
      "2018-08-10T17:58:06.239704: step 1352, loss 0.00093876, acc 1\n",
      "2018-08-10T17:58:06.397204: step 1353, loss 0.000539584, acc 1\n",
      "2018-08-10T17:58:06.553773: step 1354, loss 0.0060287, acc 1\n",
      "2018-08-10T17:58:06.714141: step 1355, loss 0.000429891, acc 1\n",
      "2018-08-10T17:58:06.873625: step 1356, loss 0.00200345, acc 1\n",
      "2018-08-10T17:58:07.029970: step 1357, loss 0.000383765, acc 1\n",
      "2018-08-10T17:58:07.186483: step 1358, loss 0.00040634, acc 1\n",
      "2018-08-10T17:58:07.343418: step 1359, loss 0.000301046, acc 1\n",
      "2018-08-10T17:58:07.498942: step 1360, loss 0.00189423, acc 1\n",
      "2018-08-10T17:58:07.658134: step 1361, loss 0.0114493, acc 1\n",
      "2018-08-10T17:58:07.818597: step 1362, loss 0.000185913, acc 1\n",
      "2018-08-10T17:58:07.977132: step 1363, loss 0.00026378, acc 1\n",
      "2018-08-10T17:58:08.135563: step 1364, loss 0.00231079, acc 1\n",
      "2018-08-10T17:58:08.288167: step 1365, loss 0.0106091, acc 1\n",
      "2018-08-10T17:58:08.449276: step 1366, loss 0.00528927, acc 1\n",
      "2018-08-10T17:58:08.605828: step 1367, loss 0.00192723, acc 1\n",
      "2018-08-10T17:58:08.761572: step 1368, loss 0.000763061, acc 1\n",
      "2018-08-10T17:58:08.920904: step 1369, loss 0.000593523, acc 1\n",
      "2018-08-10T17:58:09.078713: step 1370, loss 9.7766e-05, acc 1\n",
      "2018-08-10T17:58:09.240331: step 1371, loss 0.00078587, acc 1\n",
      "2018-08-10T17:58:09.396550: step 1372, loss 0.000861235, acc 1\n",
      "2018-08-10T17:58:09.552345: step 1373, loss 0.00284949, acc 1\n",
      "2018-08-10T17:58:09.705050: step 1374, loss 0.00119253, acc 1\n",
      "2018-08-10T17:58:09.856353: step 1375, loss 0.0010452, acc 1\n",
      "2018-08-10T17:58:10.008268: step 1376, loss 0.0277318, acc 0.984375\n",
      "2018-08-10T17:58:10.157137: step 1377, loss 0.00707991, acc 1\n",
      "2018-08-10T17:58:10.303931: step 1378, loss 0.000467913, acc 1\n",
      "2018-08-10T17:58:10.452870: step 1379, loss 0.00215812, acc 1\n",
      "2018-08-10T17:58:10.603037: step 1380, loss 0.000134554, acc 1\n",
      "2018-08-10T17:58:10.751588: step 1381, loss 0.000107623, acc 1\n",
      "2018-08-10T17:58:10.905374: step 1382, loss 0.000299229, acc 1\n",
      "2018-08-10T17:58:11.056196: step 1383, loss 0.00100137, acc 1\n",
      "2018-08-10T17:58:11.203595: step 1384, loss 0.000131825, acc 1\n",
      "2018-08-10T17:58:11.352820: step 1385, loss 0.00467304, acc 1\n",
      "2018-08-10T17:58:11.502424: step 1386, loss 0.000146592, acc 1\n",
      "2018-08-10T17:58:11.650181: step 1387, loss 0.00209712, acc 1\n",
      "2018-08-10T17:58:11.797988: step 1388, loss 0.000833296, acc 1\n",
      "2018-08-10T17:58:11.945083: step 1389, loss 0.00790921, acc 1\n",
      "2018-08-10T17:58:12.095374: step 1390, loss 0.000940023, acc 1\n",
      "2018-08-10T17:58:12.239653: step 1391, loss 0.000149825, acc 1\n",
      "2018-08-10T17:58:12.385759: step 1392, loss 0.021689, acc 0.984375\n",
      "2018-08-10T17:58:12.539408: step 1393, loss 0.00427663, acc 1\n",
      "2018-08-10T17:58:12.688508: step 1394, loss 7.26195e-05, acc 1\n",
      "2018-08-10T17:58:12.840516: step 1395, loss 0.00829493, acc 1\n",
      "2018-08-10T17:58:12.986637: step 1396, loss 0.00059944, acc 1\n",
      "2018-08-10T17:58:13.135049: step 1397, loss 0.000670563, acc 1\n",
      "2018-08-10T17:58:13.283526: step 1398, loss 0.000544793, acc 1\n",
      "2018-08-10T17:58:13.432055: step 1399, loss 0.00497655, acc 1\n",
      "2018-08-10T17:58:13.579292: step 1400, loss 0.00127674, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:58:13.723268: step 1400, loss 0.63371, acc 0.875598\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533923672/checkpoints/model-1400\n",
      "\n",
      "2018-08-10T17:58:13.926318: step 1401, loss 0.000898232, acc 1\n",
      "2018-08-10T17:58:14.076557: step 1402, loss 0.000289723, acc 1\n",
      "2018-08-10T17:58:14.228125: step 1403, loss 0.00123563, acc 1\n",
      "2018-08-10T17:58:14.376944: step 1404, loss 0.00113123, acc 1\n",
      "2018-08-10T17:58:14.537085: step 1405, loss 0.000604606, acc 1\n",
      "2018-08-10T17:58:14.695660: step 1406, loss 0.000480979, acc 1\n",
      "2018-08-10T17:58:14.855214: step 1407, loss 0.000176565, acc 1\n",
      "2018-08-10T17:58:15.011752: step 1408, loss 0.000491553, acc 1\n",
      "2018-08-10T17:58:15.166378: step 1409, loss 0.00191163, acc 1\n",
      "2018-08-10T17:58:15.322792: step 1410, loss 0.00183462, acc 1\n",
      "2018-08-10T17:58:15.479870: step 1411, loss 0.000987347, acc 1\n",
      "2018-08-10T17:58:15.633946: step 1412, loss 0.00147992, acc 1\n",
      "2018-08-10T17:58:15.791399: step 1413, loss 0.000584229, acc 1\n",
      "2018-08-10T17:58:15.950728: step 1414, loss 0.00774087, acc 1\n",
      "2018-08-10T17:58:16.108866: step 1415, loss 0.000368425, acc 1\n",
      "2018-08-10T17:58:16.265221: step 1416, loss 0.000809941, acc 1\n",
      "2018-08-10T17:58:16.418754: step 1417, loss 0.000111244, acc 1\n",
      "2018-08-10T17:58:16.577965: step 1418, loss 0.00105248, acc 1\n",
      "2018-08-10T17:58:16.735389: step 1419, loss 0.000619757, acc 1\n",
      "2018-08-10T17:58:16.892871: step 1420, loss 0.000628839, acc 1\n",
      "2018-08-10T17:58:17.048739: step 1421, loss 0.000955416, acc 1\n",
      "2018-08-10T17:58:17.205650: step 1422, loss 0.0167457, acc 0.984375\n",
      "2018-08-10T17:58:17.361326: step 1423, loss 0.000549018, acc 1\n",
      "2018-08-10T17:58:17.515110: step 1424, loss 0.00607453, acc 1\n",
      "2018-08-10T17:58:17.671646: step 1425, loss 0.000179314, acc 1\n",
      "2018-08-10T17:58:17.826820: step 1426, loss 0.000318638, acc 1\n",
      "2018-08-10T17:58:17.983902: step 1427, loss 0.00106619, acc 1\n",
      "2018-08-10T17:58:18.139397: step 1428, loss 0.000719648, acc 1\n",
      "2018-08-10T17:58:18.294793: step 1429, loss 0.00360312, acc 1\n",
      "2018-08-10T17:58:18.446157: step 1430, loss 0.00133967, acc 1\n",
      "2018-08-10T17:58:18.602774: step 1431, loss 0.000250954, acc 1\n",
      "2018-08-10T17:58:18.767389: step 1432, loss 0.000248876, acc 1\n",
      "2018-08-10T17:58:18.927408: step 1433, loss 0.00127615, acc 1\n",
      "2018-08-10T17:58:19.081304: step 1434, loss 0.000307198, acc 1\n",
      "2018-08-10T17:58:19.236737: step 1435, loss 5.37525e-05, acc 1\n",
      "2018-08-10T17:58:19.392111: step 1436, loss 0.00203392, acc 1\n",
      "2018-08-10T17:58:19.550358: step 1437, loss 0.00173637, acc 1\n",
      "2018-08-10T17:58:19.706205: step 1438, loss 0.00890448, acc 1\n",
      "2018-08-10T17:58:19.864188: step 1439, loss 0.00581295, acc 1\n",
      "2018-08-10T17:58:20.022071: step 1440, loss 0.000718471, acc 1\n",
      "2018-08-10T17:58:20.177186: step 1441, loss 0.000157307, acc 1\n",
      "2018-08-10T17:58:20.335880: step 1442, loss 0.000147083, acc 1\n",
      "2018-08-10T17:58:20.491211: step 1443, loss 0.0110933, acc 1\n",
      "2018-08-10T17:58:20.645812: step 1444, loss 0.00930155, acc 1\n",
      "2018-08-10T17:58:20.800792: step 1445, loss 0.000578354, acc 1\n",
      "2018-08-10T17:58:20.956629: step 1446, loss 0.00147552, acc 1\n",
      "2018-08-10T17:58:21.111291: step 1447, loss 0.000420515, acc 1\n",
      "2018-08-10T17:58:21.268628: step 1448, loss 0.00308953, acc 1\n",
      "2018-08-10T17:58:21.427734: step 1449, loss 0.000189347, acc 1\n",
      "2018-08-10T17:58:21.584938: step 1450, loss 0.000187556, acc 1\n",
      "2018-08-10T17:58:21.741623: step 1451, loss 0.000384927, acc 1\n",
      "2018-08-10T17:58:21.900740: step 1452, loss 0.000260274, acc 1\n",
      "2018-08-10T17:58:22.055867: step 1453, loss 0.000744108, acc 1\n",
      "2018-08-10T17:58:22.211594: step 1454, loss 0.00361475, acc 1\n",
      "2018-08-10T17:58:22.366483: step 1455, loss 0.00105338, acc 1\n",
      "2018-08-10T17:58:22.518879: step 1456, loss 0.000337194, acc 1\n",
      "2018-08-10T17:58:22.677690: step 1457, loss 0.00129103, acc 1\n",
      "2018-08-10T17:58:22.833956: step 1458, loss 0.00474211, acc 1\n",
      "2018-08-10T17:58:22.989251: step 1459, loss 0.00325521, acc 1\n",
      "2018-08-10T17:58:23.144643: step 1460, loss 0.000167357, acc 1\n",
      "2018-08-10T17:58:23.302470: step 1461, loss 0.0112519, acc 1\n",
      "2018-08-10T17:58:23.458430: step 1462, loss 0.00420131, acc 1\n",
      "2018-08-10T17:58:23.613071: step 1463, loss 0.000542919, acc 1\n",
      "2018-08-10T17:58:23.772743: step 1464, loss 0.000242027, acc 1\n",
      "2018-08-10T17:58:23.930207: step 1465, loss 0.000152535, acc 1\n",
      "2018-08-10T17:58:24.086622: step 1466, loss 7.0412e-05, acc 1\n",
      "2018-08-10T17:58:24.245873: step 1467, loss 0.000664298, acc 1\n",
      "2018-08-10T17:58:24.400272: step 1468, loss 0.000450117, acc 1\n",
      "2018-08-10T17:58:24.549470: step 1469, loss 0.000373404, acc 1\n",
      "2018-08-10T17:58:24.698947: step 1470, loss 0.000627862, acc 1\n",
      "2018-08-10T17:58:24.851166: step 1471, loss 0.000416522, acc 1\n",
      "2018-08-10T17:58:25.002844: step 1472, loss 0.000364466, acc 1\n",
      "2018-08-10T17:58:25.157352: step 1473, loss 0.000768149, acc 1\n",
      "2018-08-10T17:58:25.312340: step 1474, loss 0.00115621, acc 1\n",
      "2018-08-10T17:58:25.467550: step 1475, loss 0.00103307, acc 1\n",
      "2018-08-10T17:58:25.620079: step 1476, loss 0.00012921, acc 1\n",
      "2018-08-10T17:58:25.775379: step 1477, loss 0.0249904, acc 0.984375\n",
      "2018-08-10T17:58:25.931065: step 1478, loss 0.00554906, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:58:26.085390: step 1479, loss 0.00218347, acc 1\n",
      "2018-08-10T17:58:26.240248: step 1480, loss 0.000476438, acc 1\n",
      "2018-08-10T17:58:26.396710: step 1481, loss 0.00319212, acc 1\n",
      "2018-08-10T17:58:26.553074: step 1482, loss 0.00613378, acc 1\n",
      "2018-08-10T17:58:26.711355: step 1483, loss 0.000147745, acc 1\n",
      "2018-08-10T17:58:26.868955: step 1484, loss 0.00137188, acc 1\n",
      "2018-08-10T17:58:27.023429: step 1485, loss 0.00215471, acc 1\n",
      "2018-08-10T17:58:27.177973: step 1486, loss 0.000406705, acc 1\n",
      "2018-08-10T17:58:27.334116: step 1487, loss 0.00240762, acc 1\n",
      "2018-08-10T17:58:27.490292: step 1488, loss 0.00174593, acc 1\n",
      "2018-08-10T17:58:27.648390: step 1489, loss 0.000195203, acc 1\n",
      "2018-08-10T17:58:27.803296: step 1490, loss 0.00192231, acc 1\n",
      "2018-08-10T17:58:27.963410: step 1491, loss 0.000597689, acc 1\n",
      "2018-08-10T17:58:28.122010: step 1492, loss 0.0117664, acc 1\n",
      "2018-08-10T17:58:28.278588: step 1493, loss 0.00206144, acc 1\n",
      "2018-08-10T17:58:28.440194: step 1494, loss 0.000358517, acc 1\n",
      "2018-08-10T17:58:28.594003: step 1495, loss 0.00025861, acc 1\n",
      "2018-08-10T17:58:28.753043: step 1496, loss 0.00450291, acc 1\n",
      "2018-08-10T17:58:28.914371: step 1497, loss 0.000694344, acc 1\n",
      "2018-08-10T17:58:29.071602: step 1498, loss 0.00161315, acc 1\n",
      "2018-08-10T17:58:29.229904: step 1499, loss 0.000582503, acc 1\n",
      "2018-08-10T17:58:29.385697: step 1500, loss 0.000849721, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:58:29.538867: step 1500, loss 0.655459, acc 0.866029\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533923672/checkpoints/model-1500\n",
      "\n",
      "2018-08-10T17:58:29.756449: step 1501, loss 0.00314788, acc 1\n",
      "2018-08-10T17:58:29.914547: step 1502, loss 0.0167098, acc 0.984375\n",
      "2018-08-10T17:58:30.073538: step 1503, loss 0.000256406, acc 1\n",
      "2018-08-10T17:58:30.229658: step 1504, loss 0.00159804, acc 1\n",
      "2018-08-10T17:58:30.389793: step 1505, loss 0.000156381, acc 1\n",
      "2018-08-10T17:58:30.547002: step 1506, loss 0.00108902, acc 1\n",
      "2018-08-10T17:58:30.698002: step 1507, loss 0.000319613, acc 1\n",
      "2018-08-10T17:58:30.844736: step 1508, loss 0.000243294, acc 1\n",
      "2018-08-10T17:58:30.997291: step 1509, loss 0.00336918, acc 1\n",
      "2018-08-10T17:58:31.143557: step 1510, loss 0.00110521, acc 1\n",
      "2018-08-10T17:58:31.292133: step 1511, loss 0.00228864, acc 1\n",
      "2018-08-10T17:58:31.440403: step 1512, loss 0.000463065, acc 1\n",
      "2018-08-10T17:58:31.590212: step 1513, loss 0.00261688, acc 1\n",
      "2018-08-10T17:58:31.737897: step 1514, loss 0.00310606, acc 1\n",
      "2018-08-10T17:58:31.889782: step 1515, loss 0.00109896, acc 1\n",
      "2018-08-10T17:58:32.039058: step 1516, loss 0.0247871, acc 0.984375\n",
      "2018-08-10T17:58:32.191405: step 1517, loss 0.000608995, acc 1\n",
      "2018-08-10T17:58:32.339003: step 1518, loss 0.00206906, acc 1\n",
      "2018-08-10T17:58:32.490047: step 1519, loss 0.000911651, acc 1\n",
      "2018-08-10T17:58:32.639525: step 1520, loss 0.000265019, acc 1\n",
      "2018-08-10T17:58:32.783339: step 1521, loss 0.000235777, acc 1\n",
      "2018-08-10T17:58:32.929678: step 1522, loss 0.000166528, acc 1\n",
      "2018-08-10T17:58:33.078116: step 1523, loss 0.00381212, acc 1\n",
      "2018-08-10T17:58:33.226510: step 1524, loss 0.00076863, acc 1\n",
      "2018-08-10T17:58:33.374071: step 1525, loss 7.17472e-05, acc 1\n",
      "2018-08-10T17:58:33.522347: step 1526, loss 0.000266356, acc 1\n",
      "2018-08-10T17:58:33.672168: step 1527, loss 0.000192299, acc 1\n",
      "2018-08-10T17:58:33.817880: step 1528, loss 0.000129418, acc 1\n",
      "2018-08-10T17:58:33.966491: step 1529, loss 0.000579161, acc 1\n",
      "2018-08-10T17:58:34.113260: step 1530, loss 0.0123559, acc 0.984375\n",
      "2018-08-10T17:58:34.261294: step 1531, loss 4.15643e-05, acc 1\n",
      "2018-08-10T17:58:34.409468: step 1532, loss 0.00227434, acc 1\n",
      "2018-08-10T17:58:34.562029: step 1533, loss 0.0119588, acc 0.984375\n",
      "2018-08-10T17:58:34.707461: step 1534, loss 0.000410089, acc 1\n",
      "2018-08-10T17:58:34.857099: step 1535, loss 0.000138664, acc 1\n",
      "2018-08-10T17:58:35.009544: step 1536, loss 0.00495709, acc 1\n",
      "2018-08-10T17:58:35.161952: step 1537, loss 0.000360724, acc 1\n",
      "2018-08-10T17:58:35.319971: step 1538, loss 0.000489958, acc 1\n",
      "2018-08-10T17:58:35.476793: step 1539, loss 0.000136078, acc 1\n",
      "2018-08-10T17:58:35.632753: step 1540, loss 0.000369698, acc 1\n",
      "2018-08-10T17:58:35.788152: step 1541, loss 0.000541923, acc 1\n",
      "2018-08-10T17:58:35.947080: step 1542, loss 0.000384897, acc 1\n",
      "2018-08-10T17:58:36.106395: step 1543, loss 0.000378988, acc 1\n",
      "2018-08-10T17:58:36.262289: step 1544, loss 0.00159412, acc 1\n",
      "2018-08-10T17:58:36.419559: step 1545, loss 0.000308805, acc 1\n",
      "2018-08-10T17:58:36.581593: step 1546, loss 0.000420933, acc 1\n",
      "2018-08-10T17:58:36.735770: step 1547, loss 0.00328428, acc 1\n",
      "2018-08-10T17:58:36.895265: step 1548, loss 0.000941391, acc 1\n",
      "2018-08-10T17:58:37.052379: step 1549, loss 0.00432667, acc 1\n",
      "2018-08-10T17:58:37.208974: step 1550, loss 0.000242668, acc 1\n",
      "2018-08-10T17:58:37.370078: step 1551, loss 0.00304178, acc 1\n",
      "2018-08-10T17:58:37.524740: step 1552, loss 0.011485, acc 1\n",
      "2018-08-10T17:58:37.682899: step 1553, loss 0.0111314, acc 1\n",
      "2018-08-10T17:58:37.843790: step 1554, loss 7.46903e-05, acc 1\n",
      "2018-08-10T17:58:38.001990: step 1555, loss 0.00061368, acc 1\n",
      "2018-08-10T17:58:38.158745: step 1556, loss 0.00289661, acc 1\n",
      "2018-08-10T17:58:38.317085: step 1557, loss 0.000192486, acc 1\n",
      "2018-08-10T17:58:38.475419: step 1558, loss 0.0056287, acc 1\n",
      "2018-08-10T17:58:38.635358: step 1559, loss 0.00020459, acc 1\n",
      "2018-08-10T17:58:38.785631: step 1560, loss 0.000170255, acc 1\n",
      "2018-08-10T17:58:38.945235: step 1561, loss 0.000580928, acc 1\n",
      "2018-08-10T17:58:39.100798: step 1562, loss 0.0252708, acc 0.984375\n",
      "2018-08-10T17:58:39.257911: step 1563, loss 0.000123611, acc 1\n",
      "2018-08-10T17:58:39.414724: step 1564, loss 0.000105116, acc 1\n",
      "2018-08-10T17:58:39.570968: step 1565, loss 0.000286013, acc 1\n",
      "2018-08-10T17:58:39.726923: step 1566, loss 0.000208453, acc 1\n",
      "2018-08-10T17:58:39.887263: step 1567, loss 0.00045102, acc 1\n",
      "2018-08-10T17:58:40.042183: step 1568, loss 0.000396869, acc 1\n",
      "2018-08-10T17:58:40.200604: step 1569, loss 0.000211426, acc 1\n",
      "2018-08-10T17:58:40.355821: step 1570, loss 5.7158e-05, acc 1\n",
      "2018-08-10T17:58:40.513856: step 1571, loss 0.000319326, acc 1\n",
      "2018-08-10T17:58:40.671443: step 1572, loss 0.000197058, acc 1\n",
      "2018-08-10T17:58:40.826076: step 1573, loss 0.00167968, acc 1\n",
      "2018-08-10T17:58:40.982808: step 1574, loss 0.00024298, acc 1\n",
      "2018-08-10T17:58:41.139994: step 1575, loss 0.00194285, acc 1\n",
      "2018-08-10T17:58:41.294531: step 1576, loss 0.000929626, acc 1\n",
      "2018-08-10T17:58:41.452879: step 1577, loss 0.00035468, acc 1\n",
      "2018-08-10T17:58:41.608309: step 1578, loss 0.000168568, acc 1\n",
      "2018-08-10T17:58:41.766743: step 1579, loss 0.000953524, acc 1\n",
      "2018-08-10T17:58:41.925796: step 1580, loss 0.000127965, acc 1\n",
      "2018-08-10T17:58:42.082636: step 1581, loss 0.000122833, acc 1\n",
      "2018-08-10T17:58:42.241297: step 1582, loss 0.000800213, acc 1\n",
      "2018-08-10T17:58:42.398917: step 1583, loss 0.00879316, acc 1\n",
      "2018-08-10T17:58:42.557319: step 1584, loss 0.000243405, acc 1\n",
      "2018-08-10T17:58:42.717826: step 1585, loss 0.00059693, acc 1\n",
      "2018-08-10T17:58:42.872001: step 1586, loss 0.00334251, acc 1\n",
      "2018-08-10T17:58:43.027618: step 1587, loss 0.000445988, acc 1\n",
      "2018-08-10T17:58:43.182779: step 1588, loss 0.000162186, acc 1\n",
      "2018-08-10T17:58:43.340936: step 1589, loss 0.000133845, acc 1\n",
      "2018-08-10T17:58:43.493713: step 1590, loss 0.00182421, acc 1\n",
      "2018-08-10T17:58:43.648692: step 1591, loss 0.00666542, acc 1\n",
      "2018-08-10T17:58:43.808045: step 1592, loss 0.000205535, acc 1\n",
      "2018-08-10T17:58:43.965178: step 1593, loss 0.000816281, acc 1\n",
      "2018-08-10T17:58:44.121059: step 1594, loss 0.0108008, acc 1\n",
      "2018-08-10T17:58:44.278655: step 1595, loss 0.00228547, acc 1\n",
      "2018-08-10T17:58:44.436630: step 1596, loss 0.000288789, acc 1\n",
      "2018-08-10T17:58:44.591060: step 1597, loss 0.000168163, acc 1\n",
      "2018-08-10T17:58:44.747306: step 1598, loss 8.00924e-05, acc 1\n",
      "2018-08-10T17:58:44.901970: step 1599, loss 0.000481315, acc 1\n",
      "2018-08-10T17:58:45.056603: step 1600, loss 0.0038597, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:58:45.229028: step 1600, loss 0.697539, acc 0.866029\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533923672/checkpoints/model-1600\n",
      "\n",
      "2018-08-10T17:58:45.441064: step 1601, loss 0.000761296, acc 1\n",
      "2018-08-10T17:58:45.590085: step 1602, loss 0.000369016, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:58:45.738626: step 1603, loss 0.00445656, acc 1\n",
      "2018-08-10T17:58:45.895081: step 1604, loss 0.000351896, acc 1\n",
      "2018-08-10T17:58:46.047776: step 1605, loss 9.55294e-05, acc 1\n",
      "2018-08-10T17:58:46.203303: step 1606, loss 0.000749764, acc 1\n",
      "2018-08-10T17:58:46.360132: step 1607, loss 0.000327516, acc 1\n",
      "2018-08-10T17:58:46.518635: step 1608, loss 0.00883652, acc 1\n",
      "2018-08-10T17:58:46.676170: step 1609, loss 0.00216471, acc 1\n",
      "2018-08-10T17:58:46.849116: step 1610, loss 0.000165705, acc 1\n",
      "2018-08-10T17:58:47.007163: step 1611, loss 0.000495897, acc 1\n",
      "2018-08-10T17:58:47.161743: step 1612, loss 7.91516e-05, acc 1\n",
      "2018-08-10T17:58:47.320650: step 1613, loss 0.000441505, acc 1\n",
      "2018-08-10T17:58:47.478488: step 1614, loss 0.00135775, acc 1\n",
      "2018-08-10T17:58:47.635060: step 1615, loss 0.000556934, acc 1\n",
      "2018-08-10T17:58:47.791899: step 1616, loss 0.00034391, acc 1\n",
      "2018-08-10T17:58:47.953357: step 1617, loss 0.000315142, acc 1\n",
      "2018-08-10T17:58:48.108255: step 1618, loss 0.000802941, acc 1\n",
      "2018-08-10T17:58:48.264847: step 1619, loss 0.00346687, acc 1\n",
      "2018-08-10T17:58:48.423769: step 1620, loss 0.00118332, acc 1\n",
      "2018-08-10T17:58:48.582909: step 1621, loss 0.00210534, acc 1\n",
      "2018-08-10T17:58:48.739283: step 1622, loss 0.00125887, acc 1\n",
      "2018-08-10T17:58:48.899039: step 1623, loss 0.000230874, acc 1\n",
      "2018-08-10T17:58:49.054446: step 1624, loss 0.00353766, acc 1\n",
      "2018-08-10T17:58:49.205209: step 1625, loss 0.000461029, acc 1\n",
      "2018-08-10T17:58:49.364043: step 1626, loss 0.000169181, acc 1\n",
      "2018-08-10T17:58:49.523600: step 1627, loss 0.00291573, acc 1\n",
      "2018-08-10T17:58:49.679970: step 1628, loss 0.00084505, acc 1\n",
      "2018-08-10T17:58:49.839250: step 1629, loss 0.000912983, acc 1\n",
      "2018-08-10T17:58:49.997965: step 1630, loss 0.000594189, acc 1\n",
      "2018-08-10T17:58:50.159293: step 1631, loss 0.00099384, acc 1\n",
      "2018-08-10T17:58:50.317549: step 1632, loss 0.00469702, acc 1\n",
      "2018-08-10T17:58:50.477266: step 1633, loss 0.00220254, acc 1\n",
      "2018-08-10T17:58:50.633280: step 1634, loss 0.000591261, acc 1\n",
      "2018-08-10T17:58:50.788324: step 1635, loss 0.000315913, acc 1\n",
      "2018-08-10T17:58:50.947742: step 1636, loss 0.000608151, acc 1\n",
      "2018-08-10T17:58:51.106246: step 1637, loss 0.000155271, acc 1\n",
      "2018-08-10T17:58:51.257472: step 1638, loss 0.000334432, acc 1\n",
      "2018-08-10T17:58:51.415367: step 1639, loss 0.000146242, acc 1\n",
      "2018-08-10T17:58:51.564758: step 1640, loss 0.000201347, acc 1\n",
      "2018-08-10T17:58:51.713077: step 1641, loss 0.000114022, acc 1\n",
      "2018-08-10T17:58:51.863656: step 1642, loss 0.0010039, acc 1\n",
      "2018-08-10T17:58:52.013043: step 1643, loss 0.00564561, acc 1\n",
      "2018-08-10T17:58:52.163682: step 1644, loss 5.43256e-05, acc 1\n",
      "2018-08-10T17:58:52.318386: step 1645, loss 0.0125251, acc 0.984375\n",
      "2018-08-10T17:58:52.468262: step 1646, loss 0.000244703, acc 1\n",
      "2018-08-10T17:58:52.618880: step 1647, loss 0.000109294, acc 1\n",
      "2018-08-10T17:58:52.768504: step 1648, loss 0.00242743, acc 1\n",
      "2018-08-10T17:58:52.919097: step 1649, loss 0.00023424, acc 1\n",
      "2018-08-10T17:58:53.070216: step 1650, loss 0.000236011, acc 1\n",
      "2018-08-10T17:58:53.213764: step 1651, loss 0.00170555, acc 1\n",
      "2018-08-10T17:58:53.362905: step 1652, loss 0.00106591, acc 1\n",
      "2018-08-10T17:58:53.511470: step 1653, loss 0.00179701, acc 1\n",
      "2018-08-10T17:58:53.660540: step 1654, loss 0.00122741, acc 1\n",
      "2018-08-10T17:58:53.807751: step 1655, loss 0.000241318, acc 1\n",
      "2018-08-10T17:58:53.958945: step 1656, loss 0.00330717, acc 1\n",
      "2018-08-10T17:58:54.110237: step 1657, loss 0.000787684, acc 1\n",
      "2018-08-10T17:58:54.257436: step 1658, loss 5.89932e-05, acc 1\n",
      "2018-08-10T17:58:54.406081: step 1659, loss 0.000225603, acc 1\n",
      "2018-08-10T17:58:54.556752: step 1660, loss 0.0019098, acc 1\n",
      "2018-08-10T17:58:54.706331: step 1661, loss 0.000163668, acc 1\n",
      "2018-08-10T17:58:54.854164: step 1662, loss 8.68572e-05, acc 1\n",
      "2018-08-10T17:58:55.005551: step 1663, loss 0.000521927, acc 1\n",
      "2018-08-10T17:58:55.149320: step 1664, loss 0.000280832, acc 1\n",
      "2018-08-10T17:58:55.298506: step 1665, loss 0.00147798, acc 1\n",
      "2018-08-10T17:58:55.449248: step 1666, loss 0.000552724, acc 1\n",
      "2018-08-10T17:58:55.599317: step 1667, loss 0.000851434, acc 1\n",
      "2018-08-10T17:58:55.748089: step 1668, loss 0.00013939, acc 1\n",
      "2018-08-10T17:58:55.901319: step 1669, loss 0.000376652, acc 1\n",
      "2018-08-10T17:58:56.054393: step 1670, loss 0.00142287, acc 1\n",
      "2018-08-10T17:58:56.209380: step 1671, loss 6.46223e-05, acc 1\n",
      "2018-08-10T17:58:56.364941: step 1672, loss 0.000412353, acc 1\n",
      "2018-08-10T17:58:56.526072: step 1673, loss 0.000142101, acc 1\n",
      "2018-08-10T17:58:56.684678: step 1674, loss 6.02257e-05, acc 1\n",
      "2018-08-10T17:58:56.844580: step 1675, loss 0.00602854, acc 1\n",
      "2018-08-10T17:58:57.011982: step 1676, loss 9.15816e-05, acc 1\n",
      "2018-08-10T17:58:57.163144: step 1677, loss 0.000191123, acc 1\n",
      "2018-08-10T17:58:57.319586: step 1678, loss 0.000224441, acc 1\n",
      "2018-08-10T17:58:57.478026: step 1679, loss 0.00898081, acc 1\n",
      "2018-08-10T17:58:57.633976: step 1680, loss 0.000229846, acc 1\n",
      "2018-08-10T17:58:57.792870: step 1681, loss 0.000404919, acc 1\n",
      "2018-08-10T17:58:57.955099: step 1682, loss 0.000648946, acc 1\n",
      "2018-08-10T17:58:58.117421: step 1683, loss 0.000152524, acc 1\n",
      "2018-08-10T17:58:58.274337: step 1684, loss 0.000262198, acc 1\n",
      "2018-08-10T17:58:58.433939: step 1685, loss 0.000251022, acc 1\n",
      "2018-08-10T17:58:58.592052: step 1686, loss 0.00175736, acc 1\n",
      "2018-08-10T17:58:58.753575: step 1687, loss 0.00296348, acc 1\n",
      "2018-08-10T17:58:58.915409: step 1688, loss 0.000898698, acc 1\n",
      "2018-08-10T17:58:59.071186: step 1689, loss 0.000188521, acc 1\n",
      "2018-08-10T17:58:59.224418: step 1690, loss 0.00979038, acc 1\n",
      "2018-08-10T17:58:59.388576: step 1691, loss 0.000346718, acc 1\n",
      "2018-08-10T17:58:59.545830: step 1692, loss 3.28596e-05, acc 1\n",
      "2018-08-10T17:58:59.704800: step 1693, loss 0.000717537, acc 1\n",
      "2018-08-10T17:58:59.869683: step 1694, loss 0.0011319, acc 1\n",
      "2018-08-10T17:59:00.030390: step 1695, loss 0.000174549, acc 1\n",
      "2018-08-10T17:59:00.189275: step 1696, loss 0.000290233, acc 1\n",
      "2018-08-10T17:59:00.351005: step 1697, loss 0.00013924, acc 1\n",
      "2018-08-10T17:59:00.514983: step 1698, loss 0.00143518, acc 1\n",
      "2018-08-10T17:59:00.678162: step 1699, loss 0.00273397, acc 1\n",
      "2018-08-10T17:59:00.839799: step 1700, loss 0.00040629, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:59:00.992730: step 1700, loss 0.841274, acc 0.856459\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533923672/checkpoints/model-1700\n",
      "\n",
      "2018-08-10T17:59:01.212390: step 1701, loss 0.000272783, acc 1\n",
      "2018-08-10T17:59:01.370198: step 1702, loss 5.7487e-05, acc 1\n",
      "2018-08-10T17:59:01.526996: step 1703, loss 0.000173714, acc 1\n",
      "2018-08-10T17:59:01.685637: step 1704, loss 0.000139681, acc 1\n",
      "2018-08-10T17:59:01.845443: step 1705, loss 9.08901e-05, acc 1\n",
      "2018-08-10T17:59:02.005312: step 1706, loss 0.00105032, acc 1\n",
      "2018-08-10T17:59:02.167627: step 1707, loss 0.000192786, acc 1\n",
      "2018-08-10T17:59:02.324776: step 1708, loss 0.00015002, acc 1\n",
      "2018-08-10T17:59:02.484691: step 1709, loss 0.000323635, acc 1\n",
      "2018-08-10T17:59:02.644962: step 1710, loss 0.00046995, acc 1\n",
      "2018-08-10T17:59:02.802838: step 1711, loss 0.000338633, acc 1\n",
      "2018-08-10T17:59:02.964702: step 1712, loss 0.000355764, acc 1\n",
      "2018-08-10T17:59:03.124911: step 1713, loss 6.5087e-05, acc 1\n",
      "2018-08-10T17:59:03.286407: step 1714, loss 0.000952522, acc 1\n",
      "2018-08-10T17:59:03.448422: step 1715, loss 0.00126673, acc 1\n",
      "2018-08-10T17:59:03.601690: step 1716, loss 0.000379552, acc 1\n",
      "2018-08-10T17:59:03.761702: step 1717, loss 0.00136185, acc 1\n",
      "2018-08-10T17:59:03.928250: step 1718, loss 0.000132587, acc 1\n",
      "2018-08-10T17:59:04.088129: step 1719, loss 0.000322125, acc 1\n",
      "2018-08-10T17:59:04.247267: step 1720, loss 0.000445042, acc 1\n",
      "2018-08-10T17:59:04.409071: step 1721, loss 0.000626726, acc 1\n",
      "2018-08-10T17:59:04.571274: step 1722, loss 0.000914627, acc 1\n",
      "2018-08-10T17:59:04.733485: step 1723, loss 0.000138269, acc 1\n",
      "2018-08-10T17:59:04.894444: step 1724, loss 0.000533796, acc 1\n",
      "2018-08-10T17:59:05.055965: step 1725, loss 0.000403486, acc 1\n",
      "2018-08-10T17:59:05.215872: step 1726, loss 0.000891441, acc 1\n",
      "2018-08-10T17:59:05.381890: step 1727, loss 9.36289e-05, acc 1\n",
      "2018-08-10T17:59:05.544896: step 1728, loss 0.000362037, acc 1\n",
      "2018-08-10T17:59:05.700551: step 1729, loss 5.9671e-05, acc 1\n",
      "2018-08-10T17:59:05.859797: step 1730, loss 0.0563182, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:59:06.020647: step 1731, loss 0.000191455, acc 1\n",
      "2018-08-10T17:59:06.179753: step 1732, loss 0.00186798, acc 1\n",
      "2018-08-10T17:59:06.332987: step 1733, loss 4.92582e-05, acc 1\n",
      "2018-08-10T17:59:06.484748: step 1734, loss 0.000634112, acc 1\n",
      "2018-08-10T17:59:06.642690: step 1735, loss 0.000156403, acc 1\n",
      "2018-08-10T17:59:06.794098: step 1736, loss 0.00452802, acc 1\n",
      "2018-08-10T17:59:06.951461: step 1737, loss 0.000106517, acc 1\n",
      "2018-08-10T17:59:07.108381: step 1738, loss 0.00230957, acc 1\n",
      "2018-08-10T17:59:07.265950: step 1739, loss 0.00112178, acc 1\n",
      "2018-08-10T17:59:07.424465: step 1740, loss 0.000150566, acc 1\n",
      "2018-08-10T17:59:07.583144: step 1741, loss 0.00184068, acc 1\n",
      "2018-08-10T17:59:07.737659: step 1742, loss 0.00244589, acc 1\n",
      "2018-08-10T17:59:07.896631: step 1743, loss 3.87099e-05, acc 1\n",
      "2018-08-10T17:59:08.053365: step 1744, loss 0.00274313, acc 1\n",
      "2018-08-10T17:59:08.211569: step 1745, loss 0.00231997, acc 1\n",
      "2018-08-10T17:59:08.368859: step 1746, loss 0.000171259, acc 1\n",
      "2018-08-10T17:59:08.525686: step 1747, loss 0.000316277, acc 1\n",
      "2018-08-10T17:59:08.681674: step 1748, loss 0.000281544, acc 1\n",
      "2018-08-10T17:59:08.844149: step 1749, loss 0.00112219, acc 1\n",
      "2018-08-10T17:59:08.999441: step 1750, loss 0.000294893, acc 1\n",
      "2018-08-10T17:59:09.155335: step 1751, loss 0.0021732, acc 1\n",
      "2018-08-10T17:59:09.313298: step 1752, loss 0.000295868, acc 1\n",
      "2018-08-10T17:59:09.471346: step 1753, loss 0.000415951, acc 1\n",
      "2018-08-10T17:59:09.629626: step 1754, loss 0.00111509, acc 1\n",
      "2018-08-10T17:59:09.784122: step 1755, loss 0.0315809, acc 0.983871\n",
      "2018-08-10T17:59:09.945248: step 1756, loss 0.000169728, acc 1\n",
      "2018-08-10T17:59:10.102674: step 1757, loss 0.00094009, acc 1\n",
      "2018-08-10T17:59:10.262965: step 1758, loss 0.000149261, acc 1\n",
      "2018-08-10T17:59:10.420987: step 1759, loss 0.000149404, acc 1\n",
      "2018-08-10T17:59:10.581989: step 1760, loss 0.000340714, acc 1\n",
      "2018-08-10T17:59:10.742833: step 1761, loss 0.000194267, acc 1\n",
      "2018-08-10T17:59:10.906939: step 1762, loss 0.000479032, acc 1\n",
      "2018-08-10T17:59:11.066979: step 1763, loss 0.00210762, acc 1\n",
      "2018-08-10T17:59:11.226005: step 1764, loss 0.000663712, acc 1\n",
      "2018-08-10T17:59:11.382098: step 1765, loss 0.00109776, acc 1\n",
      "2018-08-10T17:59:11.540384: step 1766, loss 0.000138241, acc 1\n",
      "2018-08-10T17:59:11.701452: step 1767, loss 0.000229632, acc 1\n",
      "2018-08-10T17:59:11.856142: step 1768, loss 0.00686058, acc 1\n",
      "2018-08-10T17:59:12.017166: step 1769, loss 0.000791512, acc 1\n",
      "2018-08-10T17:59:12.171681: step 1770, loss 0.000277189, acc 1\n",
      "2018-08-10T17:59:12.329167: step 1771, loss 0.000504141, acc 1\n",
      "2018-08-10T17:59:12.488663: step 1772, loss 0.000572297, acc 1\n",
      "2018-08-10T17:59:12.644046: step 1773, loss 0.00011255, acc 1\n",
      "2018-08-10T17:59:12.790466: step 1774, loss 0.000540497, acc 1\n",
      "2018-08-10T17:59:12.940256: step 1775, loss 0.000305586, acc 1\n",
      "2018-08-10T17:59:13.088748: step 1776, loss 0.00024168, acc 1\n",
      "2018-08-10T17:59:13.237729: step 1777, loss 0.000604355, acc 1\n",
      "2018-08-10T17:59:13.386054: step 1778, loss 0.00027369, acc 1\n",
      "2018-08-10T17:59:13.535226: step 1779, loss 0.00101437, acc 1\n",
      "2018-08-10T17:59:13.684811: step 1780, loss 0.000316274, acc 1\n",
      "2018-08-10T17:59:13.832689: step 1781, loss 0.00212132, acc 1\n",
      "2018-08-10T17:59:13.982390: step 1782, loss 0.00162609, acc 1\n",
      "2018-08-10T17:59:14.131679: step 1783, loss 0.0005742, acc 1\n",
      "2018-08-10T17:59:14.277253: step 1784, loss 0.000229566, acc 1\n",
      "2018-08-10T17:59:14.431015: step 1785, loss 0.00441258, acc 1\n",
      "2018-08-10T17:59:14.582209: step 1786, loss 0.000513574, acc 1\n",
      "2018-08-10T17:59:14.740908: step 1787, loss 0.000115981, acc 1\n",
      "2018-08-10T17:59:14.888810: step 1788, loss 0.000470952, acc 1\n",
      "2018-08-10T17:59:15.036540: step 1789, loss 0.000701003, acc 1\n",
      "2018-08-10T17:59:15.184234: step 1790, loss 0.000102497, acc 1\n",
      "2018-08-10T17:59:15.332816: step 1791, loss 0.000472986, acc 1\n",
      "2018-08-10T17:59:15.483331: step 1792, loss 7.45367e-05, acc 1\n",
      "2018-08-10T17:59:15.631522: step 1793, loss 0.0277, acc 0.984375\n",
      "2018-08-10T17:59:15.774882: step 1794, loss 0.00152266, acc 1\n",
      "2018-08-10T17:59:15.926197: step 1795, loss 0.0259131, acc 0.984375\n",
      "2018-08-10T17:59:16.073145: step 1796, loss 0.000396715, acc 1\n",
      "2018-08-10T17:59:16.222084: step 1797, loss 0.00413087, acc 1\n",
      "2018-08-10T17:59:16.368537: step 1798, loss 0.000216399, acc 1\n",
      "2018-08-10T17:59:16.519207: step 1799, loss 0.000109561, acc 1\n",
      "2018-08-10T17:59:16.669360: step 1800, loss 0.00064672, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:59:16.815279: step 1800, loss 0.787321, acc 0.861244\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533923672/checkpoints/model-1800\n",
      "\n",
      "2018-08-10T17:59:17.028298: step 1801, loss 0.00220358, acc 1\n",
      "2018-08-10T17:59:17.185198: step 1802, loss 8.79935e-05, acc 1\n",
      "2018-08-10T17:59:17.342608: step 1803, loss 0.0218075, acc 0.984375\n",
      "2018-08-10T17:59:17.498675: step 1804, loss 0.000555077, acc 1\n",
      "2018-08-10T17:59:17.655262: step 1805, loss 0.00063569, acc 1\n",
      "2018-08-10T17:59:17.812800: step 1806, loss 0.00021196, acc 1\n",
      "2018-08-10T17:59:17.968186: step 1807, loss 0.000219583, acc 1\n",
      "2018-08-10T17:59:18.126078: step 1808, loss 0.000439905, acc 1\n",
      "2018-08-10T17:59:18.282959: step 1809, loss 7.6428e-05, acc 1\n",
      "2018-08-10T17:59:18.444600: step 1810, loss 0.0402012, acc 0.984375\n",
      "2018-08-10T17:59:18.603565: step 1811, loss 0.000286233, acc 1\n",
      "2018-08-10T17:59:18.759584: step 1812, loss 0.00286452, acc 1\n",
      "2018-08-10T17:59:18.918992: step 1813, loss 0.000656168, acc 1\n",
      "2018-08-10T17:59:19.077250: step 1814, loss 0.000144887, acc 1\n",
      "2018-08-10T17:59:19.234935: step 1815, loss 0.00248028, acc 1\n",
      "2018-08-10T17:59:19.392995: step 1816, loss 0.000175689, acc 1\n",
      "2018-08-10T17:59:19.552196: step 1817, loss 0.000586509, acc 1\n",
      "2018-08-10T17:59:19.707705: step 1818, loss 0.000121501, acc 1\n",
      "2018-08-10T17:59:19.866071: step 1819, loss 0.000305705, acc 1\n",
      "2018-08-10T17:59:20.019994: step 1820, loss 6.86255e-05, acc 1\n",
      "2018-08-10T17:59:20.176816: step 1821, loss 0.000268954, acc 1\n",
      "2018-08-10T17:59:20.334648: step 1822, loss 0.00194616, acc 1\n",
      "2018-08-10T17:59:20.494732: step 1823, loss 0.000313034, acc 1\n",
      "2018-08-10T17:59:20.654436: step 1824, loss 0.00050665, acc 1\n",
      "2018-08-10T17:59:20.811429: step 1825, loss 0.000945926, acc 1\n",
      "2018-08-10T17:59:20.972674: step 1826, loss 0.00362336, acc 1\n",
      "2018-08-10T17:59:21.128082: step 1827, loss 0.0168101, acc 0.984375\n",
      "2018-08-10T17:59:21.286407: step 1828, loss 0.000383723, acc 1\n",
      "2018-08-10T17:59:21.457020: step 1829, loss 8.96925e-05, acc 1\n",
      "2018-08-10T17:59:21.611318: step 1830, loss 0.000129911, acc 1\n",
      "2018-08-10T17:59:21.770189: step 1831, loss 0.00137989, acc 1\n",
      "2018-08-10T17:59:21.932298: step 1832, loss 0.00287918, acc 1\n",
      "2018-08-10T17:59:22.088266: step 1833, loss 7.52727e-05, acc 1\n",
      "2018-08-10T17:59:22.243727: step 1834, loss 6.74162e-05, acc 1\n",
      "2018-08-10T17:59:22.398844: step 1835, loss 0.000135956, acc 1\n",
      "2018-08-10T17:59:22.558348: step 1836, loss 0.000252141, acc 1\n",
      "2018-08-10T17:59:22.717269: step 1837, loss 0.00118888, acc 1\n",
      "2018-08-10T17:59:22.876237: step 1838, loss 0.00375205, acc 1\n",
      "2018-08-10T17:59:23.033185: step 1839, loss 0.000212169, acc 1\n",
      "2018-08-10T17:59:23.191632: step 1840, loss 0.000665382, acc 1\n",
      "2018-08-10T17:59:23.349781: step 1841, loss 0.000233742, acc 1\n",
      "2018-08-10T17:59:23.507486: step 1842, loss 0.000108341, acc 1\n",
      "2018-08-10T17:59:23.666203: step 1843, loss 0.000822638, acc 1\n",
      "2018-08-10T17:59:23.822324: step 1844, loss 0.0184423, acc 0.984375\n",
      "2018-08-10T17:59:23.981122: step 1845, loss 0.000598018, acc 1\n",
      "2018-08-10T17:59:24.133251: step 1846, loss 0.000279172, acc 1\n",
      "2018-08-10T17:59:24.290140: step 1847, loss 0.000151983, acc 1\n",
      "2018-08-10T17:59:24.449090: step 1848, loss 0.000376877, acc 1\n",
      "2018-08-10T17:59:24.604688: step 1849, loss 0.000360991, acc 1\n",
      "2018-08-10T17:59:24.764534: step 1850, loss 0.000237104, acc 1\n",
      "2018-08-10T17:59:24.921551: step 1851, loss 0.000841202, acc 1\n",
      "2018-08-10T17:59:25.081176: step 1852, loss 7.75323e-05, acc 1\n",
      "2018-08-10T17:59:25.242680: step 1853, loss 0.000168694, acc 1\n",
      "2018-08-10T17:59:25.404062: step 1854, loss 0.000145237, acc 1\n",
      "2018-08-10T17:59:25.562479: step 1855, loss 0.00120534, acc 1\n",
      "2018-08-10T17:59:25.718396: step 1856, loss 0.000469084, acc 1\n",
      "2018-08-10T17:59:25.878965: step 1857, loss 0.000128383, acc 1\n",
      "2018-08-10T17:59:26.035680: step 1858, loss 0.000215838, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:59:26.189262: step 1859, loss 0.000578543, acc 1\n",
      "2018-08-10T17:59:26.346695: step 1860, loss 0.00189672, acc 1\n",
      "2018-08-10T17:59:26.506808: step 1861, loss 0.00203076, acc 1\n",
      "2018-08-10T17:59:26.663196: step 1862, loss 0.0218686, acc 0.984375\n",
      "2018-08-10T17:59:26.821044: step 1863, loss 0.000238705, acc 1\n",
      "2018-08-10T17:59:26.984209: step 1864, loss 0.00442467, acc 1\n",
      "2018-08-10T17:59:27.140885: step 1865, loss 0.00157396, acc 1\n",
      "2018-08-10T17:59:27.290830: step 1866, loss 8.18413e-05, acc 1\n",
      "2018-08-10T17:59:27.439364: step 1867, loss 0.000354923, acc 1\n",
      "2018-08-10T17:59:27.589168: step 1868, loss 0.000405082, acc 1\n",
      "2018-08-10T17:59:27.740387: step 1869, loss 0.00164856, acc 1\n",
      "2018-08-10T17:59:27.899984: step 1870, loss 0.000676598, acc 1\n",
      "2018-08-10T17:59:28.057438: step 1871, loss 0.00522275, acc 1\n",
      "2018-08-10T17:59:28.207868: step 1872, loss 3.31803e-05, acc 1\n",
      "2018-08-10T17:59:28.362693: step 1873, loss 0.000816584, acc 1\n",
      "2018-08-10T17:59:28.519947: step 1874, loss 0.000147412, acc 1\n",
      "2018-08-10T17:59:28.680773: step 1875, loss 0.000326525, acc 1\n",
      "2018-08-10T17:59:28.843029: step 1876, loss 0.000474739, acc 1\n",
      "2018-08-10T17:59:28.999002: step 1877, loss 0.00266999, acc 1\n",
      "2018-08-10T17:59:29.159311: step 1878, loss 0.000546083, acc 1\n",
      "2018-08-10T17:59:29.319454: step 1879, loss 0.00120172, acc 1\n",
      "2018-08-10T17:59:29.477633: step 1880, loss 0.0101667, acc 1\n",
      "2018-08-10T17:59:29.638636: step 1881, loss 0.00125476, acc 1\n",
      "2018-08-10T17:59:29.793447: step 1882, loss 0.000181267, acc 1\n",
      "2018-08-10T17:59:29.953684: step 1883, loss 0.000404112, acc 1\n",
      "2018-08-10T17:59:30.110997: step 1884, loss 2.09524e-05, acc 1\n",
      "2018-08-10T17:59:30.265732: step 1885, loss 0.000203299, acc 1\n",
      "2018-08-10T17:59:30.425844: step 1886, loss 0.000912028, acc 1\n",
      "2018-08-10T17:59:30.592996: step 1887, loss 0.000292073, acc 1\n",
      "2018-08-10T17:59:30.750774: step 1888, loss 0.000138432, acc 1\n",
      "2018-08-10T17:59:30.911150: step 1889, loss 0.00161391, acc 1\n",
      "2018-08-10T17:59:31.069606: step 1890, loss 0.000101755, acc 1\n",
      "2018-08-10T17:59:31.225238: step 1891, loss 0.000128612, acc 1\n",
      "2018-08-10T17:59:31.386792: step 1892, loss 0.000219485, acc 1\n",
      "2018-08-10T17:59:31.544085: step 1893, loss 0.00158834, acc 1\n",
      "2018-08-10T17:59:31.699700: step 1894, loss 6.31688e-05, acc 1\n",
      "2018-08-10T17:59:31.856594: step 1895, loss 0.000145152, acc 1\n",
      "2018-08-10T17:59:32.013576: step 1896, loss 0.000160012, acc 1\n",
      "2018-08-10T17:59:32.170175: step 1897, loss 0.00763614, acc 1\n",
      "2018-08-10T17:59:32.324219: step 1898, loss 0.00614579, acc 1\n",
      "2018-08-10T17:59:32.483425: step 1899, loss 0.000302772, acc 1\n",
      "2018-08-10T17:59:32.642192: step 1900, loss 8.76836e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:59:32.791210: step 1900, loss 0.885222, acc 0.861244\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533923672/checkpoints/model-1900\n",
      "\n",
      "2018-08-10T17:59:33.005603: step 1901, loss 0.0002183, acc 1\n",
      "2018-08-10T17:59:33.169394: step 1902, loss 6.32705e-05, acc 1\n",
      "2018-08-10T17:59:33.319307: step 1903, loss 0.000208296, acc 1\n",
      "2018-08-10T17:59:33.468190: step 1904, loss 0.00173732, acc 1\n",
      "2018-08-10T17:59:33.616423: step 1905, loss 0.000238526, acc 1\n",
      "2018-08-10T17:59:33.763556: step 1906, loss 0.000361556, acc 1\n",
      "2018-08-10T17:59:33.913715: step 1907, loss 6.85339e-05, acc 1\n",
      "2018-08-10T17:59:34.062751: step 1908, loss 0.000238954, acc 1\n",
      "2018-08-10T17:59:34.211218: step 1909, loss 0.00114654, acc 1\n",
      "2018-08-10T17:59:34.358234: step 1910, loss 0.000444918, acc 1\n",
      "2018-08-10T17:59:34.508249: step 1911, loss 0.000247938, acc 1\n",
      "2018-08-10T17:59:34.660956: step 1912, loss 7.64878e-05, acc 1\n",
      "2018-08-10T17:59:34.811635: step 1913, loss 7.61996e-05, acc 1\n",
      "2018-08-10T17:59:34.962048: step 1914, loss 0.00221342, acc 1\n",
      "2018-08-10T17:59:35.111478: step 1915, loss 0.00168464, acc 1\n",
      "2018-08-10T17:59:35.260101: step 1916, loss 5.47716e-05, acc 1\n",
      "2018-08-10T17:59:35.409834: step 1917, loss 0.000163695, acc 1\n",
      "2018-08-10T17:59:35.555285: step 1918, loss 0.000171045, acc 1\n",
      "2018-08-10T17:59:35.703921: step 1919, loss 0.0185787, acc 0.984375\n",
      "2018-08-10T17:59:35.855033: step 1920, loss 0.000108327, acc 1\n",
      "2018-08-10T17:59:36.006347: step 1921, loss 0.00069821, acc 1\n",
      "2018-08-10T17:59:36.156603: step 1922, loss 0.000134667, acc 1\n",
      "2018-08-10T17:59:36.303925: step 1923, loss 0.000266987, acc 1\n",
      "2018-08-10T17:59:36.448483: step 1924, loss 5.49124e-05, acc 1\n",
      "2018-08-10T17:59:36.600159: step 1925, loss 0.000130973, acc 1\n",
      "2018-08-10T17:59:36.750434: step 1926, loss 0.000573167, acc 1\n",
      "2018-08-10T17:59:36.900497: step 1927, loss 1.34832e-05, acc 1\n",
      "2018-08-10T17:59:37.049509: step 1928, loss 0.000269707, acc 1\n",
      "2018-08-10T17:59:37.198178: step 1929, loss 0.000227473, acc 1\n",
      "2018-08-10T17:59:37.346367: step 1930, loss 9.74376e-05, acc 1\n",
      "2018-08-10T17:59:37.496790: step 1931, loss 0.00202857, acc 1\n",
      "2018-08-10T17:59:37.645976: step 1932, loss 0.000127608, acc 1\n",
      "2018-08-10T17:59:37.796836: step 1933, loss 5.84025e-05, acc 1\n",
      "2018-08-10T17:59:37.948809: step 1934, loss 0.000179806, acc 1\n",
      "2018-08-10T17:59:38.104353: step 1935, loss 0.000120418, acc 1\n",
      "2018-08-10T17:59:38.257931: step 1936, loss 0.000183451, acc 1\n",
      "2018-08-10T17:59:38.410386: step 1937, loss 6.00993e-05, acc 1\n",
      "2018-08-10T17:59:38.570040: step 1938, loss 0.000137546, acc 1\n",
      "2018-08-10T17:59:38.743006: step 1939, loss 0.000781393, acc 1\n",
      "2018-08-10T17:59:38.902471: step 1940, loss 0.000706878, acc 1\n",
      "2018-08-10T17:59:39.063345: step 1941, loss 0.0002204, acc 1\n",
      "2018-08-10T17:59:39.217657: step 1942, loss 0.000129456, acc 1\n",
      "2018-08-10T17:59:39.375975: step 1943, loss 0.00168482, acc 1\n",
      "2018-08-10T17:59:39.530672: step 1944, loss 0.000426278, acc 1\n",
      "2018-08-10T17:59:39.689949: step 1945, loss 2.87503e-05, acc 1\n",
      "2018-08-10T17:59:39.846836: step 1946, loss 0.000121202, acc 1\n",
      "2018-08-10T17:59:40.006592: step 1947, loss 0.000120101, acc 1\n",
      "2018-08-10T17:59:40.163095: step 1948, loss 0.0100357, acc 1\n",
      "2018-08-10T17:59:40.318836: step 1949, loss 4.69973e-05, acc 1\n",
      "2018-08-10T17:59:40.475669: step 1950, loss 0.00015434, acc 1\n",
      "2018-08-10T17:59:40.635676: step 1951, loss 0.000378795, acc 1\n",
      "2018-08-10T17:59:40.791045: step 1952, loss 0.000165565, acc 1\n",
      "2018-08-10T17:59:40.951833: step 1953, loss 0.00315939, acc 1\n",
      "2018-08-10T17:59:41.110913: step 1954, loss 0.000211095, acc 1\n",
      "2018-08-10T17:59:41.270024: step 1955, loss 0.000122508, acc 1\n",
      "2018-08-10T17:59:41.426049: step 1956, loss 8.47057e-05, acc 1\n",
      "2018-08-10T17:59:41.585692: step 1957, loss 6.49161e-05, acc 1\n",
      "2018-08-10T17:59:41.743839: step 1958, loss 0.000358374, acc 1\n",
      "2018-08-10T17:59:41.904730: step 1959, loss 9.52956e-05, acc 1\n",
      "2018-08-10T17:59:42.061742: step 1960, loss 0.000365046, acc 1\n",
      "2018-08-10T17:59:42.218565: step 1961, loss 0.000759668, acc 1\n",
      "2018-08-10T17:59:42.372939: step 1962, loss 0.000159289, acc 1\n",
      "2018-08-10T17:59:42.529322: step 1963, loss 0.000276376, acc 1\n",
      "2018-08-10T17:59:42.688006: step 1964, loss 4.33761e-05, acc 1\n",
      "2018-08-10T17:59:42.845659: step 1965, loss 0.000127936, acc 1\n",
      "2018-08-10T17:59:43.000222: step 1966, loss 0.000812765, acc 1\n",
      "2018-08-10T17:59:43.159271: step 1967, loss 0.000818248, acc 1\n",
      "2018-08-10T17:59:43.319567: step 1968, loss 0.00145581, acc 1\n",
      "2018-08-10T17:59:43.477147: step 1969, loss 0.00147032, acc 1\n",
      "2018-08-10T17:59:43.630874: step 1970, loss 0.000768395, acc 1\n",
      "2018-08-10T17:59:43.786308: step 1971, loss 0.00112687, acc 1\n",
      "2018-08-10T17:59:43.943260: step 1972, loss 0.000299153, acc 1\n",
      "2018-08-10T17:59:44.099718: step 1973, loss 0.000777027, acc 1\n",
      "2018-08-10T17:59:44.257294: step 1974, loss 0.00102764, acc 1\n",
      "2018-08-10T17:59:44.416576: step 1975, loss 0.000454659, acc 1\n",
      "2018-08-10T17:59:44.570382: step 1976, loss 0.000293555, acc 1\n",
      "2018-08-10T17:59:44.729207: step 1977, loss 0.000106319, acc 1\n",
      "2018-08-10T17:59:44.887744: step 1978, loss 0.00146312, acc 1\n",
      "2018-08-10T17:59:45.044605: step 1979, loss 0.000102491, acc 1\n",
      "2018-08-10T17:59:45.201666: step 1980, loss 0.00033955, acc 1\n",
      "2018-08-10T17:59:45.358889: step 1981, loss 9.00578e-05, acc 1\n",
      "2018-08-10T17:59:45.517162: step 1982, loss 0.000680342, acc 1\n",
      "2018-08-10T17:59:45.673910: step 1983, loss 9.59094e-05, acc 1\n",
      "2018-08-10T17:59:45.827874: step 1984, loss 0.00618268, acc 1\n",
      "2018-08-10T17:59:45.986843: step 1985, loss 4.32651e-05, acc 1\n",
      "2018-08-10T17:59:46.143007: step 1986, loss 0.0011117, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T17:59:46.300116: step 1987, loss 0.000952188, acc 1\n",
      "2018-08-10T17:59:46.460108: step 1988, loss 7.6492e-05, acc 1\n",
      "2018-08-10T17:59:46.616681: step 1989, loss 0.00105615, acc 1\n",
      "2018-08-10T17:59:46.772265: step 1990, loss 0.00187364, acc 1\n",
      "2018-08-10T17:59:46.932968: step 1991, loss 0.000255342, acc 1\n",
      "2018-08-10T17:59:47.087717: step 1992, loss 0.000109057, acc 1\n",
      "2018-08-10T17:59:47.245235: step 1993, loss 0.000243208, acc 1\n",
      "2018-08-10T17:59:47.403378: step 1994, loss 5.89883e-05, acc 1\n",
      "2018-08-10T17:59:47.560548: step 1995, loss 0.000237729, acc 1\n",
      "2018-08-10T17:59:47.716395: step 1996, loss 6.3124e-05, acc 1\n",
      "2018-08-10T17:59:47.875439: step 1997, loss 0.000375761, acc 1\n",
      "2018-08-10T17:59:48.031257: step 1998, loss 9.50083e-05, acc 1\n",
      "2018-08-10T17:59:48.181834: step 1999, loss 0.00051664, acc 1\n",
      "2018-08-10T17:59:48.331193: step 2000, loss 0.000766452, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T17:59:48.479843: step 2000, loss 0.792776, acc 0.856459\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533923672/checkpoints/model-2000\n",
      "\n",
      "2018-08-10T17:59:48.688147: step 2001, loss 0.00043518, acc 1\n",
      "2018-08-10T17:59:48.838748: step 2002, loss 0.00324565, acc 1\n",
      "2018-08-10T17:59:48.999078: step 2003, loss 0.000400438, acc 1\n",
      "2018-08-10T17:59:49.159681: step 2004, loss 0.000276461, acc 1\n",
      "2018-08-10T17:59:49.315431: step 2005, loss 0.0197368, acc 0.984375\n",
      "2018-08-10T17:59:49.472468: step 2006, loss 0.000393788, acc 1\n",
      "2018-08-10T17:59:49.630724: step 2007, loss 0.000217668, acc 1\n",
      "2018-08-10T17:59:49.789772: step 2008, loss 4.83196e-05, acc 1\n",
      "2018-08-10T17:59:49.948781: step 2009, loss 0.0765798, acc 0.984375\n",
      "2018-08-10T17:59:50.107958: step 2010, loss 5.46845e-05, acc 1\n",
      "2018-08-10T17:59:50.264015: step 2011, loss 0.000222726, acc 1\n",
      "2018-08-10T17:59:50.417969: step 2012, loss 0.000163876, acc 1\n",
      "2018-08-10T17:59:50.575195: step 2013, loss 8.02737e-05, acc 1\n",
      "2018-08-10T17:59:50.743400: step 2014, loss 5.8839e-05, acc 1\n",
      "2018-08-10T17:59:50.899970: step 2015, loss 0.000307008, acc 1\n",
      "2018-08-10T17:59:51.057970: step 2016, loss 5.4396e-05, acc 1\n",
      "2018-08-10T17:59:51.216169: step 2017, loss 7.79098e-05, acc 1\n",
      "2018-08-10T17:59:51.373480: step 2018, loss 0.000135052, acc 1\n",
      "2018-08-10T17:59:51.530706: step 2019, loss 0.000128503, acc 1\n",
      "2018-08-10T17:59:51.688715: step 2020, loss 0.000179141, acc 1\n",
      "2018-08-10T17:59:51.849174: step 2021, loss 0.000247308, acc 1\n",
      "2018-08-10T17:59:52.009941: step 2022, loss 0.000315067, acc 1\n",
      "2018-08-10T17:59:52.169311: step 2023, loss 0.000693267, acc 1\n",
      "2018-08-10T17:59:52.323685: step 2024, loss 0.00187593, acc 1\n",
      "2018-08-10T17:59:52.483999: step 2025, loss 0.0011548, acc 1\n",
      "2018-08-10T17:59:52.646970: step 2026, loss 0.000214639, acc 1\n",
      "2018-08-10T17:59:52.802732: step 2027, loss 0.00106854, acc 1\n",
      "2018-08-10T17:59:52.960598: step 2028, loss 0.0054855, acc 1\n",
      "2018-08-10T17:59:53.119763: step 2029, loss 0.000108079, acc 1\n",
      "2018-08-10T17:59:53.276596: step 2030, loss 0.0026315, acc 1\n",
      "2018-08-10T17:59:53.432481: step 2031, loss 4.13709e-05, acc 1\n",
      "2018-08-10T17:59:53.589212: step 2032, loss 0.000150957, acc 1\n",
      "2018-08-10T17:59:53.745142: step 2033, loss 9.58206e-05, acc 1\n",
      "2018-08-10T17:59:53.904138: step 2034, loss 0.00100696, acc 1\n",
      "2018-08-10T17:59:54.061468: step 2035, loss 0.000140674, acc 1\n",
      "2018-08-10T17:59:54.218709: step 2036, loss 7.36702e-05, acc 1\n",
      "2018-08-10T17:59:54.372073: step 2037, loss 0.00134977, acc 1\n",
      "2018-08-10T17:59:54.522985: step 2038, loss 0.00165382, acc 1\n",
      "2018-08-10T17:59:54.674634: step 2039, loss 0.00279821, acc 1\n",
      "2018-08-10T17:59:54.823549: step 2040, loss 0.000654651, acc 1\n",
      "2018-08-10T17:59:54.970067: step 2041, loss 7.04123e-05, acc 1\n",
      "2018-08-10T17:59:55.119466: step 2042, loss 8.53269e-05, acc 1\n",
      "2018-08-10T17:59:55.268746: step 2043, loss 4.55142e-05, acc 1\n",
      "2018-08-10T17:59:55.417703: step 2044, loss 0.00132822, acc 1\n",
      "2018-08-10T17:59:55.567552: step 2045, loss 0.0195249, acc 0.984375\n",
      "2018-08-10T17:59:55.716265: step 2046, loss 0.000287254, acc 1\n",
      "2018-08-10T17:59:55.865904: step 2047, loss 3.75233e-05, acc 1\n",
      "2018-08-10T17:59:56.014415: step 2048, loss 0.000613432, acc 1\n",
      "2018-08-10T17:59:56.163671: step 2049, loss 0.000188337, acc 1\n",
      "2018-08-10T17:59:56.312409: step 2050, loss 0.000313179, acc 1\n",
      "2018-08-10T17:59:56.461893: step 2051, loss 5.35458e-05, acc 1\n",
      "2018-08-10T17:59:56.615120: step 2052, loss 0.000144806, acc 1\n",
      "2018-08-10T17:59:56.765716: step 2053, loss 1.13765e-05, acc 1\n",
      "2018-08-10T17:59:56.914577: step 2054, loss 0.000221347, acc 1\n",
      "2018-08-10T17:59:57.063269: step 2055, loss 0.000126793, acc 1\n",
      "2018-08-10T17:59:57.212189: step 2056, loss 0.000864071, acc 1\n",
      "2018-08-10T17:59:57.361490: step 2057, loss 0.000702272, acc 1\n",
      "2018-08-10T17:59:57.508312: step 2058, loss 0.000134937, acc 1\n",
      "2018-08-10T17:59:57.657785: step 2059, loss 0.000234132, acc 1\n",
      "2018-08-10T17:59:57.806280: step 2060, loss 0.000270577, acc 1\n",
      "2018-08-10T17:59:57.957874: step 2061, loss 0.000107035, acc 1\n",
      "2018-08-10T17:59:58.108012: step 2062, loss 0.000412931, acc 1\n",
      "2018-08-10T17:59:58.256817: step 2063, loss 0.00192148, acc 1\n",
      "2018-08-10T17:59:58.404416: step 2064, loss 0.00114992, acc 1\n",
      "2018-08-10T17:59:58.557396: step 2065, loss 0.000168266, acc 1\n",
      "2018-08-10T17:59:58.710048: step 2066, loss 5.64653e-05, acc 1\n",
      "2018-08-10T17:59:58.860914: step 2067, loss 0.000888775, acc 1\n",
      "2018-08-10T17:59:59.021252: step 2068, loss 0.000139696, acc 1\n",
      "2018-08-10T17:59:59.175986: step 2069, loss 0.00277725, acc 1\n",
      "2018-08-10T17:59:59.338599: step 2070, loss 0.000831509, acc 1\n",
      "2018-08-10T17:59:59.497223: step 2071, loss 0.000164874, acc 1\n",
      "2018-08-10T17:59:59.653425: step 2072, loss 0.000196519, acc 1\n",
      "2018-08-10T17:59:59.812258: step 2073, loss 6.9969e-05, acc 1\n",
      "2018-08-10T17:59:59.970666: step 2074, loss 0.000444868, acc 1\n",
      "2018-08-10T18:00:00.132246: step 2075, loss 4.0419e-05, acc 1\n",
      "2018-08-10T18:00:00.290700: step 2076, loss 0.000291786, acc 1\n",
      "2018-08-10T18:00:00.454288: step 2077, loss 5.72201e-05, acc 1\n",
      "2018-08-10T18:00:00.617101: step 2078, loss 7.48075e-05, acc 1\n",
      "2018-08-10T18:00:00.775897: step 2079, loss 0.000973336, acc 1\n",
      "2018-08-10T18:00:00.929856: step 2080, loss 0.00016954, acc 1\n",
      "2018-08-10T18:00:01.089258: step 2081, loss 0.000184689, acc 1\n",
      "2018-08-10T18:00:01.245950: step 2082, loss 0.000249986, acc 1\n",
      "2018-08-10T18:00:01.407421: step 2083, loss 0.000422464, acc 1\n",
      "2018-08-10T18:00:01.568260: step 2084, loss 0.000218483, acc 1\n",
      "2018-08-10T18:00:01.728113: step 2085, loss 0.000128839, acc 1\n",
      "2018-08-10T18:00:01.887143: step 2086, loss 0.0181998, acc 0.984375\n",
      "2018-08-10T18:00:02.042755: step 2087, loss 0.000224239, acc 1\n",
      "2018-08-10T18:00:02.201153: step 2088, loss 2.68166e-05, acc 1\n",
      "2018-08-10T18:00:02.359070: step 2089, loss 0.000217932, acc 1\n",
      "2018-08-10T18:00:02.518912: step 2090, loss 0.000530164, acc 1\n",
      "2018-08-10T18:00:02.680811: step 2091, loss 0.000254596, acc 1\n",
      "2018-08-10T18:00:02.835853: step 2092, loss 0.000379259, acc 1\n",
      "2018-08-10T18:00:02.992598: step 2093, loss 0.000499344, acc 1\n",
      "2018-08-10T18:00:03.151166: step 2094, loss 0.000979221, acc 1\n",
      "2018-08-10T18:00:03.311487: step 2095, loss 0.000289565, acc 1\n",
      "2018-08-10T18:00:03.466457: step 2096, loss 0.00360665, acc 1\n",
      "2018-08-10T18:00:03.625139: step 2097, loss 0.00253374, acc 1\n",
      "2018-08-10T18:00:03.782137: step 2098, loss 5.36722e-05, acc 1\n",
      "2018-08-10T18:00:03.941706: step 2099, loss 0.000221578, acc 1\n",
      "2018-08-10T18:00:04.098193: step 2100, loss 0.00026211, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T18:00:04.252486: step 2100, loss 0.665191, acc 0.861244\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533923672/checkpoints/model-2100\n",
      "\n",
      "2018-08-10T18:00:04.473275: step 2101, loss 0.00278573, acc 1\n",
      "2018-08-10T18:00:04.633856: step 2102, loss 0.000679182, acc 1\n",
      "2018-08-10T18:00:04.788744: step 2103, loss 0.00269299, acc 1\n",
      "2018-08-10T18:00:04.949331: step 2104, loss 0.000144931, acc 1\n",
      "2018-08-10T18:00:05.107736: step 2105, loss 0.00026106, acc 1\n",
      "2018-08-10T18:00:05.262896: step 2106, loss 0.0168272, acc 0.983871\n",
      "2018-08-10T18:00:05.424416: step 2107, loss 0.000206409, acc 1\n",
      "2018-08-10T18:00:05.578597: step 2108, loss 0.000136492, acc 1\n",
      "2018-08-10T18:00:05.734795: step 2109, loss 0.00363272, acc 1\n",
      "2018-08-10T18:00:05.894114: step 2110, loss 0.00128915, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T18:00:06.054419: step 2111, loss 0.000207576, acc 1\n",
      "2018-08-10T18:00:06.212270: step 2112, loss 0.000421791, acc 1\n",
      "2018-08-10T18:00:06.371969: step 2113, loss 8.28368e-05, acc 1\n",
      "2018-08-10T18:00:06.530449: step 2114, loss 0.000221776, acc 1\n",
      "2018-08-10T18:00:06.693351: step 2115, loss 0.00131546, acc 1\n",
      "2018-08-10T18:00:06.848675: step 2116, loss 2.97146e-05, acc 1\n",
      "2018-08-10T18:00:07.009874: step 2117, loss 0.000674346, acc 1\n",
      "2018-08-10T18:00:07.168059: step 2118, loss 0.00129404, acc 1\n",
      "2018-08-10T18:00:07.324162: step 2119, loss 0.000118801, acc 1\n",
      "2018-08-10T18:00:07.481024: step 2120, loss 0.000220081, acc 1\n",
      "2018-08-10T18:00:07.639389: step 2121, loss 0.000131333, acc 1\n",
      "2018-08-10T18:00:07.798109: step 2122, loss 0.000194422, acc 1\n",
      "2018-08-10T18:00:07.957019: step 2123, loss 0.000166198, acc 1\n",
      "2018-08-10T18:00:08.116818: step 2124, loss 0.00010597, acc 1\n",
      "2018-08-10T18:00:08.278120: step 2125, loss 5.85426e-05, acc 1\n",
      "2018-08-10T18:00:08.442009: step 2126, loss 0.00787184, acc 1\n",
      "2018-08-10T18:00:08.599538: step 2127, loss 0.000253499, acc 1\n",
      "2018-08-10T18:00:08.757831: step 2128, loss 6.11867e-05, acc 1\n",
      "2018-08-10T18:00:08.919143: step 2129, loss 0.00195505, acc 1\n",
      "2018-08-10T18:00:09.077699: step 2130, loss 0.000245865, acc 1\n",
      "2018-08-10T18:00:09.228108: step 2131, loss 0.000345411, acc 1\n",
      "2018-08-10T18:00:09.372863: step 2132, loss 0.000163489, acc 1\n",
      "2018-08-10T18:00:09.526990: step 2133, loss 0.000234783, acc 1\n",
      "2018-08-10T18:00:09.682201: step 2134, loss 0.000398788, acc 1\n",
      "2018-08-10T18:00:09.839936: step 2135, loss 5.88813e-05, acc 1\n",
      "2018-08-10T18:00:09.999894: step 2136, loss 0.00186041, acc 1\n",
      "2018-08-10T18:00:10.157376: step 2137, loss 8.44403e-05, acc 1\n",
      "2018-08-10T18:00:10.314636: step 2138, loss 0.000577449, acc 1\n",
      "2018-08-10T18:00:10.473084: step 2139, loss 0.000173452, acc 1\n",
      "2018-08-10T18:00:10.633685: step 2140, loss 0.000177648, acc 1\n",
      "2018-08-10T18:00:10.789052: step 2141, loss 0.000316762, acc 1\n",
      "2018-08-10T18:00:10.949715: step 2142, loss 0.00115257, acc 1\n",
      "2018-08-10T18:00:11.110290: step 2143, loss 0.00256136, acc 1\n",
      "2018-08-10T18:00:11.270400: step 2144, loss 7.18118e-05, acc 1\n",
      "2018-08-10T18:00:11.426131: step 2145, loss 0.000512447, acc 1\n",
      "2018-08-10T18:00:11.587438: step 2146, loss 0.000421585, acc 1\n",
      "2018-08-10T18:00:11.748680: step 2147, loss 0.000140959, acc 1\n",
      "2018-08-10T18:00:11.912314: step 2148, loss 5.78577e-05, acc 1\n",
      "2018-08-10T18:00:12.076336: step 2149, loss 0.000392285, acc 1\n",
      "2018-08-10T18:00:12.238412: step 2150, loss 0.00310288, acc 1\n",
      "2018-08-10T18:00:12.395339: step 2151, loss 7.00615e-05, acc 1\n",
      "2018-08-10T18:00:12.554321: step 2152, loss 9.17887e-05, acc 1\n",
      "2018-08-10T18:00:12.715428: step 2153, loss 0.00147749, acc 1\n",
      "2018-08-10T18:00:12.880009: step 2154, loss 0.000261305, acc 1\n",
      "2018-08-10T18:00:13.037412: step 2155, loss 1.86976e-05, acc 1\n",
      "2018-08-10T18:00:13.194810: step 2156, loss 0.000627008, acc 1\n",
      "2018-08-10T18:00:13.357002: step 2157, loss 0.000160695, acc 1\n",
      "2018-08-10T18:00:13.512474: step 2158, loss 0.00171572, acc 1\n",
      "2018-08-10T18:00:13.672142: step 2159, loss 0.000177165, acc 1\n",
      "2018-08-10T18:00:13.833889: step 2160, loss 0.000536547, acc 1\n",
      "2018-08-10T18:00:13.996201: step 2161, loss 0.00082672, acc 1\n",
      "2018-08-10T18:00:14.154846: step 2162, loss 0.000698913, acc 1\n",
      "2018-08-10T18:00:14.316594: step 2163, loss 0.00378154, acc 1\n",
      "2018-08-10T18:00:14.476885: step 2164, loss 7.35572e-05, acc 1\n",
      "2018-08-10T18:00:14.637414: step 2165, loss 0.000203782, acc 1\n",
      "2018-08-10T18:00:14.792315: step 2166, loss 0.00115009, acc 1\n",
      "2018-08-10T18:00:14.953821: step 2167, loss 8.03258e-05, acc 1\n",
      "2018-08-10T18:00:15.111587: step 2168, loss 0.00190809, acc 1\n",
      "2018-08-10T18:00:15.265942: step 2169, loss 4.33569e-05, acc 1\n",
      "2018-08-10T18:00:15.414474: step 2170, loss 0.00021075, acc 1\n",
      "2018-08-10T18:00:15.558511: step 2171, loss 0.0049345, acc 1\n",
      "2018-08-10T18:00:15.705681: step 2172, loss 0.000212422, acc 1\n",
      "2018-08-10T18:00:15.856215: step 2173, loss 0.00080669, acc 1\n",
      "2018-08-10T18:00:16.004267: step 2174, loss 0.000407832, acc 1\n",
      "2018-08-10T18:00:16.153565: step 2175, loss 5.6143e-05, acc 1\n",
      "2018-08-10T18:00:16.303248: step 2176, loss 0.000367054, acc 1\n",
      "2018-08-10T18:00:16.455355: step 2177, loss 0.000463305, acc 1\n",
      "2018-08-10T18:00:16.602688: step 2178, loss 0.00172645, acc 1\n",
      "2018-08-10T18:00:16.752389: step 2179, loss 0.00150415, acc 1\n",
      "2018-08-10T18:00:16.909899: step 2180, loss 0.00136956, acc 1\n",
      "2018-08-10T18:00:17.059486: step 2181, loss 2.48785e-05, acc 1\n",
      "2018-08-10T18:00:17.205869: step 2182, loss 0.00167742, acc 1\n",
      "2018-08-10T18:00:17.354717: step 2183, loss 0.000226098, acc 1\n",
      "2018-08-10T18:00:17.499599: step 2184, loss 0.000218257, acc 1\n",
      "2018-08-10T18:00:17.653002: step 2185, loss 0.00506826, acc 1\n",
      "2018-08-10T18:00:17.801443: step 2186, loss 0.00028628, acc 1\n",
      "2018-08-10T18:00:17.951463: step 2187, loss 2.39907e-05, acc 1\n",
      "2018-08-10T18:00:18.101239: step 2188, loss 0.000250079, acc 1\n",
      "2018-08-10T18:00:18.257924: step 2189, loss 9.59421e-05, acc 1\n",
      "2018-08-10T18:00:18.408251: step 2190, loss 0.000154645, acc 1\n",
      "2018-08-10T18:00:18.563990: step 2191, loss 0.000510262, acc 1\n",
      "2018-08-10T18:00:18.713673: step 2192, loss 0.000129718, acc 1\n",
      "2018-08-10T18:00:18.864058: step 2193, loss 0.00286874, acc 1\n",
      "2018-08-10T18:00:19.010916: step 2194, loss 0.00124184, acc 1\n",
      "2018-08-10T18:00:19.160516: step 2195, loss 8.68194e-05, acc 1\n",
      "2018-08-10T18:00:19.308573: step 2196, loss 0.000244713, acc 1\n",
      "2018-08-10T18:00:19.454146: step 2197, loss 3.23466e-05, acc 1\n",
      "2018-08-10T18:00:19.603736: step 2198, loss 0.000257964, acc 1\n",
      "2018-08-10T18:00:19.758263: step 2199, loss 7.08986e-05, acc 1\n",
      "2018-08-10T18:00:19.914867: step 2200, loss 0.00012041, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T18:00:20.069755: step 2200, loss 0.880822, acc 0.856459\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533923672/checkpoints/model-2200\n",
      "\n",
      "2018-08-10T18:00:20.291268: step 2201, loss 0.000120473, acc 1\n",
      "2018-08-10T18:00:20.452960: step 2202, loss 0.00232751, acc 1\n",
      "2018-08-10T18:00:20.611665: step 2203, loss 4.72177e-05, acc 1\n",
      "2018-08-10T18:00:20.770595: step 2204, loss 1.42511e-05, acc 1\n",
      "2018-08-10T18:00:20.933767: step 2205, loss 0.00154577, acc 1\n",
      "2018-08-10T18:00:21.091811: step 2206, loss 0.000276431, acc 1\n",
      "2018-08-10T18:00:21.250319: step 2207, loss 0.0014231, acc 1\n",
      "2018-08-10T18:00:21.411787: step 2208, loss 0.000135447, acc 1\n",
      "2018-08-10T18:00:21.569346: step 2209, loss 0.00332721, acc 1\n",
      "2018-08-10T18:00:21.723981: step 2210, loss 0.000143157, acc 1\n",
      "2018-08-10T18:00:21.883872: step 2211, loss 0.000177609, acc 1\n",
      "2018-08-10T18:00:22.042576: step 2212, loss 0.000161617, acc 1\n",
      "2018-08-10T18:00:22.199155: step 2213, loss 2.64367e-05, acc 1\n",
      "2018-08-10T18:00:22.357531: step 2214, loss 0.000222765, acc 1\n",
      "2018-08-10T18:00:22.521070: step 2215, loss 0.000209838, acc 1\n",
      "2018-08-10T18:00:22.678771: step 2216, loss 0.00227753, acc 1\n",
      "2018-08-10T18:00:22.837091: step 2217, loss 0.00114738, acc 1\n",
      "2018-08-10T18:00:22.997298: step 2218, loss 0.00066912, acc 1\n",
      "2018-08-10T18:00:23.153276: step 2219, loss 4.42412e-05, acc 1\n",
      "2018-08-10T18:00:23.311854: step 2220, loss 6.8562e-05, acc 1\n",
      "2018-08-10T18:00:23.469810: step 2221, loss 9.21147e-05, acc 1\n",
      "2018-08-10T18:00:23.626755: step 2222, loss 0.000357872, acc 1\n",
      "2018-08-10T18:00:23.782301: step 2223, loss 0.000387081, acc 1\n",
      "2018-08-10T18:00:23.943934: step 2224, loss 2.87491e-05, acc 1\n",
      "2018-08-10T18:00:24.099634: step 2225, loss 0.000354635, acc 1\n",
      "2018-08-10T18:00:24.259468: step 2226, loss 5.7121e-05, acc 1\n",
      "2018-08-10T18:00:24.420107: step 2227, loss 6.95885e-05, acc 1\n",
      "2018-08-10T18:00:24.578687: step 2228, loss 0.000142221, acc 1\n",
      "2018-08-10T18:00:24.737448: step 2229, loss 0.000388947, acc 1\n",
      "2018-08-10T18:00:24.896648: step 2230, loss 0.000159015, acc 1\n",
      "2018-08-10T18:00:25.058878: step 2231, loss 5.41638e-05, acc 1\n",
      "2018-08-10T18:00:25.216475: step 2232, loss 7.25613e-05, acc 1\n",
      "2018-08-10T18:00:25.376756: step 2233, loss 8.74386e-05, acc 1\n",
      "2018-08-10T18:00:25.534849: step 2234, loss 0.00013786, acc 1\n",
      "2018-08-10T18:00:25.691452: step 2235, loss 0.000461027, acc 1\n",
      "2018-08-10T18:00:25.842039: step 2236, loss 0.000170696, acc 1\n",
      "2018-08-10T18:00:26.000641: step 2237, loss 0.000507461, acc 1\n",
      "2018-08-10T18:00:26.158180: step 2238, loss 5.64555e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T18:00:26.316840: step 2239, loss 0.000397379, acc 1\n",
      "2018-08-10T18:00:26.480112: step 2240, loss 0.000237471, acc 1\n",
      "2018-08-10T18:00:26.638243: step 2241, loss 0.000812172, acc 1\n",
      "2018-08-10T18:00:26.795790: step 2242, loss 0.000184854, acc 1\n",
      "2018-08-10T18:00:26.980658: step 2243, loss 4.15929e-05, acc 1\n",
      "2018-08-10T18:00:27.137323: step 2244, loss 0.000727662, acc 1\n",
      "2018-08-10T18:00:27.299063: step 2245, loss 0.000331248, acc 1\n",
      "2018-08-10T18:00:27.463753: step 2246, loss 0.000172759, acc 1\n",
      "2018-08-10T18:00:27.622805: step 2247, loss 8.85422e-05, acc 1\n",
      "2018-08-10T18:00:27.782625: step 2248, loss 6.15504e-05, acc 1\n",
      "2018-08-10T18:00:27.942484: step 2249, loss 8.57181e-05, acc 1\n",
      "2018-08-10T18:00:28.106481: step 2250, loss 0.000229893, acc 1\n",
      "2018-08-10T18:00:28.265746: step 2251, loss 0.00798341, acc 1\n",
      "2018-08-10T18:00:28.432813: step 2252, loss 0.000140477, acc 1\n",
      "2018-08-10T18:00:28.591415: step 2253, loss 4.65988e-05, acc 1\n",
      "2018-08-10T18:00:28.752968: step 2254, loss 0.000107481, acc 1\n",
      "2018-08-10T18:00:28.912523: step 2255, loss 2.01103e-05, acc 1\n",
      "2018-08-10T18:00:29.071307: step 2256, loss 0.0313526, acc 0.984375\n",
      "2018-08-10T18:00:29.229122: step 2257, loss 0.000135121, acc 1\n",
      "2018-08-10T18:00:29.388233: step 2258, loss 0.000345807, acc 1\n",
      "2018-08-10T18:00:29.550622: step 2259, loss 6.96268e-05, acc 1\n",
      "2018-08-10T18:00:29.709717: step 2260, loss 0.000119914, acc 1\n",
      "2018-08-10T18:00:29.867107: step 2261, loss 0.000633127, acc 1\n",
      "2018-08-10T18:00:30.016572: step 2262, loss 0.000725345, acc 1\n",
      "2018-08-10T18:00:30.163965: step 2263, loss 0.000195669, acc 1\n",
      "2018-08-10T18:00:30.312270: step 2264, loss 0.004817, acc 1\n",
      "2018-08-10T18:00:30.466522: step 2265, loss 0.000727136, acc 1\n",
      "2018-08-10T18:00:30.624103: step 2266, loss 0.000114382, acc 1\n",
      "2018-08-10T18:00:30.780101: step 2267, loss 0.000317704, acc 1\n",
      "2018-08-10T18:00:30.937637: step 2268, loss 0.00129136, acc 1\n",
      "2018-08-10T18:00:31.097041: step 2269, loss 0.000662438, acc 1\n",
      "2018-08-10T18:00:31.254214: step 2270, loss 0.000121241, acc 1\n",
      "2018-08-10T18:00:31.409194: step 2271, loss 0.00048481, acc 1\n",
      "2018-08-10T18:00:31.564670: step 2272, loss 0.0158218, acc 0.984375\n",
      "2018-08-10T18:00:31.723819: step 2273, loss 0.000430202, acc 1\n",
      "2018-08-10T18:00:31.883621: step 2274, loss 0.00122765, acc 1\n",
      "2018-08-10T18:00:32.036604: step 2275, loss 0.000318923, acc 1\n",
      "2018-08-10T18:00:32.195157: step 2276, loss 0.00181059, acc 1\n",
      "2018-08-10T18:00:32.354592: step 2277, loss 0.000101027, acc 1\n",
      "2018-08-10T18:00:32.516049: step 2278, loss 0.000405019, acc 1\n",
      "2018-08-10T18:00:32.677103: step 2279, loss 0.0011512, acc 1\n",
      "2018-08-10T18:00:32.834919: step 2280, loss 0.00012384, acc 1\n",
      "2018-08-10T18:00:32.997332: step 2281, loss 0.000162583, acc 1\n",
      "2018-08-10T18:00:33.156758: step 2282, loss 0.000728363, acc 1\n",
      "2018-08-10T18:00:33.316451: step 2283, loss 0.000227771, acc 1\n",
      "2018-08-10T18:00:33.475982: step 2284, loss 0.00243863, acc 1\n",
      "2018-08-10T18:00:33.634440: step 2285, loss 0.000270975, acc 1\n",
      "2018-08-10T18:00:33.792131: step 2286, loss 0.000352074, acc 1\n",
      "2018-08-10T18:00:33.954892: step 2287, loss 0.000615092, acc 1\n",
      "2018-08-10T18:00:34.106767: step 2288, loss 0.00017904, acc 1\n",
      "2018-08-10T18:00:34.265189: step 2289, loss 0.000255904, acc 1\n",
      "2018-08-10T18:00:34.423338: step 2290, loss 0.00289251, acc 1\n",
      "2018-08-10T18:00:34.584401: step 2291, loss 0.000877758, acc 1\n",
      "2018-08-10T18:00:34.741805: step 2292, loss 0.000209497, acc 1\n",
      "2018-08-10T18:00:34.899759: step 2293, loss 0.0012844, acc 1\n",
      "2018-08-10T18:00:35.056279: step 2294, loss 0.000649571, acc 1\n",
      "2018-08-10T18:00:35.212722: step 2295, loss 0.000133977, acc 1\n",
      "2018-08-10T18:00:35.373236: step 2296, loss 0.000101653, acc 1\n",
      "2018-08-10T18:00:35.530057: step 2297, loss 0.000101041, acc 1\n",
      "2018-08-10T18:00:35.686390: step 2298, loss 0.000223324, acc 1\n",
      "2018-08-10T18:00:35.843407: step 2299, loss 0.000355821, acc 1\n",
      "2018-08-10T18:00:36.004351: step 2300, loss 0.000198339, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T18:00:36.161714: step 2300, loss 0.745805, acc 0.856459\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533923672/checkpoints/model-2300\n",
      "\n",
      "2018-08-10T18:00:36.363702: step 2301, loss 0.000729349, acc 1\n",
      "2018-08-10T18:00:36.515763: step 2302, loss 0.000720774, acc 1\n",
      "2018-08-10T18:00:36.668982: step 2303, loss 0.00022434, acc 1\n",
      "2018-08-10T18:00:36.820579: step 2304, loss 0.000458937, acc 1\n",
      "2018-08-10T18:00:36.969956: step 2305, loss 0.0010212, acc 1\n",
      "2018-08-10T18:00:37.118212: step 2306, loss 0.00742954, acc 1\n",
      "2018-08-10T18:00:37.266487: step 2307, loss 7.04322e-05, acc 1\n",
      "2018-08-10T18:00:37.416858: step 2308, loss 0.000399124, acc 1\n",
      "2018-08-10T18:00:37.564593: step 2309, loss 5.15739e-05, acc 1\n",
      "2018-08-10T18:00:37.712320: step 2310, loss 0.00766168, acc 1\n",
      "2018-08-10T18:00:37.861263: step 2311, loss 4.55468e-05, acc 1\n",
      "2018-08-10T18:00:38.012534: step 2312, loss 0.000326288, acc 1\n",
      "2018-08-10T18:00:38.162778: step 2313, loss 4.38293e-05, acc 1\n",
      "2018-08-10T18:00:38.309283: step 2314, loss 0.000152188, acc 1\n",
      "2018-08-10T18:00:38.462130: step 2315, loss 0.00102376, acc 1\n",
      "2018-08-10T18:00:38.614107: step 2316, loss 0.000190012, acc 1\n",
      "2018-08-10T18:00:38.763181: step 2317, loss 0.00385524, acc 1\n",
      "2018-08-10T18:00:38.913731: step 2318, loss 0.00234899, acc 1\n",
      "2018-08-10T18:00:39.063277: step 2319, loss 0.000268152, acc 1\n",
      "2018-08-10T18:00:39.213220: step 2320, loss 5.01558e-05, acc 1\n",
      "2018-08-10T18:00:39.364941: step 2321, loss 0.000800403, acc 1\n",
      "2018-08-10T18:00:39.512691: step 2322, loss 0.00341788, acc 1\n",
      "2018-08-10T18:00:39.663319: step 2323, loss 0.000386988, acc 1\n",
      "2018-08-10T18:00:39.810720: step 2324, loss 5.07491e-05, acc 1\n",
      "2018-08-10T18:00:39.961230: step 2325, loss 0.00022813, acc 1\n",
      "2018-08-10T18:00:40.110494: step 2326, loss 0.00441393, acc 1\n",
      "2018-08-10T18:00:40.256128: step 2327, loss 0.00104518, acc 1\n",
      "2018-08-10T18:00:40.405627: step 2328, loss 0.000520089, acc 1\n",
      "2018-08-10T18:00:40.560535: step 2329, loss 0.000445024, acc 1\n",
      "2018-08-10T18:00:40.716650: step 2330, loss 0.000264312, acc 1\n",
      "2018-08-10T18:00:40.872553: step 2331, loss 0.000134105, acc 1\n",
      "2018-08-10T18:00:41.032220: step 2332, loss 2.02045e-05, acc 1\n",
      "2018-08-10T18:00:41.189137: step 2333, loss 0.000280889, acc 1\n",
      "2018-08-10T18:00:41.346340: step 2334, loss 9.09567e-05, acc 1\n",
      "2018-08-10T18:00:41.506235: step 2335, loss 0.000128689, acc 1\n",
      "2018-08-10T18:00:41.665800: step 2336, loss 0.000256371, acc 1\n",
      "2018-08-10T18:00:41.823942: step 2337, loss 0.000319102, acc 1\n",
      "2018-08-10T18:00:41.981928: step 2338, loss 5.65668e-05, acc 1\n",
      "2018-08-10T18:00:42.138071: step 2339, loss 0.000297992, acc 1\n",
      "2018-08-10T18:00:42.289224: step 2340, loss 0.00023347, acc 1\n",
      "2018-08-10T18:00:42.450304: step 2341, loss 0.00025402, acc 1\n",
      "2018-08-10T18:00:42.608299: step 2342, loss 0.00735528, acc 1\n",
      "2018-08-10T18:00:42.765945: step 2343, loss 0.000524066, acc 1\n",
      "2018-08-10T18:00:42.924908: step 2344, loss 0.000148504, acc 1\n",
      "2018-08-10T18:00:43.081747: step 2345, loss 7.81285e-05, acc 1\n",
      "2018-08-10T18:00:43.238048: step 2346, loss 0.000112712, acc 1\n",
      "2018-08-10T18:00:43.397396: step 2347, loss 0.00037687, acc 1\n",
      "2018-08-10T18:00:43.555339: step 2348, loss 0.00021156, acc 1\n",
      "2018-08-10T18:00:43.713142: step 2349, loss 0.00042601, acc 1\n",
      "2018-08-10T18:00:43.867225: step 2350, loss 0.00309472, acc 1\n",
      "2018-08-10T18:00:44.026592: step 2351, loss 0.000250367, acc 1\n",
      "2018-08-10T18:00:44.182384: step 2352, loss 0.000309265, acc 1\n",
      "2018-08-10T18:00:44.335207: step 2353, loss 8.17683e-05, acc 1\n",
      "2018-08-10T18:00:44.493310: step 2354, loss 0.000321597, acc 1\n",
      "2018-08-10T18:00:44.652759: step 2355, loss 0.000208027, acc 1\n",
      "2018-08-10T18:00:44.807915: step 2356, loss 9.51737e-05, acc 1\n",
      "2018-08-10T18:00:44.972232: step 2357, loss 9.15245e-05, acc 1\n",
      "2018-08-10T18:00:45.131804: step 2358, loss 9.23491e-05, acc 1\n",
      "2018-08-10T18:00:45.290422: step 2359, loss 0.000189179, acc 1\n",
      "2018-08-10T18:00:45.448864: step 2360, loss 6.8294e-05, acc 1\n",
      "2018-08-10T18:00:45.605980: step 2361, loss 8.58064e-05, acc 1\n",
      "2018-08-10T18:00:45.761777: step 2362, loss 9.05176e-05, acc 1\n",
      "2018-08-10T18:00:45.922094: step 2363, loss 0.000253385, acc 1\n",
      "2018-08-10T18:00:46.077273: step 2364, loss 0.000236139, acc 1\n",
      "2018-08-10T18:00:46.232553: step 2365, loss 0.000803253, acc 1\n",
      "2018-08-10T18:00:46.384828: step 2366, loss 0.000318443, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T18:00:46.544980: step 2367, loss 0.00248014, acc 1\n",
      "2018-08-10T18:00:46.701422: step 2368, loss 0.00721843, acc 1\n",
      "2018-08-10T18:00:46.858791: step 2369, loss 0.00299472, acc 1\n",
      "2018-08-10T18:00:47.021857: step 2370, loss 0.000654675, acc 1\n",
      "2018-08-10T18:00:47.176932: step 2371, loss 0.000104019, acc 1\n",
      "2018-08-10T18:00:47.337525: step 2372, loss 7.49709e-05, acc 1\n",
      "2018-08-10T18:00:47.495189: step 2373, loss 0.000918958, acc 1\n",
      "2018-08-10T18:00:47.652717: step 2374, loss 0.00332543, acc 1\n",
      "2018-08-10T18:00:47.807230: step 2375, loss 8.35057e-05, acc 1\n",
      "2018-08-10T18:00:47.966784: step 2376, loss 0.000642358, acc 1\n",
      "2018-08-10T18:00:48.125720: step 2377, loss 0.000152724, acc 1\n",
      "2018-08-10T18:00:48.280972: step 2378, loss 0.00229385, acc 1\n",
      "2018-08-10T18:00:48.432201: step 2379, loss 0.000119651, acc 1\n",
      "2018-08-10T18:00:48.590254: step 2380, loss 2.47296e-05, acc 1\n",
      "2018-08-10T18:00:48.747853: step 2381, loss 0.000135672, acc 1\n",
      "2018-08-10T18:00:48.907265: step 2382, loss 0.00187494, acc 1\n",
      "2018-08-10T18:00:49.066290: step 2383, loss 0.000146887, acc 1\n",
      "2018-08-10T18:00:49.222277: step 2384, loss 7.90345e-05, acc 1\n",
      "2018-08-10T18:00:49.380680: step 2385, loss 8.65709e-06, acc 1\n",
      "2018-08-10T18:00:49.539015: step 2386, loss 0.000166869, acc 1\n",
      "2018-08-10T18:00:49.693877: step 2387, loss 0.000890502, acc 1\n",
      "2018-08-10T18:00:49.852854: step 2388, loss 0.00102287, acc 1\n",
      "2018-08-10T18:00:50.013403: step 2389, loss 0.000176345, acc 1\n",
      "2018-08-10T18:00:50.170674: step 2390, loss 6.56706e-05, acc 1\n",
      "2018-08-10T18:00:50.328880: step 2391, loss 0.000208821, acc 1\n",
      "2018-08-10T18:00:50.482896: step 2392, loss 4.24613e-05, acc 1\n",
      "2018-08-10T18:00:50.645108: step 2393, loss 0.00052906, acc 1\n",
      "2018-08-10T18:00:50.802112: step 2394, loss 0.00696732, acc 1\n",
      "2018-08-10T18:00:50.959830: step 2395, loss 7.37259e-05, acc 1\n",
      "2018-08-10T18:00:51.109986: step 2396, loss 7.80146e-05, acc 1\n",
      "2018-08-10T18:00:51.261348: step 2397, loss 6.69507e-05, acc 1\n",
      "2018-08-10T18:00:51.415067: step 2398, loss 0.00496664, acc 1\n",
      "2018-08-10T18:00:51.571039: step 2399, loss 1.60097e-05, acc 1\n",
      "2018-08-10T18:00:51.724706: step 2400, loss 7.59879e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T18:00:51.876816: step 2400, loss 0.847587, acc 0.861244\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533923672/checkpoints/model-2400\n",
      "\n",
      "2018-08-10T18:00:52.102251: step 2401, loss 0.000170454, acc 1\n",
      "2018-08-10T18:00:52.263649: step 2402, loss 0.000641204, acc 1\n",
      "2018-08-10T18:00:52.426173: step 2403, loss 0.000474474, acc 1\n",
      "2018-08-10T18:00:52.586730: step 2404, loss 4.0518e-05, acc 1\n",
      "2018-08-10T18:00:52.745941: step 2405, loss 3.45388e-05, acc 1\n",
      "2018-08-10T18:00:52.903635: step 2406, loss 0.000264198, acc 1\n",
      "2018-08-10T18:00:53.063875: step 2407, loss 0.000666309, acc 1\n",
      "2018-08-10T18:00:53.221206: step 2408, loss 0.000400413, acc 1\n",
      "2018-08-10T18:00:53.377995: step 2409, loss 0.000115807, acc 1\n",
      "2018-08-10T18:00:53.538063: step 2410, loss 6.34286e-05, acc 1\n",
      "2018-08-10T18:00:53.695271: step 2411, loss 4.9862e-05, acc 1\n",
      "2018-08-10T18:00:53.854280: step 2412, loss 0.000281658, acc 1\n",
      "2018-08-10T18:00:54.016392: step 2413, loss 4.16848e-05, acc 1\n",
      "2018-08-10T18:00:54.177976: step 2414, loss 0.00013325, acc 1\n",
      "2018-08-10T18:00:54.338218: step 2415, loss 0.000134033, acc 1\n",
      "2018-08-10T18:00:54.497382: step 2416, loss 0.00168233, acc 1\n",
      "2018-08-10T18:00:54.660036: step 2417, loss 1.95053e-05, acc 1\n",
      "2018-08-10T18:00:54.814080: step 2418, loss 0.000230377, acc 1\n",
      "2018-08-10T18:00:54.982326: step 2419, loss 0.000303648, acc 1\n",
      "2018-08-10T18:00:55.142466: step 2420, loss 0.000994435, acc 1\n",
      "2018-08-10T18:00:55.302212: step 2421, loss 0.000317059, acc 1\n",
      "2018-08-10T18:00:55.461977: step 2422, loss 0.00526155, acc 1\n",
      "2018-08-10T18:00:55.622811: step 2423, loss 8.39016e-05, acc 1\n",
      "2018-08-10T18:00:55.781803: step 2424, loss 6.43463e-05, acc 1\n",
      "2018-08-10T18:00:55.941430: step 2425, loss 0.000108, acc 1\n",
      "2018-08-10T18:00:56.099868: step 2426, loss 0.000179157, acc 1\n",
      "2018-08-10T18:00:56.258316: step 2427, loss 0.00182528, acc 1\n",
      "2018-08-10T18:00:56.418863: step 2428, loss 0.000280266, acc 1\n",
      "2018-08-10T18:00:56.583730: step 2429, loss 5.29201e-05, acc 1\n",
      "2018-08-10T18:00:56.745822: step 2430, loss 5.53216e-05, acc 1\n",
      "2018-08-10T18:00:56.903827: step 2431, loss 5.24932e-05, acc 1\n",
      "2018-08-10T18:00:57.060115: step 2432, loss 0.000601597, acc 1\n",
      "2018-08-10T18:00:57.210465: step 2433, loss 0.000373255, acc 1\n",
      "2018-08-10T18:00:57.360059: step 2434, loss 4.95161e-05, acc 1\n",
      "2018-08-10T18:00:57.510172: step 2435, loss 5.93166e-05, acc 1\n",
      "2018-08-10T18:00:57.660734: step 2436, loss 0.000122583, acc 1\n",
      "2018-08-10T18:00:57.810802: step 2437, loss 0.000899182, acc 1\n",
      "2018-08-10T18:00:57.964242: step 2438, loss 9.96699e-05, acc 1\n",
      "2018-08-10T18:00:58.115562: step 2439, loss 0.000257983, acc 1\n",
      "2018-08-10T18:00:58.264830: step 2440, loss 7.94895e-05, acc 1\n",
      "2018-08-10T18:00:58.415047: step 2441, loss 0.000130618, acc 1\n",
      "2018-08-10T18:00:58.564963: step 2442, loss 0.000550322, acc 1\n",
      "2018-08-10T18:00:58.717356: step 2443, loss 0.000115689, acc 1\n",
      "2018-08-10T18:00:58.862772: step 2444, loss 0.000236789, acc 1\n",
      "2018-08-10T18:00:59.015169: step 2445, loss 0.000749504, acc 1\n",
      "2018-08-10T18:00:59.163441: step 2446, loss 0.00160121, acc 1\n",
      "2018-08-10T18:00:59.310876: step 2447, loss 3.7218e-05, acc 1\n",
      "2018-08-10T18:00:59.462740: step 2448, loss 0.000160646, acc 1\n",
      "2018-08-10T18:00:59.613575: step 2449, loss 0.00408618, acc 1\n",
      "2018-08-10T18:00:59.764423: step 2450, loss 0.000279044, acc 1\n",
      "2018-08-10T18:00:59.915783: step 2451, loss 0.000356001, acc 1\n",
      "2018-08-10T18:01:00.064588: step 2452, loss 6.31261e-05, acc 1\n",
      "2018-08-10T18:01:00.214122: step 2453, loss 0.000117333, acc 1\n",
      "2018-08-10T18:01:00.361695: step 2454, loss 0.000195276, acc 1\n",
      "2018-08-10T18:01:00.512389: step 2455, loss 6.34886e-05, acc 1\n",
      "2018-08-10T18:01:00.661406: step 2456, loss 2.6019e-05, acc 1\n",
      "2018-08-10T18:01:00.807051: step 2457, loss 0.000459765, acc 1\n",
      "2018-08-10T18:01:00.957101: step 2458, loss 0.00020134, acc 1\n",
      "2018-08-10T18:01:01.109003: step 2459, loss 0.00021368, acc 1\n",
      "2018-08-10T18:01:01.260184: step 2460, loss 0.000170345, acc 1\n",
      "2018-08-10T18:01:01.414302: step 2461, loss 1.70265e-05, acc 1\n",
      "2018-08-10T18:01:01.566942: step 2462, loss 0.00098447, acc 1\n",
      "2018-08-10T18:01:01.721908: step 2463, loss 0.000295044, acc 1\n",
      "2018-08-10T18:01:01.876731: step 2464, loss 0.000367377, acc 1\n",
      "2018-08-10T18:01:02.042991: step 2465, loss 8.96183e-05, acc 1\n",
      "2018-08-10T18:01:02.202395: step 2466, loss 5.71611e-05, acc 1\n",
      "2018-08-10T18:01:02.362438: step 2467, loss 3.61866e-05, acc 1\n",
      "2018-08-10T18:01:02.521119: step 2468, loss 0.000245803, acc 1\n",
      "2018-08-10T18:01:02.680355: step 2469, loss 2.19385e-05, acc 1\n",
      "2018-08-10T18:01:02.833852: step 2470, loss 8.4187e-05, acc 1\n",
      "2018-08-10T18:01:02.993528: step 2471, loss 0.000274014, acc 1\n",
      "2018-08-10T18:01:03.150895: step 2472, loss 0.00479343, acc 1\n",
      "2018-08-10T18:01:03.309783: step 2473, loss 0.000100835, acc 1\n",
      "2018-08-10T18:01:03.469826: step 2474, loss 0.000158497, acc 1\n",
      "2018-08-10T18:01:03.628094: step 2475, loss 0.000130468, acc 1\n",
      "2018-08-10T18:01:03.784635: step 2476, loss 5.35444e-05, acc 1\n",
      "2018-08-10T18:01:03.944992: step 2477, loss 0.000399967, acc 1\n",
      "2018-08-10T18:01:04.103624: step 2478, loss 0.00184551, acc 1\n",
      "2018-08-10T18:01:04.265484: step 2479, loss 0.000406674, acc 1\n",
      "2018-08-10T18:01:04.425787: step 2480, loss 0.000160343, acc 1\n",
      "2018-08-10T18:01:04.588305: step 2481, loss 2.95025e-05, acc 1\n",
      "2018-08-10T18:01:04.747880: step 2482, loss 0.000405396, acc 1\n",
      "2018-08-10T18:01:04.906064: step 2483, loss 0.00136246, acc 1\n",
      "2018-08-10T18:01:05.062342: step 2484, loss 0.000148283, acc 1\n",
      "2018-08-10T18:01:05.224091: step 2485, loss 2.40572e-05, acc 1\n",
      "2018-08-10T18:01:05.380333: step 2486, loss 8.56783e-05, acc 1\n",
      "2018-08-10T18:01:05.538788: step 2487, loss 0.00102445, acc 1\n",
      "2018-08-10T18:01:05.700036: step 2488, loss 0.000138384, acc 1\n",
      "2018-08-10T18:01:05.857610: step 2489, loss 3.0649e-05, acc 1\n",
      "2018-08-10T18:01:06.016605: step 2490, loss 0.000107821, acc 1\n",
      "2018-08-10T18:01:06.174995: step 2491, loss 0.000309657, acc 1\n",
      "2018-08-10T18:01:06.335224: step 2492, loss 4.74106e-05, acc 1\n",
      "2018-08-10T18:01:06.495931: step 2493, loss 0.000140469, acc 1\n",
      "2018-08-10T18:01:06.658378: step 2494, loss 0.000110387, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10T18:01:06.813899: step 2495, loss 4.16855e-05, acc 1\n",
      "2018-08-10T18:01:06.967102: step 2496, loss 0.00155708, acc 1\n",
      "2018-08-10T18:01:07.123622: step 2497, loss 0.00114203, acc 1\n",
      "2018-08-10T18:01:07.280303: step 2498, loss 0.000624285, acc 1\n",
      "2018-08-10T18:01:07.436925: step 2499, loss 0.000164848, acc 1\n",
      "2018-08-10T18:01:07.592750: step 2500, loss 0.000725553, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T18:01:07.752824: step 2500, loss 0.877892, acc 0.856459\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533923672/checkpoints/model-2500\n",
      "\n",
      "2018-08-10T18:01:07.974193: step 2501, loss 0.000113703, acc 1\n",
      "2018-08-10T18:01:08.131749: step 2502, loss 0.0024418, acc 1\n",
      "2018-08-10T18:01:08.288707: step 2503, loss 5.05958e-05, acc 1\n",
      "2018-08-10T18:01:08.448794: step 2504, loss 0.0037111, acc 1\n",
      "2018-08-10T18:01:08.612145: step 2505, loss 0.000133548, acc 1\n",
      "2018-08-10T18:01:08.768785: step 2506, loss 0.00131054, acc 1\n",
      "2018-08-10T18:01:08.929285: step 2507, loss 9.25101e-05, acc 1\n",
      "2018-08-10T18:01:09.090444: step 2508, loss 5.14223e-05, acc 1\n",
      "2018-08-10T18:01:09.244158: step 2509, loss 3.7657e-05, acc 1\n",
      "2018-08-10T18:01:09.402897: step 2510, loss 0.00016248, acc 1\n",
      "2018-08-10T18:01:09.560243: step 2511, loss 0.000671113, acc 1\n",
      "2018-08-10T18:01:09.716484: step 2512, loss 5.83322e-05, acc 1\n",
      "2018-08-10T18:01:09.873416: step 2513, loss 6.40917e-05, acc 1\n",
      "2018-08-10T18:01:10.030640: step 2514, loss 2.43652e-05, acc 1\n",
      "2018-08-10T18:01:10.186644: step 2515, loss 0.000292035, acc 1\n",
      "2018-08-10T18:01:10.342422: step 2516, loss 7.9042e-05, acc 1\n",
      "2018-08-10T18:01:10.502987: step 2517, loss 0.000651067, acc 1\n",
      "2018-08-10T18:01:10.662584: step 2518, loss 8.49509e-05, acc 1\n",
      "2018-08-10T18:01:10.818997: step 2519, loss 4.30657e-05, acc 1\n",
      "2018-08-10T18:01:10.978760: step 2520, loss 7.67743e-05, acc 1\n",
      "2018-08-10T18:01:11.136453: step 2521, loss 7.79043e-05, acc 1\n",
      "2018-08-10T18:01:11.287835: step 2522, loss 2.03315e-05, acc 1\n",
      "2018-08-10T18:01:11.450104: step 2523, loss 0.000118033, acc 1\n",
      "2018-08-10T18:01:11.608307: step 2524, loss 0.000256059, acc 1\n",
      "2018-08-10T18:01:11.766830: step 2525, loss 8.59452e-05, acc 1\n",
      "2018-08-10T18:01:11.925402: step 2526, loss 0.000601315, acc 1\n",
      "2018-08-10T18:01:12.078648: step 2527, loss 3.58874e-05, acc 1\n",
      "2018-08-10T18:01:12.225495: step 2528, loss 0.000165572, acc 1\n",
      "2018-08-10T18:01:12.377300: step 2529, loss 8.83446e-05, acc 1\n",
      "2018-08-10T18:01:12.531458: step 2530, loss 0.000527728, acc 1\n",
      "2018-08-10T18:01:12.688670: step 2531, loss 3.12371e-05, acc 1\n",
      "2018-08-10T18:01:12.845413: step 2532, loss 3.11243e-05, acc 1\n",
      "2018-08-10T18:01:13.002847: step 2533, loss 5.79078e-06, acc 1\n",
      "2018-08-10T18:01:13.161048: step 2534, loss 0.0013265, acc 1\n",
      "2018-08-10T18:01:13.316286: step 2535, loss 4.46085e-05, acc 1\n",
      "2018-08-10T18:01:13.473311: step 2536, loss 4.85044e-05, acc 1\n",
      "2018-08-10T18:01:13.631060: step 2537, loss 0.00203846, acc 1\n",
      "2018-08-10T18:01:13.787251: step 2538, loss 8.59558e-05, acc 1\n",
      "2018-08-10T18:01:13.949637: step 2539, loss 8.08643e-05, acc 1\n",
      "2018-08-10T18:01:14.109283: step 2540, loss 0.00113255, acc 1\n",
      "2018-08-10T18:01:14.267003: step 2541, loss 1.7598e-05, acc 1\n",
      "2018-08-10T18:01:14.426005: step 2542, loss 4.67497e-05, acc 1\n",
      "2018-08-10T18:01:14.587580: step 2543, loss 0.000334488, acc 1\n",
      "2018-08-10T18:01:14.747092: step 2544, loss 2.60545e-05, acc 1\n",
      "2018-08-10T18:01:14.904575: step 2545, loss 0.00186323, acc 1\n",
      "2018-08-10T18:01:15.062204: step 2546, loss 3.26798e-05, acc 1\n",
      "2018-08-10T18:01:15.218554: step 2547, loss 1.41334e-05, acc 1\n",
      "2018-08-10T18:01:15.369688: step 2548, loss 8.83407e-05, acc 1\n",
      "2018-08-10T18:01:15.529120: step 2549, loss 0.000747236, acc 1\n",
      "2018-08-10T18:01:15.684923: step 2550, loss 0.000834272, acc 1\n",
      "2018-08-10T18:01:15.842339: step 2551, loss 3.2376e-05, acc 1\n",
      "2018-08-10T18:01:16.004748: step 2552, loss 0.000327053, acc 1\n",
      "2018-08-10T18:01:16.160571: step 2553, loss 3.01872e-05, acc 1\n",
      "2018-08-10T18:01:16.315102: step 2554, loss 8.94661e-05, acc 1\n",
      "2018-08-10T18:01:16.476374: step 2555, loss 0.00064059, acc 1\n",
      "2018-08-10T18:01:16.636637: step 2556, loss 3.2356e-05, acc 1\n",
      "2018-08-10T18:01:16.794169: step 2557, loss 0.000187358, acc 1\n",
      "2018-08-10T18:01:16.950888: step 2558, loss 0.00098628, acc 1\n",
      "2018-08-10T18:01:17.109395: step 2559, loss 0.000120285, acc 1\n",
      "2018-08-10T18:01:17.268240: step 2560, loss 3.29664e-05, acc 1\n",
      "2018-08-10T18:01:17.422570: step 2561, loss 0.000910299, acc 1\n",
      "2018-08-10T18:01:17.580825: step 2562, loss 0.000111029, acc 1\n",
      "2018-08-10T18:01:17.739196: step 2563, loss 4.23936e-05, acc 1\n",
      "2018-08-10T18:01:17.897759: step 2564, loss 0.00024152, acc 1\n",
      "2018-08-10T18:01:18.059205: step 2565, loss 9.74927e-05, acc 1\n",
      "2018-08-10T18:01:18.213923: step 2566, loss 0.000286693, acc 1\n",
      "2018-08-10T18:01:18.362902: step 2567, loss 0.000177528, acc 1\n",
      "2018-08-10T18:01:18.513389: step 2568, loss 0.00013164, acc 1\n",
      "2018-08-10T18:01:18.662356: step 2569, loss 8.05604e-05, acc 1\n",
      "2018-08-10T18:01:18.819712: step 2570, loss 0.000187254, acc 1\n",
      "2018-08-10T18:01:18.968575: step 2571, loss 0.0136281, acc 0.984375\n",
      "2018-08-10T18:01:19.115107: step 2572, loss 7.23594e-05, acc 1\n",
      "2018-08-10T18:01:19.263023: step 2573, loss 0.000169438, acc 1\n",
      "2018-08-10T18:01:19.408548: step 2574, loss 3.51968e-05, acc 1\n",
      "2018-08-10T18:01:19.557330: step 2575, loss 5.55689e-05, acc 1\n",
      "2018-08-10T18:01:19.704668: step 2576, loss 0.00311376, acc 1\n",
      "2018-08-10T18:01:19.854268: step 2577, loss 0.000928735, acc 1\n",
      "2018-08-10T18:01:20.004136: step 2578, loss 0.000208344, acc 1\n",
      "2018-08-10T18:01:20.155686: step 2579, loss 2.57368e-05, acc 1\n",
      "2018-08-10T18:01:20.304251: step 2580, loss 1.58731e-05, acc 1\n",
      "2018-08-10T18:01:20.455028: step 2581, loss 0.000422322, acc 1\n",
      "2018-08-10T18:01:20.602229: step 2582, loss 0.000402919, acc 1\n",
      "2018-08-10T18:01:20.753119: step 2583, loss 0.000211267, acc 1\n",
      "2018-08-10T18:01:20.898262: step 2584, loss 0.000273248, acc 1\n",
      "2018-08-10T18:01:21.048699: step 2585, loss 6.7713e-05, acc 1\n",
      "2018-08-10T18:01:21.196909: step 2586, loss 7.96332e-05, acc 1\n",
      "2018-08-10T18:01:21.341858: step 2587, loss 4.63901e-05, acc 1\n",
      "2018-08-10T18:01:21.489335: step 2588, loss 0.000611552, acc 1\n",
      "2018-08-10T18:01:21.638466: step 2589, loss 0.000168233, acc 1\n",
      "2018-08-10T18:01:21.789526: step 2590, loss 0.00011117, acc 1\n",
      "2018-08-10T18:01:21.941907: step 2591, loss 0.000373309, acc 1\n",
      "2018-08-10T18:01:22.091605: step 2592, loss 0.000259547, acc 1\n",
      "2018-08-10T18:01:22.241623: step 2593, loss 0.000376931, acc 1\n",
      "2018-08-10T18:01:22.393623: step 2594, loss 0.0126534, acc 0.984375\n",
      "2018-08-10T18:01:22.548339: step 2595, loss 0.000124379, acc 1\n",
      "2018-08-10T18:01:22.708060: step 2596, loss 6.02006e-05, acc 1\n",
      "2018-08-10T18:01:22.866428: step 2597, loss 3.19545e-05, acc 1\n",
      "2018-08-10T18:01:23.026621: step 2598, loss 0.000137721, acc 1\n",
      "2018-08-10T18:01:23.185827: step 2599, loss 0.000127684, acc 1\n",
      "2018-08-10T18:01:23.340056: step 2600, loss 7.72519e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-08-10T18:01:23.494616: step 2600, loss 0.799645, acc 0.866029\n",
      "\n",
      "Saved model checkpoint to /home/mfrazzini/W266_project/deep_learning_sentiment_classification/runs/1533923672/checkpoints/model-2600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from Text_CNN import TextCNN\n",
    "\n",
    "# CNN Model\n",
    "# Leveraged Denny Britz template for CNN:\n",
    "# http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\n",
    "\n",
    "# run this to clear tf flags without re-starting kernel\n",
    "\n",
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()\n",
    "    keys_list = [keys for keys in flags_dict]\n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "        \n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS(sys.argv)\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "x_train = np.array(train_x_neural)\n",
    "y_train = np.array(train_y_neural)\n",
    "x_dev = np.array(test_x_neural)\n",
    "y_dev = np.array(test_y_neural)\n",
    "\n",
    "print(\"x_train\", np.shape(x_train))\n",
    "print(\"y_train\", np.shape(y_train))\n",
    "print(\"x_dev\", np.shape(x_dev))\n",
    "print(\"x_dev\", np.shape(y_dev))\n",
    "\n",
    "print(\"Vocabulary Size: {:d}\".format(vocab.size))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "\n",
    "\n",
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]\n",
    "            \n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=2,\n",
    "            vocab_size=vocab.size,\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
